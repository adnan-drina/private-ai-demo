apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: build-runtime-image
  namespace: private-ai-demo
  labels:
    app.kubernetes.io/part-of: model-serving
    pipeline.openshift.io/type: kubernetes
  annotations:
    argocd.argoproj.io/sync-wave: "1"
spec:
  description: |
    Builds lightweight runtime image for Path B architecture.
    Image contains only vLLM + dependencies, NO model weights.
    Results in ~5-10GB images instead of 90GB+ ModelCar images.
    
  params:
    - name: model_name
      type: string
      description: "Model name"
    - name: version_name
      type: string
      description: "Version name"
    - name: quay_org
      type: string
      description: "Quay.io organization"
    - name: base_image
      type: string
      default: "registry.access.redhat.com/ubi9/python-311:latest"
      description: "Base image for runtime"
    - name: vllm_version
      type: string
      default: "0.6.0"
      description: "vLLM version to install"
  
  workspaces:
    - name: workspace
      description: "PVC workspace"
  
  steps:
    - name: build-runtime
      image: registry.redhat.io/rhel9/buildah:latest
      imagePullPolicy: IfNotPresent
      
      securityContext:
        privileged: true  # Required for buildah
      
      env:
        - name: MODEL_NAME
          value: "$(params.model_name)"
        - name: VERSION_NAME
          value: "$(params.version_name)"
        - name: QUAY_ORG
          value: "$(params.quay_org)"
        - name: BASE_IMAGE
          value: "$(params.base_image)"
        - name: VLLM_VERSION
          value: "$(params.vllm_version)"
        - name: QUAY_USER
          valueFrom:
            secretKeyRef:
              name: quay-credentials
              key: username
        - name: QUAY_PASSWORD
          valueFrom:
            secretKeyRef:
              name: quay-credentials
              key: password
        - name: BUILDAH_STORAGE_ROOT
          value: "/workspace/buildah-storage"
        - name: BUILDAH_TMPDIR
          value: "/workspace/buildah-tmp"
      
      script: |
        #!/bin/bash
        set -euo pipefail
        
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        echo "ðŸ—ï¸  BUILD LIGHTWEIGHT RUNTIME IMAGE"
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        echo "Timestamp: $(date -u +"%Y-%m-%dT%H:%M:%SZ")"
        echo "Model: ${MODEL_NAME}"
        echo "Version: ${VERSION_NAME}"
        echo "Pattern: Path B (Runtime Only - NO weights)"
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        echo ""
        
        # Setup storage directories
        echo "ðŸ”§ Setting up buildah storage..."
        mkdir -p "${BUILDAH_STORAGE_ROOT}" "${BUILDAH_TMPDIR}"
        echo "âœ… Storage configured"
        echo ""
        
        # Define image names
        IMAGE_NAME="mistral-24b-runtime"
        IMAGE_TAG="${VERSION_NAME}"
        QUAY_IMAGE="quay.io/${QUAY_ORG}/${IMAGE_NAME}:${IMAGE_TAG}"
        
        echo "ðŸ“¦ Target image: ${QUAY_IMAGE}"
        echo ""
        
        # Read MinIO metadata from upload task
        echo "ðŸ“‹ Reading MinIO metadata from previous task..."
        if [ -f "/workspace/metadata/minio-upload.json" ]; then
          cat /workspace/metadata/minio-upload.json
          echo ""
        else
          echo "âš ï¸  No MinIO metadata found (expected from upload-to-minio task)"
        fi
        echo ""
        
        # Create Containerfile for runtime
        echo "ðŸ“ Creating Containerfile..."
        cat > /workspace/Containerfile.runtime <<'CONTAINERFILE_EOF'
        # Lightweight vLLM Runtime Image (Path B - NO model weights)
        ARG BASE_IMAGE
        FROM ${BASE_IMAGE}
        
        # Install system dependencies
        USER root
        RUN dnf install -y \
            git \
            wget \
            gcc \
            gcc-c++ \
            make \
            && dnf clean all
        
        # Install Python dependencies
        ARG VLLM_VERSION
        RUN pip install --no-cache-dir \
            vllm==${VLLM_VERSION} \
            huggingface-hub \
            boto3 \
            && pip cache purge
        
        # Install MinIO client for model fetching
        RUN wget https://dl.min.io/client/mc/release/linux-amd64/mc \
            -O /usr/local/bin/mc && \
            chmod +x /usr/local/bin/mc
        
        # Create app directory
        WORKDIR /app
        
        # Create fetch and serve scripts
        COPY fetch_model.sh /app/fetch_model.sh
        COPY serve.sh /app/serve.sh
        RUN chmod +x /app/fetch_model.sh /app/serve.sh
        
        # Model cache directory (will be mounted PVC)
        RUN mkdir -p /model-cache && \
            chown -R 1001:0 /model-cache && \
            chmod -R g+rwX /model-cache
        
        USER 1001
        
        # Default command (overridden by InferenceService)
        CMD ["/app/serve.sh"]
        CONTAINERFILE_EOF
        
        # Create model fetch script
        cat > /workspace/fetch_model.sh <<'FETCH_EOF'
        #!/bin/bash
        set -euo pipefail
        
        echo "ðŸ”½ Fetching model from MinIO..."
        echo "Endpoint: ${MINIO_ENDPOINT}"
        echo "Bucket: ${MINIO_BUCKET}"
        echo "Prefix: ${MINIO_PREFIX}"
        
        # Configure mc
        export MC_CONFIG_DIR=/tmp/.mc
        mc alias set minio "${MINIO_ENDPOINT}" "${MINIO_ACCESS_KEY}" "${MINIO_SECRET_KEY}"
        
        # Download model to cache
        echo "Downloading to /model-cache..."
        mc cp --recursive "minio/${MINIO_BUCKET}/${MINIO_PREFIX}/" /model-cache/
        
        echo "âœ… Model fetched successfully"
        ls -lh /model-cache/
        FETCH_EOF
        
        # Create serve script
        cat > /workspace/serve.sh <<'SERVE_EOF'
        #!/bin/bash
        set -euo pipefail
        
        echo "ðŸš€ Starting vLLM inference server..."
        
        # If model not in cache, fetch it
        if [ ! -d "/model-cache" ] || [ -z "$(ls -A /model-cache)" ]; then
          echo "Model cache empty, fetching from MinIO..."
          /app/fetch_model.sh
        else
          echo "Using cached model from /model-cache"
        fi
        
        # Start vLLM server
        exec python -m vllm.entrypoints.openai.api_server \
          --model /model-cache \
          --host 0.0.0.0 \
          --port 8080 \
          ${VLLM_ARGS:-}
        SERVE_EOF
        
        echo "âœ… Containerfile and scripts created"
        echo ""
        
        # Build runtime image
        echo "ðŸ—ï¸  Building runtime image..."
        buildah --root "${BUILDAH_STORAGE_ROOT}" bud \
          --format=oci \
          --build-arg BASE_IMAGE="${BASE_IMAGE}" \
          --build-arg VLLM_VERSION="${VLLM_VERSION}" \
          --file=/workspace/Containerfile.runtime \
          --tag "${QUAY_IMAGE}" \
          --layers=false \
          --squash \
          /workspace
        
        echo "âœ… Runtime image built"
        echo ""
        
        # Get image size
        IMAGE_SIZE=$(buildah --root "${BUILDAH_STORAGE_ROOT}" images --format "{{.Size}}" "${QUAY_IMAGE}")
        echo "ðŸ“ Runtime image size: ${IMAGE_SIZE}"
        echo ""
        
        # Push to Quay
        echo "â¬†ï¸  Pushing to Quay.io..."
        buildah --root "${BUILDAH_STORAGE_ROOT}" login \
          --username="${QUAY_USER}" \
          --password="${QUAY_PASSWORD}" \
          quay.io
        
        buildah --root "${BUILDAH_STORAGE_ROOT}" push \
          "${QUAY_IMAGE}"
        
        echo "âœ… Pushed to Quay.io"
        echo ""
        
        # Write metadata
        echo "ðŸ“ Writing metadata..."
        mkdir -p /workspace/metadata
        
        cat > /workspace/metadata/runtime-image.json <<METADATA_EOF
        {
          "model_name": "${MODEL_NAME}",
          "version_name": "${VERSION_NAME}",
          "runtime_image": "${QUAY_IMAGE}",
          "image_size": "${IMAGE_SIZE}",
          "pattern": "minio",
          "vllm_version": "${VLLM_VERSION}",
          "build_timestamp": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")"
        }
        METADATA_EOF
        
        echo "âœ… Metadata written"
        echo ""
        
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        echo "âœ… RUNTIME IMAGE BUILD COMPLETE"
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        echo "Image: ${QUAY_IMAGE}"
        echo "Size: ${IMAGE_SIZE}"
        echo "Pattern: Path B (Runtime + MinIO weights)"
        echo "Next: register-model task"
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
      
      computeResources:
        requests:
          memory: "4Gi"
          cpu: "2"
          ephemeral-storage: "20Gi"
        limits:
          memory: "8Gi"
          cpu: "4"
          ephemeral-storage: "30Gi"

