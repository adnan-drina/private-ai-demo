# Stage 2: Model Alignment - Deployment Status

**Date:** November 3, 2025  
**Status:** ‚ö†Ô∏è Partially Complete - LlamaStack Configuration Challenge

---

## ‚úÖ What's Working & Reproducible

### 1. **deploy.sh Script** ‚úÖ
**Location:** `stages/stage2-model-alignment/deploy.sh`

The deployment script now captures ALL required setup:

```bash
# Step 1: Create MinIO bucket for KFP artifacts
# Step 2: Create DSPA MinIO credentials secret
# Step 3: Configure SCC permissions for LlamaStack
#   - Grants anyuid SCC to rag-workload-sa
#   - Required by LlamaStack Operator (sets fsGroup: 0)
# Step 4: Enable Service Mesh sidecar injection
#   - Labels namespace with istio.io/rev=data-science-smcp
#   - Required for LlamaStack ‚Üí vLLM connectivity
# Step 5: Deploy GitOps resources
#   - Applies all manifests from gitops/stage02-model-alignment
```

**Configuration:** `.env` file with MinIO credentials (template provided)

**Usage:**
```bash
cd stages/stage2-model-alignment
cp .env.template .env
# Edit .env with MinIO credentials from stage00
./deploy.sh
```

**‚úÖ Fully reproducible** - All secrets and configuration are managed by script + `.env`

---

### 2. **Service Mesh Configuration** ‚úÖ

**Verified:**
- ‚úÖ Namespace `private-ai-demo` IS a Service Mesh member
- ‚úÖ SMCP name: `data-science-smcp` (OpenShift Service Mesh 2.6.11)
- ‚úÖ ServiceMeshMember exists and is Ready
- ‚úÖ Namespace now labeled with `istio.io/rev=data-science-smcp`

**Challenge:** 
- ‚ùå LlamaStack Operator does NOT inject sidecars despite namespace-level label
- Root cause: Operator doesn't support `podTemplate.annotations` in `LlamaStackDistribution` CRD
- Operator creates pods without any sidecar injection annotations

**Evidence:**
```bash
# Namespace labeled correctly
oc get ns private-ai-demo --show-labels | grep istio.io/rev=data-science-smcp

# But pods don't get sidecars
oc get pod -l app=llama-stack -o jsonpath='{.spec.containers[*].name}'
# Output: llamastack  (no istio-proxy)
```

---

### 3. **Base Infrastructure** ‚úÖ

All supporting components are healthy:

| Component | Status | Details |
|-----------|--------|---------|
| **Milvus** | ‚úÖ 1/1 Running | Vector database for RAG |
| **MariaDB** | ‚úÖ 1/1 Running | Backend for Milvus |
| **DSPA (KFP)** | ‚úÖ 6/6 pods | Kubeflow Pipelines ready |
| **Docling** | üïê Starting | 10min initial startup (2GB+ deps) |
| **Stage 1 Models** | ‚úÖ 2/2 Ready | vLLM InferenceServices |
| **ServiceAccounts** | ‚úÖ Created | With proper SCC grants |

**‚úÖ All infrastructure is ready for RAG workloads**

---

## ‚ùå LlamaStack Configuration Challenge

### **Problem Summary**

The LlamaStack Operator has design limitations that make it difficult to connect to Knative-based InferenceServices:

1. **Sidecar Injection Blocked**
   - Operator doesn't support `podTemplate.annotations` for sidecar injection
   - Namespace-level injection (`istio.io/rev` label) is ignored by operator
   - Pods are created without `sidecar.istio.io/inject: "true"` annotation

2. **rh-dev Distribution** ‚ö†Ô∏è
   - Has built-in `run.yaml` with hardcoded internal URLs
   - Works perfectly BUT requires sidecar for Service Mesh connectivity
   - Cannot be overridden with environment variables

3. **Custom ConfigMap Approach** ‚ùå
   - Can mount ConfigMap with custom `run.yaml` (external HTTPS URLs)
   - But creates permission errors: `/opt/app-root/src/.llama/distributions/...`
   - ConfigMap content tries to write SQLite databases in read-only context

### **Three Approaches Tried**

#### A. Sidecar Injection (RECOMMENDED but blocked)
```yaml
spec:
  server:
    podOverrides:
      annotations:  # ‚ùå NOT SUPPORTED by operator
        sidecar.istio.io/inject: "true"
```

**Result:** Operator ignores/doesn't support pod annotations

#### B. External HTTPS Routes with ConfigMap
```yaml
spec:
  server:
    distribution:
      image: registry.redhat.io/rhoai/...
    podOverrides:
      volumes:
        - name: config
          configMap:
            name: llamastack-config
      volumeMounts:
        - name: config
          mountPath: /opt/app-root/run.yaml
          subPath: run.yaml
```

**Result:** Permission denied creating SQLite databases

#### C. rh-dev Distribution (works but needs sidecar)
```yaml
spec:
  server:
    distribution:
      name: "rh-dev"
```

**Result:** 
- ‚úÖ Starts successfully
- ‚ùå Cannot connect to vLLM (no sidecar, Service Mesh blocks)

---

## üìä Current State

### **LlamaStack Pod Logs (rh-dev)**
```
INFO Starting up
INFO Llama Stack running on http://...8321
INFO Application startup complete
```

‚úÖ **Application starts successfully** with rh-dev distribution

‚ùå **But cannot register vLLM model** due to Service Mesh connectivity:
```
ValueError: Failed to connect to vLLM at 
  http://mistral-24b-quantized-predictor.private-ai-demo.svc.cluster.local/v1
```

### **Why Internal URL Fails**

1. vLLM is a **Knative Service** (KService)
2. Knative integrates with **OpenShift Service Mesh**
3. Service returns: `ExternalName ‚Üí knative-local-gateway.istio-system.svc`
4. Service Mesh requires **mTLS** (client needs istio-proxy sidecar)
5. LlamaStack pod has NO sidecar ‚Üí connection rejected

### **Why External HTTPS URL Should Work**

```bash
# External route works fine
curl -k https://mistral-24b-quantized-private-ai-demo.apps.cluster-gmgrr.gmgrr.sandbox5294.opentlc.com/v1/models
# ‚úÖ Returns 200 OK
```

But ConfigMap approach to configure this has permissions issues.

---

## üéØ Recommendations

### **Short-term: Manual Workaround**

Use external HTTPS route + disable TLS verification in the rh-dev distribution's config.

**Challenge:** Need to find where rh-dev stores its built-in config and if there's an env var override.

### **Medium-term: Red Hat Support**

**Question for Red Hat:**
> "LlamaStackDistribution operator doesn't support pod template annotations for Istio sidecar injection. How do we configure LlamaStack to connect to Knative-based InferenceServices that require Service Mesh membership?"

**Expected answer:**
- Operator enhancement to support `podTemplate.annotations`
- Alternative env var to override vLLM URL in rh-dev distribution
- Service Mesh policy to allow non-mesh pods (not ideal)

### **Long-term: Alternative Approach**

Deploy LlamaStack as a standard Deployment (not via operator) with full control over:
- Pod annotations (sidecar injection)
- ConfigMap mounts (custom run.yaml)
- Environment variables
- Security context

---

## üìù What's Captured in GitOps

### **Files Created/Updated**

1. **`stages/stage2-model-alignment/deploy.sh`**
   - ‚úÖ SCC grants for rag-workload-sa
   - ‚úÖ Service Mesh sidecar injection enablement
   - ‚úÖ MinIO secret creation
   - ‚úÖ GitOps deployment

2. **`stages/stage2-model-alignment/.env`**
   - ‚úÖ Template for MinIO credentials
   - ‚úÖ Project configuration

3. **`gitops/stage02-model-alignment/llama-stack/`**
   - ‚úÖ `serviceaccount.yaml` - with Red Hat pull secret reference
   - ‚úÖ `llamastack-distribution.yaml` - using rh-dev distribution
   - ‚úÖ `configmap.yaml` - custom run.yaml with external HTTPS URLs
   - ‚úÖ `pvc.yaml` - persistent storage for LlamaStack data
   - ‚úÖ `route.yaml` - external access to LlamaStack API

4. **`docs/02-STAGES/`**
   - ‚úÖ `STAGE-2-LLAMASTACK-STATUS.md` - detailed troubleshooting guide
   - ‚úÖ `STAGE-2-DEPLOY-STATUS.md` - this document

---

## ‚úÖ What Works Right Now

If you **manually configure external HTTPS URL** in the rh-dev distribution or find the right env var:

```yaml
env:
  - name: VLLM_URL  # if rh-dev respects this
    value: "https://mistral-24b-quantized-private-ai-demo.apps.cluster-gmgrr.gmgrr.sandbox5294.opentlc.com/v1"
  - name: VLLM_TLS_VERIFY
    value: "false"
```

Then LlamaStack should start successfully and connect to vLLM.

**Current blocker:** Finding the right env var or configuration method for rh-dev distribution.

---

## üìö References

### Documentation
- **Red Hat RHOAI 2.25 - Llama Stack:** https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/2.25/html-single/working_with_llama_stack/index
- **Service Mesh 2.6:** https://docs.openshift.com/service-mesh/2.6/service_mesh_install/installing-ossm.html

### Git Commits
- **Latest:** `24399ef` - Capture Stage 2 deployment in deploy.sh + ConfigMap approach
- **Previous:** `eb317b1` - Align LlamaStack with RHOAI 2.25 and fix image access

### Debugging Commands
```bash
# Check Service Mesh membership
oc get servicemeshmember -n private-ai-demo
oc get ns private-ai-demo --show-labels | grep istio

# Check LlamaStack
oc get llamastackdistribution llama-stack -n private-ai-demo
oc get pods -l app=llama-stack -n private-ai-demo
oc logs -l app=llama-stack -c llamastack -n private-ai-demo --tail=100

# Test vLLM connectivity (internal vs external)
# Internal (requires sidecar)
curl http://mistral-24b-quantized-predictor.private-ai-demo.svc.cluster.local/v1/models

# External (works without sidecar)
curl -k https://mistral-24b-quantized-private-ai-demo.apps.cluster-gmgrr.gmgrr.sandbox5294.opentlc.com/v1/models
```

---

## üîÑ Next Steps

1. **Research rh-dev distribution env vars**
   - Check if `VLLM_URL` is respected
   - Look for official Red Hat documentation on overriding rh-dev config

2. **Contact Red Hat Support**
   - Ask about sidecar injection support
   - Request guidance on Knative + Service Mesh + LlamaStack integration

3. **Alternative: Manual Deployment**
   - Deploy LlamaStack as standard Deployment (not via operator)
   - Full control over configuration and sidecar injection

4. **Continue with other Stage 2 components**
   - Finish Docling startup validation
   - Create Kubeflow Pipelines for RAG document ingestion
   - Create ArgoCD Application for Stage 2

---

## ‚úÖ Summary

| Aspect | Status |
|--------|--------|
| **deploy.sh script** | ‚úÖ Complete & reproducible |
| **Service Mesh setup** | ‚úÖ Configured correctly |
| **Base infrastructure** | ‚úÖ All healthy |
| **LlamaStack config** | ‚ö†Ô∏è Operator limitations |
| **Documentation** | ‚úÖ Comprehensive |
| **GitOps capture** | ‚úÖ All in git |

**The infrastructure is solid. The LlamaStack configuration needs operator enhancement or alternative deployment approach.**


