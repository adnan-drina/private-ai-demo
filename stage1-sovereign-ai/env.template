# Stage 1: Sovereign AI - Environment Configuration Template
#
# INSTRUCTIONS:
# 1. Copy this file to .env:
#    cp env.template .env
# 2. Fill in your actual values below
# 3. The .env file is git-ignored for security
# 4. Never commit .env to version control

# ============================================
# HuggingFace Configuration
# ============================================
# Get your token from: https://huggingface.co/settings/tokens
# Required for downloading Mistral 24B model
# 
# Create a token with "Read" permissions:
# 1. Go to https://huggingface.co/settings/tokens
# 2. Click "New token"
# 3. Name: "vllm-demo-token"
# 4. Type: "Read"
# 5. Copy the token below
#
# ⚠️  REQUIRED: Replace with your actual HuggingFace token
# Example format: hf_xxxxxxxxxxxxxxxxxxxxx (40 characters)
HF_TOKEN=

# ============================================
# OpenShift Configuration
# ============================================
# Namespace for vLLM deployment
PROJECT_NAME=private-ai-demo

# ============================================
# vLLM Configuration
# ============================================
# Model to deploy
MODEL_NAME=mistralai/Mistral-Nemo-Instruct-2407

# Quantization method (default: awq)
# Options: awq, gptq, squeezellm, marlin, fp8, or leave empty for none
QUANTIZATION_METHOD=awq

# GPU configuration
# Number of GPUs to use (default: 1)
NUM_GPUS=1

# GPU memory utilization (0.0-1.0, default: 0.9)
GPU_MEMORY_UTILIZATION=0.9

# ============================================
# Optional: Advanced vLLM Configuration
# ============================================
# Maximum model length (tokens)
# MAX_MODEL_LEN=4096

# Tensor parallel size (for multi-GPU)
# TENSOR_PARALLEL_SIZE=1

# Pipeline parallel size (for multi-GPU)
# PIPELINE_PARALLEL_SIZE=1

# Enable prefix caching for better performance
# ENABLE_PREFIX_CACHING=true

# KV cache data type (auto, fp8, fp16)
# KV_CACHE_DTYPE=auto

