{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TrustyAI LM-Eval Results\n",
    "\n",
    "This notebook loads and visualizes evaluation results from TrustyAI LM-Eval.\n",
    "\n",
    "## Models\n",
    "- Full Precision (4 GPUs)\n",
    "- Quantized W4A16 (1 GPU)\n",
    "\n",
    "## Benchmarks\n",
    "- ARC-Easy, HellaSwag, GSM8K, TruthfulQA MC2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, subprocess, pandas as pd, matplotlib.pyplot as plt\n",
    "import seaborn as sns, numpy as np\n",
    "from IPython.display import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_results(job_name):\n",
    "    cmd = ['oc', 'get', 'lmevaljob', job_name, '-n', 'private-ai-demo', '-o', 'jsonpath={.status}']\n",
    "    try:\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n",
    "        status = json.loads(result.stdout)\n",
    "        return status\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "print('Fetching results from latest evaluation jobs...\\n')\n",
    "# Updated to use the latest eval jobs with all 4 benchmarks\n",
    "full_status = get_eval_results('eval-mistral-full')\n",
    "quant_status = get_eval_results('eval-mistral-quantized')\n",
    "\n",
    "full_results = json.loads(full_status['results']) if full_status and 'results' in full_status else None\n",
    "quant_results = json.loads(quant_status['results']) if quant_status and 'results' in quant_status else None\n",
    "\n",
    "print('Full Precision (eval-mistral-full):', 'Available' if full_results else 'Not available')\n",
    "print('Quantized (eval-mistral-quantized):', 'Available' if quant_results else 'Not available')\n",
    "if full_results and 'results' in full_results:\n",
    "    print(f'Benchmarks: {\", \".join(full_results[\"results\"].keys())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_scores(r):\n",
    "    if not r: return {}\n",
    "    res = r.get('results', {})\n",
    "    scores = {}\n",
    "    if 'arc_easy' in res: scores['ARC-Easy'] = res['arc_easy'].get('acc_norm,none', 0) * 100\n",
    "    if 'hellaswag' in res: scores['HellaSwag'] = res['hellaswag'].get('acc_norm,none', 0) * 100\n",
    "    if 'gsm8k' in res: scores['GSM8K'] = res['gsm8k'].get('exact_match,flexible-extract', 0) * 100\n",
    "    if 'truthfulqa_mc2' in res: scores['TruthfulQA'] = res['truthfulqa_mc2'].get('acc,none', 0) * 100\n",
    "    return scores\n",
    "\n",
    "full_scores = extract_scores(full_results)\n",
    "quant_scores = extract_scores(quant_results)\n",
    "\n",
    "if full_scores or quant_scores:\n",
    "    df = pd.DataFrame({'Full (4 GPUs)': full_scores, 'Quantized (1 GPU)': quant_scores})\n",
    "    if full_scores and quant_scores:\n",
    "        df['Delta (%)'] = df['Full (4 GPUs)'] - df['Quantized (1 GPU)']\n",
    "    print('\\nComparison:\\n')\n",
    "    display(df.round(2))\n",
    "else:\n",
    "    print('No results available')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' in locals() and not df.empty:\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    df[['Full (4 GPUs)', 'Quantized (1 GPU)']].plot(kind='bar', ax=ax, color=['#1f77b4', '#ff7f0e'])\n",
    "    ax.set_title('Model Quality Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "    ax.set_ylim(0, 100)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.set_xticklabels(df.index, rotation=45, ha='right')\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, fmt='%.1f%%')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' in locals() and 'Delta (%)' in df.columns:\n",
    "    print('\\nQuantization Impact:')\n",
    "    print(f'  Average Delta: {df[\"Delta (%)\"].mean():.2f}%')\n",
    "    print(f'  Max Delta: {df[\"Delta (%)\"].max():.2f}% ({df[\"Delta (%)\"].idxmax()})')\n",
    "    print(f'  Min Delta: {df[\"Delta (%)\"].min():.2f}% ({df[\"Delta (%)\"].idxmin()})')\n",
    "    print('\\nRecommendation:')\n",
    "    avg_delta = df['Delta (%)'].mean()\n",
    "    if avg_delta < 2:\n",
    "        print('  Use Quantized: Minimal quality loss, 75% GPU cost savings')\n",
    "    elif avg_delta < 5:\n",
    "        print('  Evaluate per use case: Moderate quality-cost trade-off')\n",
    "    else:\n",
    "        print('  Use Full Precision: Significant quality loss from quantization')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
