---
apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: extract-metadata
  namespace: private-ai-demo
  labels:
    app: docling-rag-pipeline
    component: metadata-extraction
spec:
  description: |
    Extract structured metadata from Docling-processed documents.
    Identifies article IDs, page numbers, section types, and citations.
  
  params:
    - name: scenario
      type: string
      description: "Scenario name"
  
  workspaces:
    - name: documents
      description: "Workspace with processed documents"
      mountPath: /workspace/documents
  
  steps:
    - name: extract
      image: python:3.11-slim
      script: |
        #!/usr/bin/env python3
        import os
        import json
        import re
        from pathlib import Path
        
        print("=" * 60)
        print("  Metadata Extraction")
        print("=" * 60)
        print()
        
        scenario = "$(params.scenario)"
        processed_dir = Path(f"/workspace/documents/processed/scenario2-{scenario}")
        
        print(f"Scenario: {scenario}")
        print(f"Input: {processed_dir}")
        print()
        
        def extract_article_ids(text):
            """Extract all article IDs (e.g., 'Art. 52', 'Article 5')"""
            patterns = [
                r'Art\.\s*\d+(?:\(\d+\))?(?:\([a-z]\))?',  # Art. 52, Art. 52(1), Art. 52(1)(a)
                r'Article\s+\d+(?:\(\d+\))?(?:\([a-z]\))?', # Article 52
            ]
            articles = []
            for pattern in patterns:
                articles.extend(re.findall(pattern, text))
            return list(set(articles))  # Remove duplicates
        
        def extract_page_numbers(text):
            """Extract page numbers from text"""
            pages = re.findall(r'page\s+(\d+)', text, re.IGNORECASE)
            return [int(p) for p in pages]
        
        def extract_annex_refs(text):
            """Extract annex references (e.g., 'Annex III', 'Annex III, point 1(a)')"""
            patterns = [
                r'Annex\s+[IVX]+(?:,\s*point\s+\d+(?:\([a-z]\))?)?',
            ]
            annexes = []
            for pattern in patterns:
                annexes.extend(re.findall(pattern, text))
            return list(set(annexes))
        
        def identify_section_type(text, filename):
            """Identify if text is recital, operative, or annex"""
            text_lower = text.lower()
            
            # Check for annex
            if 'annex' in text_lower or 'annex' in filename.lower():
                return 'annex'
            
            # Check for recital markers
            if 'whereas' in text_lower or 'recital' in text_lower:
                return 'recital'
            
            # Check for article markers
            if 'article' in text_lower or 'art.' in text_lower:
                return 'operative'
            
            # Default to operative
            return 'operative'
        
        total_articles = 0
        total_pages = 0
        total_annexes = 0
        
        # Process all .md files
        for md_file in processed_dir.glob("*.md"):
            if md_file.name.endswith("-response.json"):
                continue
            
            print(f"ðŸ“„ Processing: {md_file.name}")
            
            with open(md_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Extract metadata
            articles = extract_article_ids(content)
            pages = extract_page_numbers(content)
            annexes = extract_annex_refs(content)
            section_type = identify_section_type(content, md_file.name)
            
            metadata = {
                "filename": md_file.name,
                "source": md_file.stem,
                "scenario": scenario,
                "articles": articles,
                "pages": pages,
                "annexes": annexes,
                "section_type": section_type,
                "article_count": len(articles),
                "page_count": len(pages),
                "annex_count": len(annexes),
                "content_length": len(content),
                "word_count": len(content.split())
            }
            
            # Save metadata
            metadata_file = processed_dir / f"{md_file.stem}-metadata.json"
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2)
            
            print(f"  Articles: {len(articles)}")
            print(f"  Pages: {len(pages)}")
            print(f"  Annexes: {len(annexes)}")
            print(f"  Section: {section_type}")
            print(f"  ðŸ’¾ Saved: {metadata_file.name}")
            print()
            
            total_articles += len(articles)
            total_pages += len(pages)
            total_annexes += len(annexes)
        
        print(f"âœ… Metadata extraction complete!")
        print(f"   Total articles: {total_articles}")
        print(f"   Total pages: {total_pages}")
        print(f"   Total annexes: {total_annexes}")
        
        # Write results
        with open("$(results.articles-found.path)", 'w') as f:
            f.write(str(total_articles))
        
        with open("$(results.pages-found.path)", 'w') as f:
            f.write(str(total_pages))
  
  results:
    - name: articles-found
      description: "Total number of article references found"
    
    - name: pages-found
      description: "Total number of page references found"


