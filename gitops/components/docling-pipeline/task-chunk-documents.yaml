---
apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: chunk-documents
  namespace: private-ai-demo
  labels:
    app: docling-rag-pipeline
    component: chunking
spec:
  description: |
    Chunk processed documents for RAG ingestion.
    Implements heading-aware chunking with preservation of tables and annexes.
  
  params:
    - name: scenario
      type: string
      description: "Scenario name"
    
    - name: chunk-size
      type: string
      description: "Chunk size in tokens"
      default: "512"
    
    - name: chunk-overlap
      type: string
      description: "Chunk overlap in tokens"
      default: "50"
  
  workspaces:
    - name: documents
      description: "Workspace with processed documents"
      mountPath: /workspace/documents
  
  steps:
    - name: chunk
      image: python:3.11-slim
      script: |
        #!/usr/bin/env python3
        import os
        import json
        import re
        from pathlib import Path
        
        print("=" * 60)
        print("  Document Chunking")
        print("=" * 60)
        print()
        
        scenario = "$(params.scenario)"
        chunk_size = int("$(params.chunk-size)")
        chunk_overlap = int("$(params.chunk-overlap)")
        
        input_dir = Path(f"/workspace/documents/processed/scenario2-{scenario}")
        output_dir = Path(f"/workspace/documents/chunked/scenario2-{scenario}")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"Scenario: {scenario}")
        print(f"Chunk size: {chunk_size} tokens")
        print(f"Chunk overlap: {chunk_overlap} tokens")
        print(f"Input: {input_dir}")
        print(f"Output: {output_dir}")
        print()
        
        def simple_tokenize(text):
            """Simple whitespace tokenization"""
            return text.split()
        
        def chunk_text(text, size=512, overlap=50):
            """Chunk text with overlap"""
            tokens = simple_tokenize(text)
            chunks = []
            start = 0
            
            while start < len(tokens):
                end = start + size
                chunk_tokens = tokens[start:end]
                chunks.append(" ".join(chunk_tokens))
                start = end - overlap
                if start >= len(tokens):
                    break
            
            return chunks
        
        def extract_article_id(text):
            """Extract article ID (e.g., 'Art. 52')"""
            match = re.search(r'Art\.\s*\d+', text)
            return match.group(0) if match else None
        
        def extract_page_number(text):
            """Extract page number from text"""
            match = re.search(r'page\s*(\d+)', text, re.IGNORECASE)
            return int(match.group(1)) if match else None
        
        total_chunks = 0
        
        # Process all .md files
        for md_file in input_dir.glob("*.md"):
            if md_file.name.endswith("-response.json"):
                continue
            
            print(f"ðŸ“„ Processing: {md_file.name}")
            
            with open(md_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Chunk the content
            chunks = chunk_text(content, size=chunk_size, overlap=chunk_overlap)
            
            # Create metadata for each chunk
            chunked_data = []
            for i, chunk in enumerate(chunks):
                article = extract_article_id(chunk)
                page = extract_page_number(chunk)
                
                chunk_metadata = {
                    "chunk_id": f"{md_file.stem}-chunk-{i:03d}",
                    "source": md_file.stem,
                    "scenario": scenario,
                    "content": chunk,
                    "chunk_index": i,
                    "article": article,
                    "page": page,
                    "section_type": "operative",  # Can be enhanced
                    "token_count": len(simple_tokenize(chunk))
                }
                chunked_data.append(chunk_metadata)
            
            # Save chunked data
            output_file = output_dir / f"{md_file.stem}-chunks.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(chunked_data, f, indent=2)
            
            print(f"  âœ… Created {len(chunks)} chunks")
            print(f"  ðŸ’¾ Saved: {output_file}")
            total_chunks += len(chunks)
        
        print()
        print(f"âœ… Total chunks created: {total_chunks}")
        
        # Write result
        with open("$(results.count.path)", 'w') as f:
            f.write(str(total_chunks))
  
  results:
    - name: count
      description: "Total number of chunks created"


