---
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: mistral-24b-quantized
  namespace: private-ai-demo
  labels:
    app.kubernetes.io/part-of: vllm-serving
    app.kubernetes.io/component: inference
    app.kubernetes.io/name: mistral-24b-quantized
    app.openshift.io/runtime: python
    opendatahub.io/dashboard: "true"
    modelregistry.opendatahub.io/name: private-ai-model-registry
    modelregistry.opendatahub.io/registered-model-id: "1"
    modelregistry.opendatahub.io/model-version-id: "14"  # v1.1-quantized
  annotations:
    app.openshift.io/connects-to: '[{"apiVersion":"apps/v1","kind":"Deployment","name":"llama-stack"}]'
    openshift.io/display-name: "Mistral 24B Quantized"
    serving.kserve.io/deploymentMode: "Serverless"
    description: "Mistral Small 24B Instruct quantized (w4a16) - 1 GPU deployment"
    serving.knative.openshift.io/enablePassthrough: "true"
    sidecar.istio.io/inject: "true"
    sidecar.istio.io/rewriteAppHTTPProbers: "true"
    autoscaling.knative.dev/min-scale: "1"
    autoscaling.knative.dev/scale-to-zero-pod-retention-period: "60m"
spec:
  predictor:
    serviceAccountName: ai-workload-sa
    minReplicas: 1
    maxReplicas: 1
    model:
      runtime: vllm-cuda-runtime  # Shared runtime for all vLLM models
      modelFormat:
        name: vLLM
      # ModelCar approach: model is baked into the container image
      storageUri: "oci://image-registry.openshift-image-registry.svc:5000/private-ai-demo/mistral-24b-quantized:w4a16-2501"
      # Model-specific resources for 1-GPU quantized model
      resources:
        requests:
          cpu: "4"
          memory: 24Gi
          nvidia.com/gpu: "1"
        limits:
          cpu: "8"
          memory: 24Gi
          nvidia.com/gpu: "1"
      # Model-specific vLLM args
      args:
      - --tensor-parallel-size=1
      - --gpu-memory-utilization=0.95
      - --max-model-len=8192
      - --enable-chunked-prefill
      - --enable-prefix-caching
      - --trust-remote-code
      - --enable-auto-tool-choice
      - --tool-call-parser=mistral
    # Node placement for g6.4xlarge (1 GPU node)
    nodeSelector:
      node.kubernetes.io/instance-type: g6.4xlarge
    tolerations:
    - key: nvidia.com/gpu
      operator: Exists
      effect: NoSchedule

