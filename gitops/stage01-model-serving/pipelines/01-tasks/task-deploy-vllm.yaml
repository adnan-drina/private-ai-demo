apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: deploy-vllm-inferenceservice
  namespace: private-ai-demo
  labels:
    app.kubernetes.io/name: deploy-vllm
    app.kubernetes.io/component: pipeline-task
    app.kubernetes.io/part-of: private-ai-demo
  annotations:
    argocd.argoproj.io/sync-wave: "3"
    tekton.dev/pipelines.minVersion: "0.17.0"
    tekton.dev/tags: "deployment,kserve,vllm"
    tekton.dev/displayName: "Deploy vLLM InferenceService"
spec:
  description: >-
    This task creates or updates a KServe InferenceService for vLLM model serving.
    It deploys the model from a ModelCar OCI image with proper GPU configuration.

  params:
    - name: service_name
      type: string
      description: Name of the InferenceService
    
    - name: storage_uri
      type: string
      description: OCI image URI (e.g., "oci://quay.io/org/model:tag")
    
    - name: runtime_name
      type: string
      description: vLLM ServingRuntime name
      default: "vllm-nvidia-gpu"
    
    - name: gpu_count
      type: string
      description: Number of GPUs to request
      default: "1"
    
    - name: cpu_request
      type: string
      description: CPU request
      default: "4"
    
    - name: memory_request
      type: string
      description: Memory request
      default: "32Gi"
    
    - name: memory_limit
      type: string
      description: Memory limit
      default: "48Gi"
    
    - name: node_selector_label
      type: string
      description: Node selector label for GPU nodes (e.g., "node.kubernetes.io/instance-type=g6.4xlarge")
      default: ""
    
    - name: tensor_parallel_size
      type: string
      description: vLLM tensor parallelism size (should match GPU count)
      default: "1"

  steps:
    - name: deploy-inferenceservice
      image: registry.access.redhat.com/openshift4/ose-cli:latest
      script: |
        #!/bin/bash
        set -e
        
        echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
        echo "üöÄ Deploying vLLM InferenceService"
        echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
        echo "Service:  $(params.service_name)"
        echo "Image:    $(params.storage_uri)"
        echo "Runtime:  $(params.runtime_name)"
        echo "GPUs:     $(params.gpu_count)"
        echo "Memory:   $(params.memory_request)"
        echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
        
        # Create base InferenceService YAML
        cat > /tmp/isvc.yaml <<'ENDISVC'
        apiVersion: serving.kserve.io/v1beta1
        kind: InferenceService
        metadata:
          name: SERVICENAME_PLACEHOLDER
          namespace: private-ai-demo
          labels:
            app.kubernetes.io/name: SERVICENAME_PLACEHOLDER
            app.kubernetes.io/component: inference
            app.kubernetes.io/part-of: private-ai-demo
          annotations:
            serving.kserve.io/deploymentMode: RawDeployment
            serving.knative.openshift.io/enablePassthrough: "true"
        spec:
          predictor:
            model:
              modelFormat:
                name: vllm
              runtime: RUNTIME_PLACEHOLDER
              storageUri: STORAGEURI_PLACEHOLDER
              args:
                - --model=/models
                - --tensor-parallel-size=TENSORPARALLEL_PLACEHOLDER
                - --disable-custom-all-reduce
                - --max-model-len=4096
                - --gpu-memory-utilization=0.9
                - --trust-remote-code
              resources:
                requests:
                  cpu: "CPUREQ_PLACEHOLDER"
                  memory: "MEMREQ_PLACEHOLDER"
                  nvidia.com/gpu: "GPUCOUNT_PLACEHOLDER"
                limits:
                  memory: "MEMLIMIT_PLACEHOLDER"
                  nvidia.com/gpu: "GPUCOUNT_PLACEHOLDER"
        ENDISVC
        
        # Replace placeholders
        sed -i "s|SERVICENAME_PLACEHOLDER|$(params.service_name)|g" /tmp/isvc.yaml
        sed -i "s|RUNTIME_PLACEHOLDER|$(params.runtime_name)|g" /tmp/isvc.yaml
        sed -i "s|STORAGEURI_PLACEHOLDER|$(params.storage_uri)|g" /tmp/isvc.yaml
        sed -i "s|TENSORPARALLEL_PLACEHOLDER|$(params.tensor_parallel_size)|g" /tmp/isvc.yaml
        sed -i "s|CPUREQ_PLACEHOLDER|$(params.cpu_request)|g" /tmp/isvc.yaml
        sed -i "s|MEMREQ_PLACEHOLDER|$(params.memory_request)|g" /tmp/isvc.yaml
        sed -i "s|MEMLIMIT_PLACEHOLDER|$(params.memory_limit)|g" /tmp/isvc.yaml
        sed -i "s|GPUCOUNT_PLACEHOLDER|$(params.gpu_count)|g" /tmp/isvc.yaml
        
        # Add node selector and tolerations if specified
        if [ -n "$(params.node_selector_label)" ]; then
          LABEL_KEY=$(echo "$(params.node_selector_label)" | cut -d'=' -f1)
          LABEL_VALUE=$(echo "$(params.node_selector_label)" | cut -d'=' -f2)
          
          # Add nodeSelector and tolerations to the YAML
          python3 << ENDPYTHON
        import yaml
        with open('/tmp/isvc.yaml', 'r') as f:
            doc = yaml.safe_load(f)
        doc['spec']['predictor']['model']['nodeSelector'] = {'${LABEL_KEY}': '${LABEL_VALUE}'}
        doc['spec']['predictor']['model']['tolerations'] = [
            {'key': 'nvidia.com/gpu', 'operator': 'Exists', 'effect': 'NoSchedule'}
        ]
        with open('/tmp/isvc.yaml', 'w') as f:
            yaml.dump(doc, f)
        ENDPYTHON
        fi
        
        # Apply the InferenceService
        oc apply -f /tmp/isvc.yaml
        
        echo ""
        echo "‚è≥ Waiting for InferenceService to become Ready..."
        
        # Wait for Ready condition (with timeout)
        TIMEOUT=600
        ELAPSED=0
        INTERVAL=10
        
        while [ $ELAPSED -lt $TIMEOUT ]; do
          READY=$(oc get inferenceservice $(params.service_name) \
            -n private-ai-demo \
            -o jsonpath='{.status.conditions[?(@.type=="Ready")].status}' 2>/dev/null || echo "")
          
          if [ "$READY" = "True" ]; then
            echo "‚úÖ InferenceService is Ready!"
            
            # Get the endpoint URL
            URL=$(oc get inferenceservice $(params.service_name) \
              -n private-ai-demo \
              -o jsonpath='{.status.url}' 2>/dev/null || echo "")
            
            if [ -n "$URL" ]; then
              echo ""
              echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
              echo "‚úÖ Deployment Complete!"
              echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
              echo "Endpoint URL: $URL"
              echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
            fi
            exit 0
          fi
          
          # Show current status
          STATUS=$(oc get inferenceservice $(params.service_name) \
            -n private-ai-demo \
            -o jsonpath='{.status.conditions[?(@.type=="Ready")].message}' 2>/dev/null || echo "Initializing...")
          echo "  Status: $STATUS"
          
          sleep $INTERVAL
          ELAPSED=$((ELAPSED + INTERVAL))
        done
        
        echo ""
        echo "‚ö†Ô∏è  Timeout waiting for InferenceService to become Ready"
        echo "Check status with: oc get inferenceservice $(params.service_name) -n private-ai-demo"
        exit 1

