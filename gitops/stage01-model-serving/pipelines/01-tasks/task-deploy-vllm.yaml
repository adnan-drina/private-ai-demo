apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: deploy-vllm-inferenceservice
  namespace: private-ai-demo
  labels:
    app.kubernetes.io/name: deploy-vllm
    app.kubernetes.io/component: pipeline-task
    app.kubernetes.io/part-of: private-ai-demo
  annotations:
    argocd.argoproj.io/sync-wave: "3"
    tekton.dev/pipelines.minVersion: "0.17.0"
    tekton.dev/tags: "deployment,kserve,vllm"
    tekton.dev/displayName: "Deploy vLLM InferenceService"
spec:
  description: >-
    This task creates or updates a KServe InferenceService for vLLM model serving.
    It deploys the model from a ModelCar OCI image with proper GPU configuration.

  params:
    - name: service_name
      type: string
      description: Name of the InferenceService
    
    - name: storage_uri
      type: string
      description: OCI image URI (e.g., "oci://quay.io/org/model:tag")
    
    - name: runtime_name
      type: string
      description: vLLM ServingRuntime name
      default: "vllm-nvidia-gpu"
    
    - name: gpu_count
      type: string
      description: Number of GPUs to request
      default: "1"
    
    - name: cpu_request
      type: string
      description: CPU request
      default: "4"
    
    - name: memory_request
      type: string
      description: Memory request
      default: "32Gi"
    
    - name: memory_limit
      type: string
      description: Memory limit
      default: "48Gi"
    
    - name: node_selector_label
      type: string
      description: Node selector label for GPU nodes (e.g., "node.kubernetes.io/instance-type=g6.4xlarge")
      default: ""
    
    - name: tensor_parallel_size
      type: string
      description: vLLM tensor parallelism size (should match GPU count)
      default: "1"

  steps:
    - name: deploy-inferenceservice
      image: registry.access.redhat.com/openshift4/ose-cli:latest
      script: |
        #!/bin/bash
        set -e
        
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        echo "ğŸš€ Deploying vLLM InferenceService"
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        echo "Service:  $(params.service_name)"
        echo "Image:    $(params.storage_uri)"
        echo "Runtime:  $(params.runtime_name)"
        echo "GPUs:     $(params.gpu_count)"
        echo "Memory:   $(params.memory_request)"
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        
        # Build node selector if provided
        NODE_SELECTOR=""
        if [ -n "$(params.node_selector_label)" ]; then
          LABEL_KEY=$(echo "$(params.node_selector_label)" | cut -d'=' -f1)
          LABEL_VALUE=$(echo "$(params.node_selector_label)" | cut -d'=' -f2)
          NODE_SELECTOR="
            nodeSelector:
              ${LABEL_KEY}: \"${LABEL_VALUE}\"
            tolerations:
              - key: \"nvidia.com/gpu\"
                operator: \"Exists\"
                effect: \"NoSchedule\""
        fi
        
        # Create InferenceService manifest
        cat <<EOF | oc apply -f -
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: $(params.service_name)
  namespace: private-ai-demo
  labels:
    app.kubernetes.io/name: $(params.service_name)
    app.kubernetes.io/component: inference
    app.kubernetes.io/part-of: private-ai-demo
  annotations:
    serving.kserve.io/deploymentMode: RawDeployment
    serving.knative.openshift.io/enablePassthrough: "true"
spec:
  predictor:
    model:
      modelFormat:
        name: vllm
      runtime: $(params.runtime_name)
      storageUri: $(params.storage_uri)
      args:
        - --model=/models
        - --tensor-parallel-size=$(params.tensor_parallel_size)
        - --disable-custom-all-reduce
        - --max-model-len=4096
        - --gpu-memory-utilization=0.9
        - --trust-remote-code
      resources:
        requests:
          cpu: "$(params.cpu_request)"
          memory: "$(params.memory_request)"
          nvidia.com/gpu: "$(params.gpu_count)"
        limits:
          memory: "$(params.memory_limit)"
          nvidia.com/gpu: "$(params.gpu_count)"
      ${NODE_SELECTOR}
EOF
        
        echo ""
        echo "â³ Waiting for InferenceService to become Ready..."
        
        # Wait for Ready condition (with timeout)
        TIMEOUT=600
        ELAPSED=0
        INTERVAL=10
        
        while [ $ELAPSED -lt $TIMEOUT ]; do
          READY=$(oc get inferenceservice $(params.service_name) \
            -n private-ai-demo \
            -o jsonpath='{.status.conditions[?(@.type=="Ready")].status}' 2>/dev/null || echo "")
          
          if [ "$READY" = "True" ]; then
            echo "âœ… InferenceService is Ready!"
            
            # Get the endpoint URL
            URL=$(oc get inferenceservice $(params.service_name) \
              -n private-ai-demo \
              -o jsonpath='{.status.url}' 2>/dev/null || echo "")
            
            if [ -n "$URL" ]; then
              echo ""
              echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
              echo "âœ… Deployment Complete!"
              echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
              echo "Endpoint URL: $URL"
              echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            fi
            exit 0
          fi
          
          # Show current status
          STATUS=$(oc get inferenceservice $(params.service_name) \
            -n private-ai-demo \
            -o jsonpath='{.status.conditions[?(@.type=="Ready")].message}' 2>/dev/null || echo "Initializing...")
          echo "  Status: $STATUS"
          
          sleep $INTERVAL
          ELAPSED=$((ELAPSED + INTERVAL))
        done
        
        echo ""
        echo "âš ï¸  Timeout waiting for InferenceService to become Ready"
        echo "Check status with: oc get inferenceservice $(params.service_name) -n private-ai-demo"
        exit 1

