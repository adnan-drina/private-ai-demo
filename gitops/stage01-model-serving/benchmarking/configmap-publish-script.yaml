apiVersion: v1
kind: ConfigMap
metadata:
  name: guidellm-publish-script
  namespace: private-ai-demo
  labels:
    app.kubernetes.io/name: benchmark-publish-script
    app.kubernetes.io/component: benchmarking
    app.kubernetes.io/part-of: benchmark-testing
  annotations:
    description: "Python script for publishing GuideLLM results to Model Registry"
data:
  publish.py: |
    #!/usr/bin/env python3
    """
    Publish GuideLLM benchmark results to OpenShift AI Model Registry
    Uses only Python standard library (no external dependencies)
    Compatible with Red Hat UBI9 Python 3.11 image
    """

    import json
    import time
    import urllib.request
    import urllib.error
    import urllib.parse
    import ssl
    from pathlib import Path
    from datetime import date
    from typing import Dict, Any, Optional


    def print_header(title: str):
        """Print formatted section header"""
        print("=" * 70)
        print(f"  {title}")
        print("=" * 70)
        print()


    def wait_for_benchmark_results(results_path: Path, timeout_seconds: int = 720) -> bool:
        """Wait for benchmark results file to be completed (may take up to 12 minutes for benchmark to complete)"""
        print("‚è≥ Waiting for benchmark to complete...")
        print(f"   Looking for: {results_path}")
        print(f"   Timeout: {timeout_seconds}s ({timeout_seconds//60} minutes)")
        
        # Benchmark takes ~8-10 minutes (120s √ó 4 concurrency levels + warmup/cooldown)
        # Strategy: Wait for file to exist AND have stable size (not being written to)
        # Check every 20 seconds
        iterations = timeout_seconds // 20
        last_size = -1
        stable_count = 0
        
        for i in range(1, iterations + 1):
            if results_path.exists():
                current_size = results_path.stat().st_size
                
                # File exists and size is stable (hasn't changed for 2 checks = 40 seconds)
                if current_size > 0 and current_size == last_size:
                    stable_count += 1
                    if stable_count >= 2:
                        print(f"‚úÖ Benchmark results complete! (after {i*20}s, size: {current_size} bytes)")
                        return True
                    else:
                        print(f"   File found, checking stability... ({current_size} bytes)")
                else:
                    stable_count = 0
                    last_size = current_size
                    if i % 3 == 0:  # Print progress every minute
                        print(f"   Benchmark still running... ({i*20}s elapsed, file size: {current_size} bytes)")
            else:
                if i % 3 == 0:  # Print progress every minute
                    print(f"   Still waiting for file to appear... ({i*20}s elapsed / {timeout_seconds}s timeout)")
            
            time.sleep(20)
        
        print("‚ùå Benchmark results not completed within timeout")
        return False


    def parse_benchmark_metrics(results_path: Path) -> Optional[Dict[str, float]]:
        """Parse GuideLLM benchmark JSON and extract key metrics"""
        print("üìä Parsing benchmark metrics...")
        
        with results_path.open() as f:
            data = json.load(f)
        
        benchmarks = data.get("benchmarks", [])
        if not benchmarks:
            print("‚ùå No benchmarks found in results")
            return None
        
        # Try to find benchmark with concurrency ~5 (baseline)
        target_bench = None
        for bench in benchmarks:
            concurrency = bench.get("metrics", {}).get("request_concurrency", {}).get("successful", {}).get("mean", 0)
            if 4 <= int(concurrency) <= 6:
                target_bench = bench
                break
        
        # Fall back to first benchmark
        if not target_bench:
            print("‚ö†Ô∏è  No baseline metrics found, using first benchmark")
            target_bench = benchmarks[0]
        
        # Extract metrics
        metrics_data = target_bench.get("metrics", {})
        
        def get_metric(path: list, default=0.0) -> float:
            """Safely navigate nested dict and round to 2 decimals"""
            value = metrics_data
            for key in path:
                value = value.get(key, {})
            result = value if isinstance(value, (int, float)) else default
            return round(result, 2)
        
        metrics = {
            "throughput": get_metric(["tokens_per_second", "successful", "mean"]),
            "ttft_p50": get_metric(["time_to_first_token_ms", "successful", "median"]),
            "ttft_p90": get_metric(["time_to_first_token_ms", "successful", "percentiles", "p90"]),
            "ttft_p99": get_metric(["time_to_first_token_ms", "successful", "percentiles", "p99"]),
            "itl_p50": get_metric(["inter_token_latency_ms", "successful", "median"]),
            "itl_p90": get_metric(["inter_token_latency_ms", "successful", "percentiles", "p90"]),
            "itl_p99": get_metric(["inter_token_latency_ms", "successful", "percentiles", "p99"]),
            "latency_p50": round(get_metric(["request_latency", "successful", "median"]) / 1000, 2),
            "latency_p90": round(get_metric(["request_latency", "successful", "percentiles", "p90"]) / 1000, 2),
            "concurrency": int(get_metric(["request_concurrency", "successful", "mean"]))
        }
        
        print(json.dumps(metrics, indent=2))
        print()
        
        return metrics


    def calculate_cost(throughput: float, instance_cost_per_hour: float) -> float:
        """Calculate cost per 1K tokens"""
        tokens_per_hour = throughput * 3600
        cost_per_1k = (instance_cost_per_hour / tokens_per_hour) * 1000
        return round(cost_per_1k, 6)


    def http_request(url: str, token: str, method: str = "GET", data: Any = None) -> Dict[str, Any]:
        """Make HTTP request using urllib (built-in)"""
        headers = {
            "Authorization": f"Bearer {token}",
            "Content-Type": "application/json"
        }
        
        # Create SSL context (for HTTPS)
        ctx = ssl.create_default_context()
        ctx.check_hostname = False
        ctx.verify_mode = ssl.CERT_NONE
        
        # Prepare request
        req = urllib.request.Request(url, headers=headers, method=method)
        
        if data is not None:
            req.data = json.dumps(data).encode('utf-8')
        
        try:
            with urllib.request.urlopen(req, context=ctx, timeout=30) as response:
                return json.loads(response.read().decode('utf-8'))
        except urllib.error.HTTPError as e:
            print(f"‚ùå HTTP Error {e.code}: {e.reason}")
            print(e.read().decode('utf-8'))
            raise
        except urllib.error.URLError as e:
            print(f"‚ùå URL Error: {e.reason}")
            raise


    def find_model_and_version(registry_url: str, token: str, model_name: str, version_name: str) -> tuple:
        """Find model and version IDs in the registry"""
        print("üîç Finding model in registry...")
        
        # Get model ID
        models_url = f"{registry_url}/api/model_registry/v1alpha3/registered_models"
        models_data = http_request(models_url, token)
        
        model_id = None
        for model in models_data.get("items", []):
            if model.get("name") == model_name:
                model_id = model.get("id")
                break
        
        if not model_id:
            raise ValueError(f"Model '{model_name}' not found in registry")
        
        print(f"‚úÖ Found model (ID: {model_id})")
        
        # Get version ID
        versions_url = f"{registry_url}/api/model_registry/v1alpha3/registered_models/{model_id}/versions"
        versions_data = http_request(versions_url, token)
        
        version_id = None
        for version in versions_data.get("items", []):
            if version.get("name") == version_name:
                version_id = version.get("id")
                break
        
        if not version_id:
            raise ValueError(f"Model version '{version_name}' not found")
        
        print(f"‚úÖ Found version (ID: {version_id})")
        print()
        
        return model_id, version_id


    def publish_to_registry(
        registry_url: str,
        token: str,
        version_id: str,
        metrics: Dict[str, float],
        config: Dict[str, str]
    ):
        """Publish benchmark metrics to Model Registry as custom properties"""
        print("üìù Publishing metrics to Model Registry...")
        
        # Get current version data
        version_url = f"{registry_url}/api/model_registry/v1alpha3/model_versions/{version_id}"
        version_data = http_request(version_url, token)
        
        # Model Registry requires custom properties to be pre-defined
        # Instead, update the description field with benchmark results
        import json
        
        benchmark_summary = (
            f"GuideLLM Benchmark Results ({date.today().isoformat()}):\n"
            f"‚Ä¢ Throughput: {metrics['throughput']:.2f} tokens/sec\n"
            f"‚Ä¢ TTFT (p50/p90/p99): {metrics['ttft_p50']:.2f}/{metrics['ttft_p90']:.2f}/{metrics['ttft_p99']:.2f} ms\n"
            f"‚Ä¢ ITL (p50/p90/p99): {metrics['itl_p50']:.2f}/{metrics['itl_p90']:.2f}/{metrics['itl_p99']:.2f} ms\n"
            f"‚Ä¢ Concurrency: {metrics['concurrency']}\n"
            f"‚Ä¢ Cost: ${config.get('cost_per_1k_tokens_usd', 'N/A')} per 1K tokens\n"
            f"‚Ä¢ Instance: {config.get('instance_type', 'N/A')} ({config.get('gpu_count', 'N/A')} GPU)"
        )
        
        # Get existing description
        existing_desc = version_data.get("description", "")
        
        # Append benchmark results if not already there
        if "GuideLLM Benchmark" not in existing_desc:
            new_desc = f"{existing_desc}\n\n{benchmark_summary}".strip()
        else:
            # Replace old benchmark results
            import re
            new_desc = re.sub(
                r'GuideLLM Benchmark Results.*?(?=\n\n|\Z)',
                benchmark_summary,
                existing_desc,
                flags=re.DOTALL
            )
        
        # Create update payload (only update description, avoid custom properties)
        update_payload = {
            "description": new_desc
        }
        
        result = http_request(version_url, token, method="PATCH", data=update_payload)
        
        if result.get("id"):
            print("‚úÖ Successfully published metrics to Model Registry!")
            print()
            print("View in OpenShift AI Dashboard:")
            print(f"  Models ‚Üí Model registry ‚Üí private-ai-model-registry ‚Üí {config.get('model_name', 'model')}")
        else:
            print("‚ùå Failed to update Model Registry")
            print(json.dumps(result, indent=2))
            raise ValueError("Model Registry update failed")


    def main():
        """Main execution flow"""
        print_header("üìä PUBLISHING TO MODEL REGISTRY")
        
        # Configuration (passed as environment variables or defaults)
        import os
        
        results_file = os.getenv("BENCHMARK_FILE", "/results/mistral-24b-quantized-benchmark.json")
        registry_url = os.getenv("REGISTRY_URL", "http://private-ai-model-registry.rhoai-model-registries.svc.cluster.local:8080")
        model_name = os.getenv("MODEL_NAME", "mistral-24b-quantized")
        version_name = os.getenv("MODEL_VERSION", "v1.0-quantized")
        
        # Model configuration
        config = {
            "model_name": model_name,
            "gpu_count": os.getenv("GPU_COUNT", "1"),
            "gpu_type": os.getenv("GPU_TYPE", "NVIDIA L4"),
            "instance_type": os.getenv("INSTANCE_TYPE", "g6.4xlarge"),
            "tensor_parallel_size": os.getenv("TENSOR_PARALLEL_SIZE", "1"),
            "instance_cost_per_hour_usd": os.getenv("INSTANCE_COST", "1.84"),
            "vllm_gpu_memory_utilization": "0.95",
            "vllm_max_model_len": "8192",
            "vllm_chunked_prefill": "true",
            "vllm_prefix_caching": "true"
        }
        
        try:
            # Step 1: Wait for benchmark results
            results_path = Path(results_file)
            if not wait_for_benchmark_results(results_path):
                exit(1)
            print()
            
            # Step 2: Parse metrics
            metrics = parse_benchmark_metrics(results_path)
            if not metrics:
                exit(1)
            
            # Step 3: Calculate cost
            cost = calculate_cost(metrics["throughput"], float(config["instance_cost_per_hour_usd"]))
            print(f"üí∞ Cost: ${cost} per 1K tokens")
            print()
            
            config["cost_per_1k_tokens_usd"] = str(cost)
            
            # Step 4: Get ServiceAccount token
            token_path = Path("/var/run/secrets/kubernetes.io/serviceaccount/token")
            token = token_path.read_text().strip()
            
            # Step 5: Find model and version
            model_id, version_id = find_model_and_version(registry_url, token, model_name, version_name)
            
            # Step 6: Publish to registry
            publish_to_registry(registry_url, token, version_id, metrics, config)
            
            print()
            print_header("‚úÖ BENCHMARK & PUBLISH COMPLETE")
            
        except Exception as e:
            print(f"\n‚ùå Error: {e}")
            import traceback
            traceback.print_exc()
            exit(1)


    if __name__ == "__main__":
        main()

