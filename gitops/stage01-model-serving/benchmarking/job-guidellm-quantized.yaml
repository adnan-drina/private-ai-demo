---
apiVersion: batch/v1
kind: Job
metadata:
  name: guidellm-benchmark-quantized
  namespace: private-ai-demo
  labels:
    app: guidellm-benchmark
    model: mistral-24b-quantized
    app.kubernetes.io/name: guidellm
    app.kubernetes.io/component: benchmark
    app.kubernetes.io/part-of: benchmark-testing
  annotations:
    app.openshift.io/connects-to: '[{"apiVersion":"serving.kserve.io/v1beta1","kind":"InferenceService","name":"mistral-24b-quantized"}]'
    description: "GuideLLM benchmark for Mistral 24B Quantized + Model Registry publishing"
spec:
  ttlSecondsAfterFinished: 86400  # Keep for 24 hours
  backoffLimit: 3
  template:
    metadata:
      labels:
        app: guidellm-benchmark
        model: mistral-24b-quantized
    spec:
      serviceAccountName: ai-workload-sa
      restartPolicy: OnFailure
      
      # Step 1: Fetch tokenizer from HuggingFace
      initContainers:
      - name: fetch-tokenizer
        image: python:3.11-slim
        command: ["bash", "-c"]
        args:
          - |
            set -ex
            export HOME=/tmp
            
            echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
            echo "  üì• Fetching Tokenizer from HuggingFace"
            echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
            
            pip install --no-cache-dir "huggingface_hub>=0.23" || exit 1
            
            python - <<'PY'
            from huggingface_hub import hf_hub_download
            import os
            
            repo = "RedHatAI/Mistral-Small-24B-Instruct-2501-quantized.w4a16"
            os.makedirs("/mnt/tokenizer", exist_ok=True)
            
            for fname in ["tokenizer.json", "tokenizer.model", "config.json", "special_tokens_map.json", "tokenizer_config.json"]:
                try:
                    hf_hub_download(repo_id=repo, filename=fname, local_dir="/mnt/tokenizer", local_dir_use_symlinks=False)
                    print(f"  ‚úÖ {fname}")
                except:
                    print(f"  ‚ö†Ô∏è  {fname} not found (optional)")
            
            print("\n‚úÖ Tokenizer ready\n")
            PY
        volumeMounts:
        - name: tokenizer
          mountPath: /mnt/tokenizer
        - name: results
          mountPath: /results
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1"
      
      # Step 2: Run GuideLLM benchmark
      containers:
      - name: run-guidellm
        image: quay.io/rh-aiservices-bu/guidellm:f1f8ca8
        command: ["/bin/bash", "-c"]
        args:
          - |
            set -ex
            echo "üöÄ Starting GuideLLM benchmark..."
            
            # Use external route (works from within cluster, same as notebooks)
            CLUSTER_DOMAIN=$(oc get ingresses.config.openshift.io cluster -o jsonpath='{.spec.domain}' 2>/dev/null || echo "apps.cluster-qtvt5.qtvt5.sandbox2082.opentlc.com")
            TARGET_URL="https://mistral-24b-quantized-private-ai-demo.${CLUSTER_DOMAIN}/v1"
            echo "Target: ${TARGET_URL}"
            
            # Run benchmark and save JSON
            guidellm benchmark run \
              --target="${TARGET_URL}" \
              --model=mistral-24b-quantized \
              --processor=/mnt/tokenizer \
              --data="prompt_tokens=256,output_tokens=256,samples=500" \
              --rate-type=concurrent \
              --rate=1,5,10,25 \
              --warmup-percent=0.05 \
              --cooldown-percent=0.025 \
              --max-seconds=120 \
            --output-path=/results/mistral-24b-quantized-benchmark.json
            
            GUIDELLM_EXIT=$?
            
            # Check if benchmark succeeded
            if [ $GUIDELLM_EXIT -eq 0 ]; then
              echo "‚úÖ Benchmark complete"
              echo "üìÑ Results saved to: /results/mistral-24b-quantized-benchmark.json"
              ls -lh /results/
            else
              echo "‚ùå Benchmark failed with exit code $GUIDELLM_EXIT"
            fi
            
            exit $GUIDELLM_EXIT
        volumeMounts:
        - name: tokenizer
          mountPath: /mnt/tokenizer
          readOnly: true
        - name: results
          mountPath: /results
        resources:
          requests:
            memory: "2Gi"
            cpu: "1"
          limits:
            memory: "4Gi"
            cpu: "2"
      
      # Step 3: Publish results to Model Registry (runs as sidecar)
      - name: publish-to-registry
        image: registry.access.redhat.com/ubi9/python-311:latest
        command: ["python3", "/scripts/publish.py"]
        env:
        - name: BENCHMARK_FILE
          value: "/results/mistral-24b-quantized-benchmark.json"
        - name: MODEL_NAME
          value: "mistral-24b-quantized"
        - name: MODEL_VERSION
          value: "v1.0-quantized"
        - name: GPU_COUNT
          value: "1"
        - name: INSTANCE_TYPE
          value: "g6.4xlarge"
        - name: INSTANCE_COST_PER_HOUR_USD
          value: "1.84"
        - name: REGISTRY_URL
          value: "http://private-ai-model-registry.rhoai-model-registries.svc.cluster.local:8080"
        volumeMounts:
        - name: results
          mountPath: /results
          readOnly: true
        - name: publish-script
          mountPath: /scripts
          readOnly: true
        resources:
          requests:
            memory: "256Mi"
            cpu: "200m"
          limits:
            memory: "512Mi"
            cpu: "500m"
      
      volumes:
      - name: tokenizer
        emptyDir: {}
      - name: results
        persistentVolumeClaim:
          claimName: benchmark-quantized-results
      - name: publish-script
        configMap:
          name: guidellm-publish-script
