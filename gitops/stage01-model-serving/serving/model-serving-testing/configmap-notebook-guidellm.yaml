---
apiVersion: v1
kind: ConfigMap
metadata:
  name: notebook-guidellm-benchmark
  namespace: private-ai-demo
  labels:
    app: model-serving-testing
    app.kubernetes.io/name: guidellm-benchmark-notebook
    app.kubernetes.io/component: workbench
    app.kubernetes.io/part-of: private-ai-demo
  annotations:
    argocd.argoproj.io/sync-wave: "1"
    openshift.io/display-name: "GuideLLM Benchmark Notebook"
    description: "Interactive notebook for running GuideLLM benchmarks on deployed models"
data:
  01-guidellm-benchmark.ipynb: |
    {
     "cells": [
      {
       "cell_type": "markdown",
       "metadata": {},
       "source": [
        "# GuideLLM Benchmark Testing\n",
        "\n",
        "This notebook provides an interactive interface to run GuideLLM benchmarks on deployed vLLM models.\n",
        "\n",
        "## Benefits of Notebook vs Jobs:\n",
        "- **Interactive**: Run, modify, and re-run tests easily\n",
        "- **Visual**: See results immediately with charts and tables\n",
        "- **Flexible**: Easily adjust parameters and test scenarios\n",
        "- **Educational**: Step-by-step execution helps understand the process\n",
        "- **Shareable**: Easy to share and collaborate with team\n",
        "\n",
        "## What We'll Test:\n",
        "1. Mistral 24B Quantized (W4A16) - 1 GPU\n",
        "2. Mistral 24B Full Precision - 4 GPUs\n",
        "\n",
        "## Metrics:\n",
        "- **Throughput**: Tokens/second\n",
        "- **Latency**: Time to First Token (TTFT)\n",
        "- **Scalability**: Performance under load (concurrent requests)\n"
       ]
      },
      {
       "cell_type": "code",
       "execution_count": null,
       "metadata": {},
       "outputs": [],
       "source": [
        "# Install dependencies\n",
        "# NOTE: guidellm requires pydantic>=2, but base image has codeflare-sdk requiring pydantic<2\n",
        "# Solution: Install guidellm with --no-deps, then upgrade pydantic for guidellm to work\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "!pip install -q --upgrade pip\n",
        "# Install guidellm without dependencies\n",
        "!pip install -q --no-deps 'guidellm==0.2.1'\n",
        "# Manually install guidellm dependencies (will upgrade pydantic to v2, breaking codeflare-sdk)\n",
        "!pip install -q 'pydantic>=2.0.0' 'pydantic-settings>=2.0.0' 'loguru' 'datasets' 'ftfy>=6.0.0' 'transformers' 'pandas' 'matplotlib' 'tabulate' 'httpx' 'click' 'rich' 'openai' 'numpy' 'tqdm'"
       ]
      },
      {
       "cell_type": "code",
       "execution_count": null,
       "metadata": {},
       "outputs": [],
       "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime"
       ]
      },
      {
       "cell_type": "markdown",
       "metadata": {},
       "source": [
        "## 1. Discover Model Endpoints\n",
        "\n",
        "Find the deployed InferenceService endpoints automatically."
       ]
      },
      {
       "cell_type": "code",
       "execution_count": null,
       "metadata": {},
       "outputs": [],
       "source": [
        "import subprocess\n",
        "\n",
        "def get_model_endpoints():\n",
        "    \"\"\"Discover InferenceService endpoints\"\"\"\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            ['oc', 'get', 'inferenceservice', '-n', 'private-ai-demo', '-o', 'json'],\n",
        "            capture_output=True, text=True, check=True\n",
        "        )\n",
        "        services = json.loads(result.stdout)\n",
        "        \n",
        "        endpoints = {}\n",
        "        for svc in services.get('items', []):\n",
        "            name = svc['metadata']['name']\n",
        "            url = svc.get('status', {}).get('url', '')\n",
        "            if url:\n",
        "                endpoints[name] = url\n",
        "        \n",
        "        return endpoints\n",
        "    except Exception as e:\n",
        "        print(f\"Error discovering endpoints: {e}\")\n",
        "        return {}\n",
        "\n",
        "endpoints = get_model_endpoints()\n",
        "print(\"üìç Discovered Model Endpoints:\")\n",
        "for name, url in endpoints.items():\n",
        "    print(f\"  - {name}: {url}\")"
       ]
      },
      {
       "cell_type": "markdown",
       "metadata": {},
       "source": [
        "## 2. Configure Benchmark Scenarios\n",
        "\n",
        "Define test scenarios with different load levels."
       ]
      },
      {
       "cell_type": "code",
       "execution_count": null,
       "metadata": {},
       "outputs": [],
       "source": [
        "# Benchmark configuration\n",
        "SCENARIOS = [\n",
        "    {\"name\": \"Light Load\", \"concurrent_requests\": 1, \"max_requests\": 10},\n",
        "    {\"name\": \"Medium Load\", \"concurrent_requests\": 5, \"max_requests\": 25},\n",
        "    {\"name\": \"Heavy Load\", \"concurrent_requests\": 10, \"max_requests\": 50},\n",
        "]\n",
        "\n",
        "COMMON_PARAMS = {\n",
        "    \"max_output_tokens\": 512,\n",
        "    \"prompt\": \"Explain the benefits of model quantization in AI.\",\n",
        "}\n",
        "\n",
        "print(\"‚úÖ Configured {} test scenarios\".format(len(SCENARIOS)))"
       ]
      },
      {
       "cell_type": "markdown",
       "metadata": {},
       "source": [
        "## 3. Run Benchmarks\n",
        "\n",
        "Execute GuideLLM benchmarks for each model and scenario."
       ]
      },
      {
       "cell_type": "code",
       "execution_count": null,
       "metadata": {},
       "outputs": [],
       "source": [
        "def run_guidellm_benchmark(model_name, endpoint_url, scenario):\n",
        "    \"\"\"Run GuideLLM benchmark\"\"\"\n",
        "    print(f\"\\nüîÑ Testing {model_name} - {scenario['name']}...\")\n",
        "    \n",
        "    # Use the external HTTPS URL with /v1 suffix for OpenAI compatibility\n",
        "    # KServe InferenceServices require routing through Istio gateway\n",
        "    target_url = f\"{endpoint_url}/v1\"\n",
        "    \n",
        "    # GuideLLM v0.2.1 uses 'benchmark' command (no 'run' subcommand)\n",
        "    # Disable SSL verification for self-signed certs via environment variables\n",
        "    import os as os_module\n",
        "    import ssl\n",
        "    \n",
        "    env = os_module.environ.copy()\n",
        "    env['CURL_CA_BUNDLE'] = ''\n",
        "    env['REQUESTS_CA_BUNDLE'] = ''\n",
        "    env['SSL_CERT_FILE'] = ''\n",
        "    env['PYTHONHTTPSVERIFY'] = '0'\n",
        "    \n",
        "    cmd = [\n",
        "        'guidellm',\n",
        "        'benchmark',\n",
        "        f'--target={target_url}',\n",
        "        f'--model={model_name}',\n",
        "        f'--data=prompt_tokens=256,output_tokens=256,samples={scenario[\"max_requests\"]}',\n",
        "        f'--rate-type=concurrent',\n",
        "        f'--rate={scenario[\"concurrent_requests\"]}',\n",
        "        '--max-seconds=120',\n",
        "        '--output-path=/tmp/guidellm_results.json',\n",
        "    ]\n",
        "    \n",
        "    try:\n",
        "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=300, env=env)\n",
        "        \n",
        "        if result.returncode == 0:\n",
        "            print(f\"‚úÖ Completed successfully!\")\n",
        "            \n",
        "            # Load results from output file\n",
        "            if os.path.exists('/tmp/guidellm_results.json'):\n",
        "                with open('/tmp/guidellm_results.json', 'r') as f:\n",
        "                    data = json.load(f)\n",
        "                \n",
        "                # Extract key metrics from GuideLLM output\n",
        "                metrics = data.get('benchmarks', [{}])[0] if 'benchmarks' in data else data\n",
        "                throughput = metrics.get('request_throughput', {}).get('median', 'N/A')\n",
        "                print(f\"  Throughput: {throughput} req/s\")\n",
        "                return metrics\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è  Results file not found\")\n",
        "                return None\n",
        "        else:\n",
        "            print(f\"‚ùå Failed: {result.stderr}\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "        return None\n",
        "\n",
        "# Run benchmarks\n",
        "results = {}\n",
        "for model_name, endpoint_url in endpoints.items():\n",
        "    results[model_name] = []\n",
        "    for scenario in SCENARIOS:\n",
        "        result = run_guidellm_benchmark(model_name, endpoint_url, scenario)\n",
        "        if result:\n",
        "            result['scenario'] = scenario['name']\n",
        "            result['model'] = model_name\n",
        "            results[model_name].append(result)\n",
        "\n",
        "print(\"\\n‚úÖ All benchmarks complete!\")"
       ]
      },
      {
       "cell_type": "markdown",
       "metadata": {},
       "source": [
        "## 4. Analyze Results\n",
        "\n",
        "Compare performance metrics across models and scenarios."
       ]
      },
      {
       "cell_type": "code",
       "execution_count": null,
       "metadata": {},
       "outputs": [],
       "source": [
        "# Compile results into DataFrame\n",
        "all_results = []\n",
        "for model, tests in results.items():\n",
        "    all_results.extend(tests)\n",
        "\n",
        "df = pd.DataFrame(all_results)\n",
        "\n",
        "if not df.empty:\n",
        "    print(\"üìä BENCHMARK RESULTS\")\n",
        "    print(\"=\"*80)\n",
        "    display(df[['model', 'scenario', 'throughput', 'ttft_p50', 'ttft_p99']])\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No results to display\")"
       ]
      },
      {
       "cell_type": "markdown",
       "metadata": {},
       "source": [
        "## 5. Visualize Performance\n",
        "\n",
        "Create charts to compare model performance."
       ]
      },
      {
       "cell_type": "code",
       "execution_count": null,
       "metadata": {},
       "outputs": [],
       "source": [
        "if not df.empty:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "    \n",
        "    # Throughput comparison\n",
        "    df.pivot(index='scenario', columns='model', values='throughput').plot(\n",
        "        kind='bar', ax=axes[0], title='Throughput (tokens/s)'\n",
        "    )\n",
        "    axes[0].set_ylabel('Tokens/Second')\n",
        "    axes[0].legend(title='Model')\n",
        "    \n",
        "    # Latency comparison (TTFT p99)\n",
        "    df.pivot(index='scenario', columns='model', values='ttft_p99').plot(\n",
        "        kind='bar', ax=axes[1], title='Time to First Token P99 (ms)', color=['red', 'blue']\n",
        "    )\n",
        "    axes[1].set_ylabel('Latency (ms)')\n",
        "    axes[1].legend(title='Model')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No data to visualize\")"
       ]
      },
      {
       "cell_type": "markdown",
       "metadata": {},
       "source": [
        "## 6. Save Results to Model Registry\n",
        "\n",
        "Store benchmark results as model metadata."
       ]
      },
      {
       "cell_type": "code",
       "execution_count": null,
       "metadata": {},
       "outputs": [],
       "source": [
        "# TODO: Implement Model Registry integration\n",
        "# This will store results as custom properties on model versions\n",
        "\n",
        "print(\"üíæ Results saved locally (Model Registry integration pending)\")"
       ]
      },
      {
       "cell_type": "markdown",
       "metadata": {},
       "source": [
        "## Summary\n",
        "\n",
        "This notebook replaces the GuideLLM Job-based approach with an interactive workflow:\n",
        "\n",
        "**Advantages:**\n",
        "- ‚úÖ Interactive - modify and re-run easily\n",
        "- ‚úÖ Visual - immediate feedback with charts\n",
        "- ‚úÖ Flexible - adjust scenarios on the fly\n",
        "- ‚úÖ Educational - understand each step\n",
        "- ‚úÖ Maintainable - easier to update than Jobs\n",
        "\n",
        "**Next Steps:**\n",
        "1. Run this notebook to benchmark both models\n",
        "2. Check LM-Eval notebook for quality testing\n",
        "3. Review results in Model Registry\n"
       ]
      }
     ],
     "metadata": {
      "kernelspec": {
       "display_name": "Python 3",
       "language": "python",
       "name": "python3"
      },
      "language_info": {
       "name": "python",
       "version": "3.11.0"
      }
     },
     "nbformat": 4,
     "nbformat_minor": 4
    }

