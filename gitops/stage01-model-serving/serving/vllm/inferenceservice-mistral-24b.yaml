---
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: mistral-24b
  namespace: private-ai-demo
  labels:
    app.kubernetes.io/part-of: vllm-serving
    app.kubernetes.io/component: inference
    app.kubernetes.io/name: mistral-24b
    app.openshift.io/runtime: python
    opendatahub.io/dashboard: "true"
    modelregistry.opendatahub.io/name: private-ai-model-registry
    modelregistry.opendatahub.io/registered-model-name: "Mistral-Small-24B-Instruct"
    modelregistry.opendatahub.io/model-version: "full-fp16"
  annotations:
    argocd.argoproj.io/sync-wave: "5"
    app.openshift.io/connects-to: '[{"apiVersion":"apps/v1","kind":"Deployment","name":"llama-stack"}]'
    openshift.io/display-name: "Mistral 24B Full"
    serving.kserve.io/deploymentMode: "Serverless"
    description: "Mistral Small 24B Instruct full precision - 4 GPU (weights in S3)"
    security.opendatahub.io/enable-auth: "true"
    serving.knative.openshift.io/enablePassthrough: "true"
    sidecar.istio.io/inject: "true"
    sidecar.istio.io/rewriteAppHTTPProbers: "true"
    autoscaling.knative.dev/min-scale: "1"
    autoscaling.knative.dev/target: "4"
    autoscaling.knative.dev/scale-to-zero-pod-retention-period: "60m"
    # S3 configuration (no protocol prefix - host:port only)
    serving.kserve.io/s3-endpoint: "minio.model-storage.svc:9000"
    serving.kserve.io/s3-usehttps: "0"
    serving.kserve.io/s3-verifyssl: "0"
    serving.kserve.io/s3-region: "us-east-1"
    serving.kserve.io/s3-secret-name: "s3-credentials-kserve"
spec:
  predictor:
    containerConcurrency: 32  # Phase 1 optimization: match max_num_seqs (was 4)
    serviceAccountName: ai-workload-sa
    automountServiceAccountToken: false
    minReplicas: 1
    maxReplicas: 1
    model:
      modelFormat:
        name: vLLM
      name: ""
      runtime: vllm-cuda-runtime  # Shared runtime for all vLLM models
      # Path B: weights in MinIO (S3-compatible) populated by pipeline
      storageUri: s3://llm-models/Mistral-Small-24B-Instruct/full-fp16/
      # Model-specific resources for 4-GPU full precision model
      resources:
        requests:
          cpu: "16"
          memory: 80Gi
          nvidia.com/gpu: "4"
          ephemeral-storage: "60Gi"  # S3-backed: 48GB model + vLLM runtime + cache
        limits:
          cpu: "32"
          memory: 80Gi
          nvidia.com/gpu: "4"
          ephemeral-storage: "70Gi"
      # Model-specific vLLM args for Mistral Small 24B full precision (FP16)
      args:
        - --dtype=bfloat16                  # Use BF16 for better performance on L4 GPUs
        - --tensor-parallel-size=4          # Distribute across 4 GPUs
        - --gpu-memory-utilization=0.85     # Conservative for 4-GPU setup
        - --max-model-len=32768             # Mistral Small supports 32K context
        - --max-num-seqs=24                  # Cap sequences to reduce multi-GPU stalls
        - --enable-chunked-prefill          # Performance optimization
        - --enable-prefix-caching           # Cache optimization for repeated prompts
        - --trust-remote-code               # Allow custom model code
        - --disable-custom-all-reduce       # Required for PCIe-only multi-GPU (>2)
    # Node placement for g6.12xlarge (4 GPU node)
    nodeSelector:
      node.kubernetes.io/instance-type: g6.12xlarge
    tolerations:
    - key: nvidia.com/gpu
      operator: Exists
      effect: NoSchedule

