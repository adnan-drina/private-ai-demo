---
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: mistral-24b-quantized
  namespace: private-ai-demo
  labels:
    app.kubernetes.io/part-of: vllm-serving
    app.kubernetes.io/component: inference
    app.kubernetes.io/name: mistral-24b-quantized
    app.openshift.io/runtime: python
    opendatahub.io/dashboard: "true"
    modelregistry.opendatahub.io/name: private-ai-model-registry
    modelregistry.opendatahub.io/registered-model-name: "Mistral-Small-24B-Instruct"
    modelregistry.opendatahub.io/registered-model-id: "2"
    modelregistry.opendatahub.io/model-version: "quant-w4a16"
    modelregistry.opendatahub.io/model-version-id: "3"
  annotations:
    argocd.argoproj.io/sync-wave: "5"
    app.openshift.io/connects-to: '[{"apiVersion":"apps/v1","kind":"Deployment","name":"llama-stack"}]'
    openshift.io/display-name: "Mistral-24B-quantized"
    serving.kserve.io/deploymentMode: "Serverless"
    description: "Mistral Small 24B Instruct W4A16 quantized - 1 GPU (weights in PVC)"
    autoscaling.knative.dev/min-scale: "1"
    autoscaling.knative.dev/target: "6"
    autoscaling.knative.dev/scale-to-zero-pod-retention-period: "60m"
    opendatahub.io/accelerator-profile: nvidia-l4-1gpu
    opendatahub.io/accelerator-name: nvidia-l4-1gpu
    serving.knative.openshift.io/enablePassthrough: "true"
    serving.kserve.io/enablePrometheusScraping: "true"
    # S3 configuration per KServe: host:port + s3-usehttps controls scheme
spec:
  predictor:
    logger:
      mode: all
      url: http://trustyai-service.private-ai-demo.svc.cluster.local
    containerConcurrency: 16  # Phase 1 optimization: match max_num_seqs (was 6)
    serviceAccountName: ai-workload-sa
    automountServiceAccountToken: false
    minReplicas: 1
    maxReplicas: 1
    model:
      modelFormat:
        name: vLLM
      name: ""
      runtime: vllm-cuda-runtime
      # PVC storage populated via mirror job
      storageUri: pvc://mistral-24b-quantized-pvc/
      resources:
        requests:
          cpu: "6"
          memory: 16Gi
          nvidia.com/gpu: "1"
          ephemeral-storage: "4Gi"
        limits:
          cpu: "10"
          memory: 20Gi
          nvidia.com/gpu: "1"
          ephemeral-storage: "8Gi"
      # Model-specific vLLM args for Mistral Small 24B quantized (W4A16)
      # Quantization: compressed-tensors (auto-detected by vLLM)
      args:
        - --gpu-memory-utilization=0.70     # Allow enough headroom for KV cache at 8K context
        - --max-model-len=8192              # Keeps KV cache within 1x L4 limits
        - --max-num-seqs=16                  # Limit concurrent sequences per GPU
        - --enable-chunked-prefill          # Performance optimization
        - --enable-prefix-caching           # Cache optimization for repeated prompts
        - --trust-remote-code               # Allow custom model code
        - --dtype=auto                      # Auto-detect dtype from model config
    # Node placement for g6.4xlarge (1 GPU node) - matches nvidia-l4-1gpu hardware profile
    nodeSelector:
      node.kubernetes.io/instance-type: g6.4xlarge
    tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
