---
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: mistral-24b-quantized
  namespace: private-ai-demo
  labels:
    app.kubernetes.io/part-of: vllm-serving
    app.kubernetes.io/component: inference
    app.kubernetes.io/name: mistral-24b-quantized
    app.openshift.io/runtime: python
    opendatahub.io/dashboard: "true"
    modelregistry.opendatahub.io/registered-model-id: "4"
    modelregistry.opendatahub.io/model-version-id: "9"
    modelregistry.opendatahub.io/name: private-ai-model-registry
    modelregistry.opendatahub.io/inference-service-id: "11"
  annotations:
    argocd.argoproj.io/sync-wave: "5"
    app.openshift.io/connects-to: '[{"apiVersion":"apps/v1","kind":"Deployment","name":"llama-stack"}]'
    openshift.io/display-name: "Mistral-24B-quantized"
    serving.kserve.io/deploymentMode: "Serverless"
    description: "Mistral Small 24B Instruct W4A16 quantized - 1 GPU deployment (ModelCar OCI image)"
    serving.knative.openshift.io/enablePassthrough: "true"
    sidecar.istio.io/inject: "true"
    sidecar.istio.io/rewriteAppHTTPProbers: "true"
    security.opendatahub.io/enable-auth: "true"
spec:
  predictor:
    automountServiceAccountToken: false
    imagePullSecrets:
      - name: internal-registry-private-ai
    minReplicas: 1
    maxReplicas: 1
    model:
      modelFormat:
        name: vLLM
      name: ""
      runtime: vllm-cuda-runtime
      # OCI image from ModelCar pipeline (stored in OpenShift internal registry)
      storageUri: oci://image-registry.openshift-image-registry.svc:5000/private-ai-demo/mistral-24b-quantized:w4a16-2501
      resources:
        requests:
          cpu: "6"
          memory: 16Gi
          nvidia.com/gpu: "1"
        limits:
          cpu: "10"
          memory: 20Gi
          nvidia.com/gpu: "1"
    tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
