apiVersion: tekton.dev/v1
kind: PipelineRun
metadata:
  generateName: test-mistral-quantized-
  namespace: private-ai-demo
  labels:
    app.kubernetes.io/name: test-mistral-quantized
    app.kubernetes.io/component: pipeline-run
    app.kubernetes.io/part-of: private-ai-demo
    tekton.dev/pipeline: model-testing
  annotations:
    argocd.argoproj.io/hook: Skip
    description: "Test pipeline for Mistral 24B Quantized model (already deployed)"
spec:
  taskRunTemplate:
    serviceAccountName: model-pipeline-sa
  pipelineRef:
    name: model-testing
  
  params:
    # InferenceService (must be already deployed!)
    - name: inference_service_name
      value: "mistral-24b-quantized"
    - name: namespace
      value: "private-ai-demo"
    
    # vLLM Model Name (Option C: Explicit, no autodiscovery)
    - name: vllm_model_name
      value: "mistral-24b-quantized"  # vLLM model name (what the API knows)
    
    # Model Registry metadata (MUST match Pipeline A registration!)
    - name: model_name
      value: "Mistral-Small-24B-Instruct"  # Canonical model name (matches Pipeline A)
    - name: version_name
      value: "quantized-w4a16-1gpu"         # Version name (matches Pipeline A)
    
    # Evaluation config (fast for demo)
    - name: eval_tasks
      value: "hellaswag,arc_easy"
    - name: eval_limit
      value: "500"  # Limit samples for faster testing
    - name: max_model_len
      value: "8192"
    
    # Benchmark config (rates: 1, 5, 10 as requested)
    # Optimized for realistic chat workloads per vLLM best practices:
    # - Medium generations (256 tokens) for balanced performance
    # - Realistic prompt length (200 tokens) instead of stress-test length
    # - Allow EOS termination (configured in task, not ignore_eos)
    - name: tokenizer_name
      value: "mistralai/Mistral-Small-24B-Instruct-2501"  # HuggingFace tokenizer for synthetic data
    - name: request_count
      value: "100"
    - name: max_seconds
      value: "120"  # 2 minutes per scenario
    - name: prompt_tokens
      value: "200"  # Realistic chat prompt length (was 512)
    - name: generated_tokens
      value: "256"  # Balanced generation length (was 128, stress test was 512)
  
  workspaces:
    - name: results
      volumeClaimTemplate:
        spec:
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: 10Gi  # Small PVC for results only
          storageClassName: gp3-csi
  
  timeouts:
    pipeline: "1h30m"   # Total: eval 45min + benchmark 30min + publish 5min
    tasks: "1h"         # Individual task timeout


