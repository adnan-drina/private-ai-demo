apiVersion: tekton.dev/v1
kind: Pipeline
metadata:
  name: modelcar-build-deploy-v2
  namespace: private-ai-demo
  labels:
    app.kubernetes.io/name: modelcar-pipeline
    app.kubernetes.io/component: pipeline
    app.kubernetes.io/part-of: private-ai-demo
  annotations:
    argocd.argoproj.io/sync-wave: "4"
    tekton.dev/pipelines.minVersion: "0.17.0"
    tekton.dev/tags: "mlops,modelcar,vllm"
    tekton.dev/displayName: "ModelCar Build and Deploy Pipeline v2"
spec:
  description: >-
    Production-ready ModelCar pipeline (v2) that works for BOTH quantized and full-precision models:
    
    1. build-and-push-to-quay: Downloads model, builds image, pushes to Quay (all in one pod)
       - Uses PVC for ALL storage (no node ephemeral blow-up)
       - Pushes directly to Quay (no intermediate OCI archive)
       - Works for 80GB+ images without eviction
    
    2. mirror-to-internal: Registry-to-registry copy from Quay to OpenShift internal
       - Runs in fresh pod with fresh SA token (no expiry issues)
       - No local extraction (network copy only)
    
    3. register-model: Publishes to Model Registry
       - Reads metadata from PVC
       - Registers for deployment
    
    This design:
    - Prevents ephemeral storage exhaustion (quantized: 14GB OCI OK, full: 87GB OCI caused eviction)
    - Avoids SA token expiry (mirror task gets fresh 2h token)
    - Works for both 8GB and 48GB+ models
    - Follows production best practices (Quay as source of truth, then mirror internally)

  params:
    # Model source parameters
    - name: hf_repo
      type: string
      description: HuggingFace repository ID
    
    - name: hf_revision
      type: string
      description: HuggingFace revision to download
      default: "main"
    
    # Image and registry parameters
    - name: image_stream_name
      type: string
      description: Name of the ImageStream in OpenShift
    
    - name: image_tag
      type: string
      description: Tag for the built image
      default: "latest"
    
    - name: quay_org
      type: string
      description: Quay.io organization name
    
    - name: quay_repo
      type: string
      description: Quay.io repository name
    
    - name: quay_tag
      type: string
      description: Tag for Quay image
      default: "latest"
    
    # Model Registry parameters
    - name: model_name
      type: string
      description: Registered model name in Model Registry
    
    - name: version_name
      type: string
      description: Model version identifier
    
    - name: version_description
      type: string
      description: Description for this model version
      default: ""
    
    - name: base_model
      type: string
      description: Base model HuggingFace ID (for metadata)
      default: ""
    
    - name: quantization
      type: string
      description: Quantization type (e.g., w4a16, fp16, none)
      default: "none"
    
    - name: gpu_requirement
      type: string
      description: Number of GPUs required for deployment
      default: "1"
    
    # Internal parameters (computed)
    - name: internal_registry
      type: string
      description: OpenShift internal registry hostname
      default: "image-registry.openshift-image-registry.svc:5000"
    
    - name: namespace
      type: string
      description: Target namespace
      default: "private-ai-demo"

  workspaces:
    - name: shared-workspace
      description: Shared 500Gi PVC for model, build context, and cross-task metadata
    - name: quay-auth
      description: Quay.io authentication secret (robot account credentials)

  tasks:
    # Task 1: Download + Build + Push to Quay (all in one pod)
    # This is the ONLY heavy-I/O task. It does everything on the PVC.
    - name: build-and-push-to-quay
      timeout: "5h"  # Large models need time (full: ~3.5h observed)
      taskRef:
        name: build-and-push-to-quay
        kind: Task
      params:
        - name: hf_repo
          value: $(params.hf_repo)
        - name: hf_revision
          value: $(params.hf_revision)
        - name: quay_org
          value: $(params.quay_org)
        - name: quay_repo
          value: $(params.quay_repo)
        - name: quay_tag
          value: $(params.quay_tag)
      workspaces:
        - name: source
          workspace: shared-workspace
        - name: quay-auth
          workspace: quay-auth
    
    # Task 2: Mirror from Quay to OpenShift internal registry
    # Runs in NEW pod with FRESH service account token (no expiry)
    # Registry-to-registry copy (no local extraction, no ephemeral blow-up)
    - name: mirror-to-internal
      timeout: "1h"
      runAfter: ["build-and-push-to-quay"]
      taskRef:
        name: mirror-to-internal
        kind: Task
      params:
        - name: namespace
          value: $(params.namespace)
        - name: image_stream_name
          value: $(params.image_stream_name)
        - name: image_tag
          value: $(params.image_tag)
        - name: internal_registry
          value: $(params.internal_registry)
        - name: tlsverify
          value: "false"
      workspaces:
        - name: source
          workspace: shared-workspace
        - name: quay-auth
          workspace: quay-auth
    
    # Task 3: Register model in Model Registry
    # Uses internal registry URL (Quay URL also available in metadata if needed)
    - name: register-model
      timeout: "10m"
      runAfter: ["mirror-to-internal"]
      taskRef:
        name: register-model
        kind: Task
      params:
        - name: model_name
          value: $(params.model_name)
        - name: version_name
          value: $(params.version_name)
        - name: image_uri
          value: oci://$(params.internal_registry)/$(params.namespace)/$(params.image_stream_name):$(params.image_tag)
        - name: model_format_name
          value: "ModelCar"
        - name: model_format_version
          value: "1"
        - name: description
          value: $(params.version_description)
        - name: base_model
          value: $(params.base_model)
        - name: quantization
          value: $(params.quantization)
        - name: gpu_requirement
          value: $(params.gpu_requirement)

  finally:
    - name: cleanup-summary
      taskSpec:
        workspaces:
          - name: source
        steps:
          - name: summary
            image: registry.access.redhat.com/ubi9/ubi-minimal:latest
            script: |
              #!/bin/bash
              
              echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
              echo "ğŸ¯ ModelCar Pipeline Complete"
              echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
              
              METADATA_FILE="$(workspaces.source.path)/.pipeline-metadata/publish-metadata.json"
              
              if [ -f "$METADATA_FILE" ]; then
                echo ""
                echo "ğŸ“‹ Final Image Metadata:"
                cat "$METADATA_FILE" | sed 's/^/   /'
                echo ""
              fi
              
              echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
              echo "âœ… ModelCar built, pushed, mirrored, and registered"
              echo ""
              echo "Next steps:"
              echo "  1. Check Model Registry:"
              echo "     oc get registeredmodels -n private-ai-model-registry"
              echo ""
              echo "  2. Deploy model (separate process):"
              echo "     Create InferenceService or ServingRuntime CR"
              echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
      workspaces:
        - name: source
          workspace: shared-workspace

