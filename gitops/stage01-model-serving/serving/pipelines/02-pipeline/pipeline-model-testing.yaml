apiVersion: tekton.dev/v1
kind: Pipeline
metadata:
  name: model-testing
  namespace: private-ai-demo
  labels:
    app.kubernetes.io/name: model-testing-pipeline
    app.kubernetes.io/component: pipeline
    app.kubernetes.io/part-of: private-ai-demo
    app.kubernetes.io/version: "1.0"
  annotations:
    argocd.argoproj.io/sync-wave: "4"
    tekton.dev/pipelines.minVersion: "0.17.0"
    tekton.dev/tags: "testing,evaluation,benchmarking"
    tekton.dev/displayName: "Model Testing Pipeline (Evaluation + Benchmarking)"
spec:
  description: >-
    Tests deployed models with evaluation and benchmarking tasks.
    Inspired by Red Hat's model-car-importer approach:
    https://developers.redhat.com/articles/2025/10/06/optimize-and-deploy-llms-production-openshift-ai
    
    Tasks:
    1. run-lm-eval: Evaluate model accuracy (hellaswag, arc_easy, etc.)
    2. run-guidellm: Benchmark performance (TTFT, throughput, latency)
    3. publish-results: Store results in Model Registry
    
    Benefits:
    - Test deployed models (no download/build overhead)
    - Compare different model versions
    - Track performance over time
    - Store results in Model Registry for analysis

  params:
    # InferenceService identification
    - name: inference_service_name
      type: string
      description: Name of the deployed InferenceService to test
    
    - name: namespace
      type: string
      description: Namespace where the InferenceService is deployed
      default: "private-ai-demo"
    
    # Model Registry metadata
    - name: model_name
      type: string
      description: Registered model name in Model Registry
    
    - name: version_name
      type: string
      description: Model version identifier
    
    # Evaluation parameters
    - name: eval_tasks
      type: string
      description: Comma-separated list of evaluation tasks
      default: "hellaswag,arc_easy"
    
    - name: eval_limit
      type: string
      description: Limit number of samples per task (for faster evaluation)
      default: "500"
    
    - name: max_model_len
      type: string
      description: Maximum context length for the model
      default: "8192"
    
    # Benchmark parameters
    - name: request_count
      type: string
      description: Number of requests per load scenario
      default: "100"
    
    - name: max_seconds
      type: string
      description: Maximum time to run each scenario (seconds)
      default: "120"
    
    - name: tokenizer_name
      type: string
      description: HuggingFace tokenizer model ID for synthetic data generation
      default: ""
    
    - name: prompt_tokens
      type: string
      description: Number of tokens in prompt
      default: "512"
    
    - name: generated_tokens
      type: string
      description: Number of tokens to generate
      default: "512"

  workspaces:
    - name: results
      description: Shared workspace for storing test results

  tasks:
    # Task 1: Run LM-Eval for accuracy evaluation
    - name: run-lm-eval
      taskRef:
        name: run-lm-eval
        kind: Task
      params:
        - name: inference_service_name
          value: $(params.inference_service_name)
        - name: namespace
          value: $(params.namespace)
        - name: model_name
          value: $(params.model_name)
        - name: tasks
          value: $(params.eval_tasks)
        - name: limit
          value: $(params.eval_limit)
        - name: max_model_len
          value: $(params.max_model_len)
      workspaces:
        - name: results
          workspace: results
    
    # Task 2: Run GuideLLM for performance benchmarking
    - name: run-guidellm
      runAfter: ["run-lm-eval"]
      taskRef:
        name: run-guidellm
        kind: Task
      params:
        - name: inference_service_name
          value: $(params.inference_service_name)
        - name: namespace
          value: $(params.namespace)
        - name: model_name
          value: $(params.model_name)
        - name: tokenizer_name
          value: $(params.tokenizer_name)
        - name: request_count
          value: $(params.request_count)
        - name: max_seconds
          value: $(params.max_seconds)
        - name: prompt_tokens
          value: $(params.prompt_tokens)
        - name: generated_tokens
          value: $(params.generated_tokens)
      workspaces:
        - name: results
          workspace: results
    
    # Task 3: Publish results to Model Registry
    - name: publish-results
      runAfter: ["run-lm-eval", "run-guidellm"]
      taskRef:
        name: publish-test-results
        kind: Task
      params:
        - name: model_name
          value: $(params.model_name)
        - name: version_name
          value: $(params.version_name)
        - name: eval_results_path
          value: "/workspace/results/eval/results.json"
        - name: benchmark_results_path
          value: "/workspace/results/benchmarks/"
      workspaces:
        - name: results
          workspace: results

  results:
    - name: eval-status
      description: Evaluation task status
      value: $(tasks.run-lm-eval.results.EVAL_STATUS)
    
    - name: benchmark-status
      description: Benchmark task status
      value: $(tasks.run-guidellm.results.BENCHMARK_STATUS)
    
    - name: publish-status
      description: Publish task status
      value: $(tasks.publish-results.results.PUBLISH_STATUS)


