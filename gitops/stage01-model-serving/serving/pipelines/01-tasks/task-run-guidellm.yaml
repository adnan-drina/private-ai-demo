apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: run-guidellm
  namespace: private-ai-demo
  labels:
    app.kubernetes.io/name: run-guidellm
    app.kubernetes.io/component: pipeline-task
    app.kubernetes.io/part-of: private-ai-demo
  annotations:
    argocd.argoproj.io/sync-wave: "3"
    tekton.dev/pipelines.minVersion: "0.17.0"
    tekton.dev/tags: "benchmarking,guidellm,performance"
    tekton.dev/displayName: "Run GuideLLM Performance Benchmark"
spec:
  description: >-
    Runs GuideLLM performance benchmarking against a deployed InferenceService.
    Tests multiple load scenarios (rate: 1, 5, 10) to measure:
    - TTFT (Time to First Token)
    - Throughput (tokens/second)
    - Latency percentiles (p50, p90, p95, p99)
    
    Based on Red Hat's approach from:
    https://developers.redhat.com/articles/2025/10/06/optimize-and-deploy-llms-production-openshift-ai

  timeout: 30m0s

  params:
    - name: inference_service_name
      type: string
      description: Name of the deployed InferenceService to benchmark
    
    - name: namespace
      type: string
      description: Namespace where the InferenceService is deployed
      default: "private-ai-demo"
    
    - name: model_name
      type: string
      description: vLLM model name (must match the name registered in vLLM, e.g. 'mistral-24b-quantized')
    
    - name: tokenizer_name
      type: string
      description: HuggingFace tokenizer model ID (e.g. 'mistralai/Mistral-Small-24B-Instruct-2501')
      default: ""
    
    - name: request_count
      type: string
      description: Number of requests per load scenario
      default: "100"
    
    - name: max_seconds
      type: string
      description: Maximum time to run each scenario (seconds)
      default: "120"
    
    - name: prompt_tokens
      type: string
      description: Number of tokens in prompt
      default: "512"
    
    - name: generated_tokens
      type: string
      description: Number of tokens to generate
      default: "512"

  workspaces:
    - name: results
      description: Workspace to store benchmark results

  results:
    - name: BENCHMARK_STATUS
      description: Status of benchmark (succeeded/failed)
    
    - name: RESULTS_FILE
      description: Path to results JSON file
    
    - name: SUMMARY
      description: Summary of benchmark results

  steps:
    - name: get-inference-url
      image: registry.redhat.io/openshift4/ose-cli:latest
      script: |
        #!/bin/bash
        set -e
        
        echo "ğŸ” Discovering InferenceService URL"
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        echo "InferenceService: $(params.inference_service_name)"
        echo "Namespace: $(params.namespace)"
        echo ""
        
        # Get the InferenceService URL
        ISVC_URL=$(oc get inferenceservice $(params.inference_service_name) \
          -n $(params.namespace) \
          -o jsonpath='{.status.url}' 2>/dev/null || echo "")
        
        if [ -z "$ISVC_URL" ]; then
          echo "âŒ ERROR: InferenceService $(params.inference_service_name) not found or not ready"
          exit 1
        fi
        
        # Remove https:// prefix and /v1 suffix if present
        ISVC_URL=$(echo "$ISVC_URL" | sed 's|https://||' | sed 's|/v1||' | sed 's|/$||')
        
        echo "âœ… InferenceService URL: https://${ISVC_URL}"
        echo ""
        
        # Test connectivity
        echo "ğŸ”— Testing connectivity..."
        HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" -k "https://${ISVC_URL}/v1/models" || echo "000")
        
        if [ "$HTTP_CODE" = "200" ]; then
          echo "âœ… Model endpoint is accessible"
        else
          echo "âš ï¸  Warning: Got HTTP ${HTTP_CODE} from model endpoint"
          echo "   (Continuing anyway - endpoint may respond differently to benchmarks)"
        fi
        
        # Save URL for next step
        echo -n "${ISVC_URL}" > /workspace/results/model_url.txt
        
        echo ""
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

    - name: run-benchmarks
      image: ghcr.io/neuralmagic/guidellm:latest
      workingDir: /tmp
      env:
        - name: HOME
          value: /tmp
        - name: HF_HOME
          value: /tmp/.cache/huggingface
        - name: HF_TOKEN
          valueFrom:
            secretKeyRef:
              name: huggingface-token
              key: HF_TOKEN
        - name: PYTHONHTTPSVERIFY
          value: "0"
        - name: REQUESTS_CA_BUNDLE
          value: ""
        - name: SSL_CERT_FILE
          value: ""
      script: |
        #!/bin/bash
        set -e
        
        MODEL_URL=$(cat /workspace/results/model_url.txt)
        
        echo "ğŸš€ Running GuideLLM Performance Benchmarks"
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        echo "Model URL: https://${MODEL_URL}"
        echo "Model Name: $(params.model_name)"
        echo "Request Count: $(params.request_count)"
        echo "Max Duration: $(params.max_seconds)s per scenario"
        echo "Prompt Tokens: $(params.prompt_tokens)"
        echo "Generated Tokens: $(params.generated_tokens)"
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        echo ""
        
        # Install complete SSL bypass (4-layer pattern for httpx)
        # Use $HOME/.local for user-writable location
        echo "ğŸ”§ Installing SSL bypass for self-signed certificates..."
        mkdir -p $HOME/.local/lib/python3.13/site-packages
        SITE_PACKAGES=$HOME/.local/lib/python3.13/site-packages
        export PYTHONPATH=$SITE_PACKAGES:$PYTHONPATH
        echo 'import ssl, warnings' > $SITE_PACKAGES/sitecustomize.py
        echo 'warnings.filterwarnings("ignore")' >> $SITE_PACKAGES/sitecustomize.py
        echo '' >> $SITE_PACKAGES/sitecustomize.py
        echo '# 1. Kill SSL verification at the socket layer' >> $SITE_PACKAGES/sitecustomize.py
        echo 'ssl._create_default_https_context = ssl._create_unverified_context' >> $SITE_PACKAGES/sitecustomize.py
        echo 'def _insecure_ctx(*args, **kwargs):' >> $SITE_PACKAGES/sitecustomize.py
        echo '    ctx = ssl.SSLContext(ssl.PROTOCOL_TLS_CLIENT)' >> $SITE_PACKAGES/sitecustomize.py
        echo '    ctx.check_hostname = False' >> $SITE_PACKAGES/sitecustomize.py
        echo '    ctx.verify_mode = ssl.CERT_NONE' >> $SITE_PACKAGES/sitecustomize.py
        echo '    return ctx' >> $SITE_PACKAGES/sitecustomize.py
        echo 'ssl.create_default_context = _insecure_ctx' >> $SITE_PACKAGES/sitecustomize.py
        echo '' >> $SITE_PACKAGES/sitecustomize.py
        echo '# 2. Silence urllib3 insecure warnings' >> $SITE_PACKAGES/sitecustomize.py
        echo 'import urllib3' >> $SITE_PACKAGES/sitecustomize.py
        echo 'urllib3.disable_warnings()' >> $SITE_PACKAGES/sitecustomize.py
        echo '' >> $SITE_PACKAGES/sitecustomize.py
        echo '# 3. Force requests to ALWAYS send verify=False' >> $SITE_PACKAGES/sitecustomize.py
        echo 'import requests' >> $SITE_PACKAGES/sitecustomize.py
        echo '_orig_request = requests.Session.request' >> $SITE_PACKAGES/sitecustomize.py
        echo 'def _insecure_request(self, *args, **kwargs):' >> $SITE_PACKAGES/sitecustomize.py
        echo '    kwargs["verify"] = False' >> $SITE_PACKAGES/sitecustomize.py
        echo '    return _orig_request(self, *args, **kwargs)' >> $SITE_PACKAGES/sitecustomize.py
        echo 'requests.Session.request = _insecure_request' >> $SITE_PACKAGES/sitecustomize.py
        echo '' >> $SITE_PACKAGES/sitecustomize.py
        echo '# 4. Force httpx to ALWAYS send verify=False (for GuideLLM)' >> $SITE_PACKAGES/sitecustomize.py
        echo 'try:' >> $SITE_PACKAGES/sitecustomize.py
        echo '    import httpx' >> $SITE_PACKAGES/sitecustomize.py
        echo '    _orig_httpx_client = httpx.Client' >> $SITE_PACKAGES/sitecustomize.py
        echo '    def _insecure_httpx_client(*args, **kwargs):' >> $SITE_PACKAGES/sitecustomize.py
        echo '        kwargs["verify"] = False' >> $SITE_PACKAGES/sitecustomize.py
        echo '        return _orig_httpx_client(*args, **kwargs)' >> $SITE_PACKAGES/sitecustomize.py
        echo '    httpx.Client = _insecure_httpx_client' >> $SITE_PACKAGES/sitecustomize.py
        echo '    _orig_httpx_async = httpx.AsyncClient' >> $SITE_PACKAGES/sitecustomize.py
        echo '    def _insecure_httpx_async(*args, **kwargs):' >> $SITE_PACKAGES/sitecustomize.py
        echo '        kwargs["verify"] = False' >> $SITE_PACKAGES/sitecustomize.py
        echo '        return _orig_httpx_async(*args, **kwargs)' >> $SITE_PACKAGES/sitecustomize.py
        echo '    httpx.AsyncClient = _insecure_httpx_async' >> $SITE_PACKAGES/sitecustomize.py
        echo 'except ImportError:' >> $SITE_PACKAGES/sitecustomize.py
        echo '    pass' >> $SITE_PACKAGES/sitecustomize.py
        echo "âœ… Complete SSL bypass installed (ssl + urllib3 + requests + httpx)"
        echo ""
        
        # Create results directory in /tmp (writable)
        mkdir -p /tmp/benchmarks
        
        # Build processor argument if tokenizer_name is provided
        PROCESSOR_ARG=""
        if [ -n "$(params.tokenizer_name)" ]; then
          PROCESSOR_ARG="--processor $(params.tokenizer_name)"
        fi
        
        # Test scenarios: rate 1, 5, 10 (as per user request)
        RATES="1 5 10"
        
        for RATE in $RATES; do
          echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
          echo "ğŸ“Š Scenario: Constant load at ${RATE} req/s"
          echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
          
          guidellm benchmark \
            --target "https://${MODEL_URL}/v1" \
            --model "$(params.model_name)" \
            ${PROCESSOR_ARG} \
            --data "prompt_tokens=$(params.prompt_tokens),output_tokens=$(params.generated_tokens),samples=$(params.request_count)" \
            --rate-type constant \
            --rate ${RATE} \
            --max-seconds $(params.max_seconds) \
            --output-path "/tmp/benchmarks/rate_${RATE}.json" \
            2>&1 | tee "/tmp/benchmarks/rate_${RATE}.log"
          
          echo "âœ… Completed: Rate ${RATE} req/s"
          echo ""
        done
        
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        echo "âœ… All Benchmarks Complete!"
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        
        # Extract summary from all results
        echo ""
        echo "ğŸ“Š Results Summary:"
        echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
        
        SUMMARY=""
        for RATE in $RATES; do
          RESULT_FILE="/tmp/benchmarks/rate_${RATE}.json"
          if [ -f "$RESULT_FILE" ]; then
            # Extract key metrics (adjust jq query based on actual GuideLLM output)
            THROUGHPUT=$(jq -r '.throughput // "N/A"' "$RESULT_FILE" 2>/dev/null || echo "N/A")
            TTFT_P99=$(jq -r '.ttft_p99 // "N/A"' "$RESULT_FILE" 2>/dev/null || echo "N/A")
            
            echo "Rate ${RATE} req/s: throughput=${THROUGHPUT} tokens/s, ttft_p99=${TTFT_P99}ms"
            SUMMARY="${SUMMARY}rate_${RATE}={throughput:${THROUGHPUT},ttft_p99:${TTFT_P99}};"
          fi
        done
        
        echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
        echo ""
        
        # Copy results to workspace for downstream tasks
        echo "ğŸ“¦ Copying results to workspace..."
        if mkdir -p /workspace/results/benchmarks 2>/dev/null; then
          cp -r /tmp/benchmarks/* /workspace/results/benchmarks/ 2>/dev/null || true
          echo "âœ… Results copied to /workspace/results/benchmarks/"
        else
          echo "âš ï¸  Warning: Could not create /workspace/results/benchmarks (permission denied)"
          echo "   Results remain in /tmp/benchmarks/"
        fi
        
        # Save summary
        echo -n "$SUMMARY" > $(results.SUMMARY.path)
        echo -n "succeeded" > $(results.BENCHMARK_STATUS.path)
        echo -n "/tmp/benchmarks/" > $(results.RESULTS_FILE.path)
        
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
      
      resources:
        requests:
          memory: "2Gi"
          cpu: "1"
        limits:
          memory: "4Gi"
          cpu: "2"


