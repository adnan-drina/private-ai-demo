apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: run-guidellm
  namespace: private-ai-demo
  labels:
    app.kubernetes.io/name: run-guidellm
    app.kubernetes.io/component: pipeline-task
    app.kubernetes.io/part-of: private-ai-demo
  annotations:
    argocd.argoproj.io/sync-wave: "3"
    tekton.dev/pipelines.minVersion: "0.17.0"
    tekton.dev/tags: "benchmarking,guidellm,performance"
    tekton.dev/displayName: "Run GuideLLM Performance Benchmark"
spec:
  description: >-
    Runs GuideLLM performance benchmarking against a deployed InferenceService.
    Tests multiple load scenarios (rate: 1, 5, 10) to measure:
    - TTFT (Time to First Token)
    - Throughput (tokens/second)
    - Latency percentiles (p50, p90, p95, p99)
    
    Based on Red Hat's approach from:
    https://developers.redhat.com/articles/2025/10/06/optimize-and-deploy-llms-production-openshift-ai

  timeout: 30m0s

  params:
    - name: inference_service_name
      type: string
      description: Name of the deployed InferenceService to benchmark
    
    - name: namespace
      type: string
      description: Namespace where the InferenceService is deployed
      default: "private-ai-demo"
    
    - name: model_name
      type: string
      description: Model name for results identification
    
    - name: request_count
      type: string
      description: Number of requests per load scenario
      default: "100"
    
    - name: max_seconds
      type: string
      description: Maximum time to run each scenario (seconds)
      default: "120"
    
    - name: prompt_tokens
      type: string
      description: Number of tokens in prompt
      default: "512"
    
    - name: generated_tokens
      type: string
      description: Number of tokens to generate
      default: "512"

  workspaces:
    - name: results
      description: Workspace to store benchmark results

  results:
    - name: BENCHMARK_STATUS
      description: Status of benchmark (succeeded/failed)
    
    - name: RESULTS_FILE
      description: Path to results JSON file
    
    - name: SUMMARY
      description: Summary of benchmark results

  steps:
    - name: get-inference-url
      image: registry.access.redhat.com/openshift4/ose-cli:latest
      script: |
        #!/bin/bash
        set -e
        
        echo "ğŸ” Discovering InferenceService URL"
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        echo "InferenceService: $(params.inference_service_name)"
        echo "Namespace: $(params.namespace)"
        echo ""
        
        # Get the InferenceService URL
        ISVC_URL=$(oc get inferenceservice $(params.inference_service_name) \
          -n $(params.namespace) \
          -o jsonpath='{.status.url}' 2>/dev/null || echo "")
        
        if [ -z "$ISVC_URL" ]; then
          echo "âŒ ERROR: InferenceService $(params.inference_service_name) not found or not ready"
          exit 1
        fi
        
        # Remove https:// prefix and /v1 suffix if present
        ISVC_URL=$(echo "$ISVC_URL" | sed 's|https://||' | sed 's|/v1||' | sed 's|/$||')
        
        echo "âœ… InferenceService URL: https://${ISVC_URL}"
        echo ""
        
        # Test connectivity
        echo "ğŸ”— Testing connectivity..."
        HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" -k "https://${ISVC_URL}/v1/models" || echo "000")
        
        if [ "$HTTP_CODE" = "200" ]; then
          echo "âœ… Model endpoint is accessible"
        else
          echo "âš ï¸  Warning: Got HTTP ${HTTP_CODE} from model endpoint"
          echo "   (Continuing anyway - endpoint may respond differently to benchmarks)"
        fi
        
        # Save URL for next step
        echo -n "${ISVC_URL}" > /workspace/results/model_url.txt
        
        echo ""
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

    - name: run-benchmarks
      image: ghcr.io/neuralmagic/guidellm:latest
      workingDir: /workspace/results
      env:
        - name: HOME
          value: /tmp
      script: |
        #!/bin/bash
        set -e
        
        MODEL_URL=$(cat /workspace/results/model_url.txt)
        
        echo "ğŸš€ Running GuideLLM Performance Benchmarks"
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        echo "Model URL: https://${MODEL_URL}"
        echo "Model Name: $(params.model_name)"
        echo "Request Count: $(params.request_count)"
        echo "Max Duration: $(params.max_seconds)s per scenario"
        echo "Prompt Tokens: $(params.prompt_tokens)"
        echo "Generated Tokens: $(params.generated_tokens)"
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        echo ""
        
        # Create results directory
        mkdir -p /workspace/results/benchmarks
        
        # Test scenarios: rate 1, 5, 10 (as per user request)
        RATES="1 5 10"
        
        for RATE in $RATES; do
          echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
          echo "ğŸ“Š Scenario: Constant load at ${RATE} req/s"
          echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
          
          guidellm \
            --target "https://${MODEL_URL}/v1" \
            --model "$(params.model_name)" \
            --data-type emulated \
            --data "prompt_tokens=$(params.prompt_tokens),generated_tokens=$(params.generated_tokens)" \
            --request-count $(params.request_count) \
            --max-seconds $(params.max_seconds) \
            --load-type constant \
            --rate ${RATE} \
            --output-path "/workspace/results/benchmarks/rate_${RATE}.json" \
            2>&1 | tee "/workspace/results/benchmarks/rate_${RATE}.log"
          
          echo "âœ… Completed: Rate ${RATE} req/s"
          echo ""
        done
        
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        echo "âœ… All Benchmarks Complete!"
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        
        # Extract summary from all results
        echo ""
        echo "ğŸ“Š Results Summary:"
        echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
        
        SUMMARY=""
        for RATE in $RATES; do
          RESULT_FILE="/workspace/results/benchmarks/rate_${RATE}.json"
          if [ -f "$RESULT_FILE" ]; then
            # Extract key metrics (adjust jq query based on actual GuideLLM output)
            THROUGHPUT=$(jq -r '.throughput // "N/A"' "$RESULT_FILE" 2>/dev/null || echo "N/A")
            TTFT_P99=$(jq -r '.ttft_p99 // "N/A"' "$RESULT_FILE" 2>/dev/null || echo "N/A")
            
            echo "Rate ${RATE} req/s: throughput=${THROUGHPUT} tokens/s, ttft_p99=${TTFT_P99}ms"
            SUMMARY="${SUMMARY}rate_${RATE}={throughput:${THROUGHPUT},ttft_p99:${TTFT_P99}};"
          fi
        done
        
        echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
        echo ""
        
        # Save summary
        echo -n "$SUMMARY" > $(results.SUMMARY.path)
        echo -n "succeeded" > $(results.BENCHMARK_STATUS.path)
        echo -n "/workspace/results/benchmarks/" > $(results.RESULTS_FILE.path)
        
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
      
      resources:
        requests:
          memory: "2Gi"
          cpu: "1"
        limits:
          memory: "4Gi"
          cpu: "2"


