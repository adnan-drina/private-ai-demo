apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: register-model
  namespace: private-ai-demo
  labels:
    app.kubernetes.io/name: register-model
    app.kubernetes.io/component: pipeline-task
    app.kubernetes.io/part-of: private-ai-demo
  annotations:
    argocd.argoproj.io/sync-wave: "3"
    tekton.dev/pipelines.minVersion: "0.17.0"
    tekton.dev/tags: "model-registry,mlops"
    tekton.dev/displayName: "Register Model in Model Registry"
spec:
  description: >-
    This task registers a model in the OpenShift AI Model Registry using the
    Python SDK. It creates or updates a RegisteredModel and creates a new
    ModelVersion pointing to the ModelCar OCI image.

  params:
    - name: model_name
      type: string
      description: Name of the registered model (e.g., "Mistral-24B-Instruct")
    
    - name: version_name
      type: string
      description: Version identifier (e.g., "quantized-2501", "full-2501")
    
    - name: image_uri
      type: string
      description: OCI image URI (e.g., "oci://quay.io/org/model:tag")
    
    - name: model_format_name
      type: string
      description: Model format name
      default: "ModelCar"
    
    - name: model_format_version
      type: string
      description: Model format version
      default: "1"
    
    - name: description
      type: string
      description: Model version description
      default: ""
    
    - name: base_model
      type: string
      description: Base model name (e.g., "mistralai/Mistral-Large-Instruct-2407")
      default: ""
    
    - name: quantization
      type: string
      description: Quantization type (e.g., "w4a16", "fp16", "none")
      default: "none"
    
    - name: gpu_requirement
      type: string
      description: Number of GPUs required
      default: "1"

  stepTemplate:
    resources:
      requests:
        memory: "512Mi"
        cpu: "250m"
      limits:
        memory: "512Mi"
        cpu: "500m"

  steps:
    - name: register-via-sdk
      image: registry.access.redhat.com/ubi9/python-311:latest
      env:
        - name: MODEL_REGISTRY_HOST
          value: "private-ai-model-registry.rhoai-model-registries.svc.cluster.local"
      script: |
        #!/usr/bin/env bash
        set -euo pipefail
        
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        echo "ğŸ“ [$(date -u +%Y-%m-%dT%H:%M:%SZ)] Register Model"
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        
        # Runtime context
        echo "ğŸ” DEBUG: Runtime context"
        echo "  Task: register-model"
        echo "  Time (UTC): $(date -u +%Y-%m-%dT%H:%M:%SZ)"
        echo "  Node: $(cat /etc/hostname)"
        echo ""
        
        # Install Python SDK
        echo "ğŸ“¦ Installing model-registry Python SDK..."
        pip install --quiet model-registry==0.2.10
        
        # Run Python script using SDK
        python3 << 'EOFPYTHON'
        import os
        import sys
        from model_registry import ModelRegistry
        from datetime import datetime

        print("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        print(f"ğŸ“ [{datetime.utcnow().isoformat()}Z] Registering Model (Python SDK)")
        print("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")

        # Get parameters
        model_name = "$(params.model_name)"
        version_name = "$(params.version_name)"
        image_uri = "$(params.image_uri)"
        format_name = "$(params.model_format_name)"
        format_version = "$(params.model_format_version)"
        description = "$(params.description)" or f"ModelCar image for {model_name}"
        base_model = "$(params.base_model)"
        quantization = "$(params.quantization)"
        gpu_requirement = "$(params.gpu_requirement)"

        print("\nğŸ¯ Registration parameters:")
        print(f"  Model Name:        {model_name}")
        print(f"  Version:           {version_name}")
        print(f"  Base Model:        {base_model}")
        print(f"  Quantization:      {quantization}")
        print(f"  GPU Requirement:   {gpu_requirement}")
        print(f"  Image URI:         {image_uri}")
        print(f"  Format:            {format_name} v{format_version}")
        print("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")

        # Get Model Registry host
        registry_host = os.environ.get("MODEL_REGISTRY_HOST", "private-ai-model-registry.rhoai-model-registries.svc.cluster.local")
        
        print(f"\nğŸ”— Registry Host: {registry_host}")

        try:
            # Initialize SDK
            print("\nğŸ”Œ Connecting to Model Registry...")
            registry = ModelRegistry(
                server_address=f"http://{registry_host}",
                port=8080,
                author="GitOps-Pipeline",
                is_secure=False  # HTTP connection, no authentication needed
            )
            
            print("âœ… Connected successfully!")
            
            # Register model with SDK and rich metadata
            print(f"\nğŸ“ Registering model: {model_name} version {version_name}...")
            
            # Build metadata dictionary
            metadata = {
                "pipeline": "ModelCar",
                "container_image": image_uri,
                "format": "ModelCar",
                "deployment_type": "vLLM",
                "use_case": "inference",
            }
            
            # Add optional metadata if provided
            if base_model:
                metadata["base_model"] = base_model
                metadata["source"] = "HuggingFace"
            
            if quantization and quantization != "none":
                metadata["quantization"] = quantization
                metadata["optimization"] = "AWQ" if "w4" in quantization.lower() else "None"
            
            if gpu_requirement:
                metadata["gpu_requirement"] = gpu_requirement
                metadata["accelerator"] = "NVIDIA-GPU"
            
            print(f"\nğŸ“‹ Metadata: {metadata}")
            
            registered_model = registry.register_model(
                name=model_name,
                uri=image_uri,
                model_format_name=format_name,
                model_format_version=format_version,
                version=version_name,
                description=description,
                metadata=metadata
            )
            
            print(f"\nâœ… Model registered successfully! [{datetime.utcnow().isoformat()}Z]")
            print(f"   Registered Model ID: {registered_model.id}")
            print(f"   Model Name: {registered_model.name}")
            print(f"   Version: {version_name}")
            
            # Single-line machine-readable summary (for easy grepping)
            print(f'\nTASK3_SUMMARY {{"stage":"registered","model":"{model_name}","version":"{version_name}","image":"{image_uri}","time_utc":"{datetime.utcnow().isoformat()}Z"}}')
            
            print("\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
            print("âœ… Task 3 Complete: Model registered in Model Registry")
            print("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
            
        except Exception as e:
            print(f"\nâŒ ERROR: Registration failed [{datetime.utcnow().isoformat()}Z]", file=sys.stderr)
            print(f"Error: {str(e)}", file=sys.stderr)
            import traceback
            traceback.print_exc(file=sys.stderr)
            sys.exit(1)

        EOFPYTHON
        
        # Final PIPELINE_SUMMARY (comprehensive success receipt)
        echo ""
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        echo "PIPELINE_SUMMARY {\"pipeline\":\"modelcar-v2\",\"status\":\"success\",\"model\":\"$(params.model_name)\",\"version\":\"$(params.version_name)\",\"image\":\"$(params.image_uri)\",\"base_model\":\"$(params.base_model)\",\"quantization\":\"$(params.quantization)\",\"gpu_requirement\":\"$(params.gpu_requirement)\",\"time_utc\":\"$(date -u +%Y-%m-%dT%H:%M:%SZ)\"}"
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        echo ""
        echo "ğŸ‰ PIPELINE COMPLETE ğŸ‰"
        echo ""
        echo "âœ… All tasks succeeded:"
        echo "   1. Model downloaded from HuggingFace"
        echo "   2. ModelCar image built and pushed to Quay"
        echo "   3. Image mirrored to OpenShift internal registry"
        echo "   4. Model registered in Model Registry"
        echo ""
        echo "ğŸš€ Model is now ready for deployment!"
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
