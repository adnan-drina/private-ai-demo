apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: run-lm-eval
  namespace: private-ai-demo
  labels:
    app.kubernetes.io/name: run-lm-eval
    app.kubernetes.io/component: pipeline-task
    app.kubernetes.io/part-of: private-ai-demo
  annotations:
    argocd.argoproj.io/sync-wave: "3"
    tekton.dev/pipelines.minVersion: "0.17.0"
    tekton.dev/tags: "evaluation,lm-eval,trustyai"
    tekton.dev/displayName: "Run LM-Eval on Deployed Model"
spec:
  description: >-
    Runs language model evaluation (lm-eval) against a deployed InferenceService.
    Uses TrustyAI operator integration for standardized evaluation tasks.
    
    This task evaluates model accuracy on benchmark datasets like:
    - hellaswag: Commonsense reasoning
    - arc_easy: Science Q&A
    - gsm8k: Math problem solving
    - truthfulqa_mc2: Truthfulness evaluation
    
    Results are saved as JSON for downstream processing.

  timeout: 45m0s

  params:
    - name: inference_service_name
      type: string
      description: Name of the deployed InferenceService to evaluate
    
    - name: namespace
      type: string
      description: Namespace where the InferenceService is deployed
      default: "private-ai-demo"
    
    - name: model_name
      type: string
      description: Model name for results identification
    
    - name: tasks
      type: string
      description: Comma-separated list of evaluation tasks
      default: "hellaswag,arc_easy"
    
    - name: limit
      type: string
      description: Limit number of samples per task (for faster evaluation)
      default: "500"
    
    - name: max_model_len
      type: string
      description: Maximum context length for the model
      default: "8192"

  workspaces:
    - name: results
      description: Workspace to store evaluation results

  results:
    - name: EVAL_STATUS
      description: Status of evaluation (succeeded/failed)
    
    - name: RESULTS_FILE
      description: Path to results JSON file
    
    - name: ACCURACY_SUMMARY
      description: Summary of accuracy scores

  steps:
    - name: get-inference-url
      image: registry.redhat.io/openshift4/ose-cli:latest
      script: |
        #!/bin/bash
        set -e
        
        echo "ğŸ” Discovering InferenceService URL"
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        echo "InferenceService: $(params.inference_service_name)"
        echo "Namespace: $(params.namespace)"
        echo ""
        
        # Get the InferenceService URL
        ISVC_URL=$(oc get inferenceservice $(params.inference_service_name) \
          -n $(params.namespace) \
          -o jsonpath='{.status.url}' 2>/dev/null || echo "")
        
        if [ -z "$ISVC_URL" ]; then
          echo "âŒ ERROR: InferenceService $(params.inference_service_name) not found or not ready"
          exit 1
        fi
        
        echo "âœ… InferenceService URL: ${ISVC_URL}"
        echo ""
        
        # Test connectivity
        echo "ğŸ”— Testing connectivity..."
        HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" -k "${ISVC_URL}/v1/models" || echo "000")
        
        if [ "$HTTP_CODE" = "200" ]; then
          echo "âœ… Model endpoint is accessible"
        else
          echo "âš ï¸  Warning: Got HTTP ${HTTP_CODE} from model endpoint"
          echo "   (Continuing anyway - vLLM may respond differently)"
        fi
        
        # Save URL for next step
        echo -n "${ISVC_URL}" > /workspace/results/model_url.txt
        
        echo ""
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

    - name: create-lmevaljob
      image: registry.redhat.io/openshift4/ose-cli:latest
      script: |
        #!/bin/bash
        set -e
        
        MODEL_URL=$(cat /workspace/results/model_url.txt)
        
        echo "ğŸ§ª Creating TrustyAI LMEvalJob"
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        echo "Model URL: https://${MODEL_URL}"
        echo "Model Name: $(params.model_name)"
        echo "Tasks: $(params.tasks)"
        echo "Limit: $(params.limit) samples per task"
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        echo ""
        
        # Create LMEvalJob using TrustyAI operator (Red Hat way!)
        cat <<EOF | oc apply -f -
        apiVersion: trustyai.opendatahub.io/v1alpha1
        kind: LMEvalJob
        metadata:
          generateName: eval-$(params.inference_service_name)-
          namespace: $(params.namespace)
          labels:
            app.kubernetes.io/name: lm-eval
            app.kubernetes.io/component: trustyai-eval
            pipeline-task: "true"
        spec:
          model: local-completions
          modelArgs:
            - name: model
              value: "$(params.model_name)"
            - name: base_url
              value: "https://${MODEL_URL}/v1/completions"
            - name: num_concurrent
              value: "2"
            - name: max_retries
              value: "5"
            - name: timeout
              value: "600"
          taskList:
            taskNames: [$(echo "$(params.tasks)" | sed 's/,/", "/g' | sed 's/^/"/' | sed 's/$/"/')]
          limit: "$(params.limit)"
          batchSize: "1"
          logSamples: true
          offlineMode: "false"
        EOF
        
        # Get the created job name
        sleep 5
        JOB_NAME=$(oc get lmevaljob -n $(params.namespace) -l pipeline-task=true --sort-by=.metadata.creationTimestamp -o name | tail -1 | cut -d'/' -f2)
        echo ""
        echo "âœ… LMEvalJob created: ${JOB_NAME}"
        echo -n "${JOB_NAME}" > /workspace/results/lmevaljob_name.txt
        
        echo ""
        echo "â³ Waiting for evaluation to complete..."
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        
        # Wait for job to complete (timeout after 45 minutes)
        TIMEOUT=2700
        ELAPSED=0
        while [ $ELAPSED -lt $TIMEOUT ]; do
          STATUS=$(oc get lmevaljob ${JOB_NAME} -n $(params.namespace) -o jsonpath='{.status.state}' 2>/dev/null || echo "Unknown")
          
          if [ "$STATUS" == "Complete" ]; then
            echo "âœ… Evaluation completed successfully!"
            break
          elif [ "$STATUS" == "Failed" ]; then
            echo "âŒ Evaluation failed!"
            exit 1
          fi
          
          echo "Status: ${STATUS} (${ELAPSED}s elapsed)"
          sleep 30
          ELAPSED=$((ELAPSED + 30))
        done
        
        if [ $ELAPSED -ge $TIMEOUT ]; then
          echo "âŒ Timeout waiting for evaluation"
          exit 1
        fi
        
        echo ""
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        
        # Save results
        echo -n "succeeded" > $(results.EVAL_STATUS.path)
        echo -n "/workspace/results/eval/${JOB_NAME}.json" > $(results.RESULTS_FILE.path)
        echo -n "See LMEvalJob: ${JOB_NAME}" > $(results.ACCURACY_SUMMARY.path)


