apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: build-and-push-to-quay
  namespace: private-ai-demo
  labels:
    app.kubernetes.io/name: modelcar-pipeline
    app.kubernetes.io/component: task
  annotations:
    argocd.argoproj.io/sync-wave: "3"
    tekton.dev/pipelines.minVersion: "0.17.0"
    tekton.dev/tags: "modelcar,build,push,quay"
    tekton.dev/displayName: "Build ModelCar and Push to Quay"
spec:
  description: >-
    All-in-one task for ModelCar build and publish:
    1. Downloads model from HuggingFace to PVC
    2. Builds ModelCar image with Buildah (using PVC for storage)
    3. Pushes directly to Quay.io (no intermediate OCI archive)
    4. Writes publish metadata to PVC for downstream tasks
    
    This design keeps ALL heavy I/O on the PVC (not node ephemeral storage),
    preventing eviction for large models (48GB+ weights, 80GB+ images).

  params:
    - name: hf_repo
      type: string
      description: HuggingFace repository ID (e.g., "mistralai/Mistral-7B-Instruct-v0.3")
    
    - name: hf_revision
      type: string
      description: HuggingFace revision (branch, tag, or commit hash)
      default: "main"
    
    - name: quay_org
      type: string
      description: Quay.io organization name
    
    - name: quay_repo
      type: string
      description: Quay.io repository name
    
    - name: quay_tag
      type: string
      description: Tag for Quay image
      default: "latest"
    
    - name: target_path
      type: string
      description: Path relative to workspace where model is downloaded
      default: "models"

  workspaces:
    - name: source
      description: Shared workspace (500Gi PVC) for model, build context, and metadata
    - name: quay-auth
      description: Quay.io authentication secret (robot account credentials)
      mountPath: /workspace/quay-auth
      readOnly: true

  results:
    - name: IMAGE_URL
      description: Full Quay image URL (with tag)
    - name: IMAGE_DIGEST
      description: Image digest (sha256:...)
    - name: METADATA_PATH
      description: Path to publish-metadata.json on PVC

  steps:
    - name: download-model
      image: registry.access.redhat.com/ubi9/python-311:latest
      resources:
        requests:
          memory: "4Gi"
          cpu: "1"
        limits:
          memory: "8Gi"
          cpu: "2"
      env:
        - name: HF_TOKEN
          valueFrom:
            secretKeyRef:
              name: huggingface-token
              key: token
              optional: true
      script: |
        #!/bin/bash
        set -e
        
        START_TIME=$(date +%s)
        
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        echo "ğŸ“¥ [$(date -u +%Y-%m-%dT%H:%M:%SZ)] Step 1/4: Download Model"
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        
        # Runtime context for debugging
        echo "ğŸ” DEBUG: Runtime context"
        echo "  Task: build-and-push-to-quay / download-model"
        echo "  Time (UTC): $(date -u +%Y-%m-%dT%H:%M:%SZ)"
        echo "  Node: $(cat /etc/hostname)"
        echo "  UID/GID: $(id)"
        echo "  PWD: $(pwd)"
        echo "  Workspace mount:"
        mount | grep /workspace | sed 's/^/    /' || true
        echo ""
        
        # Pipeline parameters
        echo "ğŸ¯ Pipeline Parameters:"
        echo "  HF Repository: $(params.hf_repo)"
        echo "  HF Revision:   $(params.hf_revision)"
        echo "  Quay Org:      $(params.quay_org)"
        echo "  Quay Repo:     $(params.quay_repo)"
        echo "  Quay Tag:      $(params.quay_tag)"
        echo "  Target Path:   $(workspaces.source.path)/$(params.target_path)"
        echo ""
        
        # Resource pressure snapshot BEFORE download
        echo "ğŸ“Š Disk snapshot BEFORE download:"
        df -h /workspace / | sed 's/^/  /'
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        
        # Install HuggingFace Hub library (pin to 0.x for CLI compatibility)
        echo "ğŸ“¦ Installing dependencies..."
        pip install --quiet --no-cache-dir 'huggingface-hub<1.0'
        
        # Add PATH for huggingface-cli
        export PATH=$PATH:$HOME/.local/bin
        
        # Create target directory with setgid for cross-task access
        install -d -m 2775 -g 1000970000 "$(workspaces.source.path)/$(params.target_path)"
        
        # Download model using huggingface-cli
        huggingface-cli download \
          "$(params.hf_repo)" \
          --revision "$(params.hf_revision)" \
          --local-dir "$(workspaces.source.path)/$(params.target_path)" \
          --local-dir-use-symlinks False \
          --resume-download \
          --max-workers 20 \
          --include "*.safetensors" "*.bin" "*.json" "*.txt" "*.model" "*.py" "tokenizer*" "config*" "generation*" "special_tokens*"
        
        # Calculate stats
        END_TIME=$(date +%s)
        DOWNLOAD_TIME=$((END_TIME - START_TIME))
        MODEL_SIZE=$(du -sb "$(workspaces.source.path)/$(params.target_path)" | awk '{print $1}')
        
        echo ""
        echo "âœ… Download Complete! [$(date -u +%Y-%m-%dT%H:%M:%SZ)]"
        echo "   Model size: $(numfmt --to=iec-i --suffix=B $MODEL_SIZE)"
        echo "   Time: ${DOWNLOAD_TIME}s ($(($DOWNLOAD_TIME / 60))m $(($DOWNLOAD_TIME % 60))s)"
        echo ""
        
        # Workspace state after download
        echo "ğŸ“‚ Workspace snapshot AFTER download:"
        ls -lh $(workspaces.source.path) | sed 's/^/  /' || true
        echo ""
        echo "ğŸ“‚ Model directory:"
        ls -lh $(workspaces.source.path)/$(params.target_path) | head -15 | sed 's/^/  /' || true
        echo "   ... (showing first 15 files)"
        echo ""
        du -sh $(workspaces.source.path)/$(params.target_path) | sed 's/^/  /'
        echo ""
        
        # Resource pressure snapshot AFTER download
        echo "ğŸ“Š Disk snapshot AFTER download:"
        df -h /workspace / | sed 's/^/  /'
        echo ""
        echo "ğŸ“Š Top 5 largest dirs in workspace:"
        du -sh $(workspaces.source.path)/* 2>/dev/null | sort -hr | head -5 | sed 's/^/  /' || true
        echo ""
        
        # Write initial pipeline status
        mkdir -p $(workspaces.source.path)/.pipeline-metadata
        cat > $(workspaces.source.path)/.pipeline-metadata/pipeline-status.json <<EOF
        {
          "stage": "model_downloaded",
          "timestamp_utc": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
          "hf_repo": "$(params.hf_repo)",
          "hf_revision": "$(params.hf_revision)",
          "model_size_bytes": ${MODEL_SIZE},
          "download_time_seconds": ${DOWNLOAD_TIME},
          "node": "$(cat /etc/hostname)"
        }
        EOF
        echo "ğŸ“ Pipeline status updated: model_downloaded"
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

    - name: build-image
      image: registry.redhat.io/rhel9/buildah:latest
      resources:
        requests:
          memory: "4Gi"
          cpu: "1"
        limits:
          memory: "8Gi"
          cpu: "2"
      script: |
        #!/bin/bash
        set -e
        
        START_TIME=$(date +%s)
        
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        echo "ğŸ”¨ [$(date -u +%Y-%m-%dT%H:%M:%SZ)] Step 2/4: Build Image"
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        
        # Runtime context
        echo "ğŸ” DEBUG: Runtime context"
        echo "  Task: build-and-push-to-quay / build-image"
        echo "  Time (UTC): $(date -u +%Y-%m-%dT%H:%M:%SZ)"
        echo "  Node: $(cat /etc/hostname)"
        echo "  UID/GID: $(id)"
        echo ""
        
        # Resource pressure snapshot BEFORE build
        echo "ğŸ“Š Disk snapshot BEFORE build:"
        df -h /workspace / | sed 's/^/  /'
        free -h | sed 's/^/  /' || true
        echo ""
        
        # Configure Buildah to use PVC for ALL storage (not node ephemeral)
        export HOME=$(workspaces.source.path)/.buildah-home
        export BUILDAH_STORAGE_ROOT=$(workspaces.source.path)/.buildah-storage
        
        # Create directories with setgid for cross-task access
        install -d -m 2775 -g 1000970000 "$HOME"
        install -d -m 2775 -g 1000970000 "$BUILDAH_STORAGE_ROOT"
        
        echo "ğŸ¯ Storage configuration:"
        echo "  HOME:    $HOME"
        echo "  STORAGE: $BUILDAH_STORAGE_ROOT"
        echo "  Driver:  vfs (PVC-backed, prevents ephemeral storage blow-up)"
        echo ""
        
        # Create Containerfile
        echo "ğŸ“ Creating Containerfile..."
        cat > $(workspaces.source.path)/Containerfile << 'EOF'
        FROM registry.access.redhat.com/ubi9/ubi-micro:9.5
        COPY models /models
        USER 1001
        LABEL name="modelcar" vendor="Red Hat" version="1.0"
        CMD ["/bin/sh", "-c", "echo ModelCar ready"]
        EOF
        
        echo "âœ… Containerfile created"
        echo ""
        
        # Build image
        IMAGE_TAG="quay.io/$(params.quay_org)/$(params.quay_repo):$(params.quay_tag)"
        echo "ğŸš€ Building image: $IMAGE_TAG"
        echo ""
        
        buildah --root ${BUILDAH_STORAGE_ROOT} --storage-driver=vfs bud \
          --format oci \
          --layers=false \
          --squash \
          --isolation chroot \
          --ulimit nofile=8192:8192 \
          --tag "$IMAGE_TAG" \
          --file $(workspaces.source.path)/Containerfile \
          $(workspaces.source.path)
        
        # Get image info
        IMAGE_ID=$(buildah --root ${BUILDAH_STORAGE_ROOT} --storage-driver=vfs images -q "$IMAGE_TAG")
        
        # Calculate build time
        END_TIME=$(date +%s)
        BUILD_TIME=$((END_TIME - START_TIME))
        
        echo ""
        echo "âœ… Build Complete! [$(date -u +%Y-%m-%dT%H:%M:%SZ)]"
        echo "   Image: $IMAGE_TAG"
        echo "   ID:    $IMAGE_ID"
        echo "   Time:  ${BUILD_TIME}s ($(($BUILD_TIME / 60))m $(($BUILD_TIME % 60))s)"
        echo ""
        
        # Build store usage
        echo "ğŸ§± Buildah storage usage:"
        du -sh $(workspaces.source.path)/.buildah-home | sed 's/^/  /' || true
        du -sh $(workspaces.source.path)/.buildah-storage | sed 's/^/  /' || true
        echo ""
        buildah --root ${BUILDAH_STORAGE_ROOT} --storage-driver=vfs images | sed 's/^/  /' || true
        echo ""
        
        # Resource pressure snapshot AFTER build
        echo "ğŸ“Š Disk snapshot AFTER build:"
        df -h /workspace / | sed 's/^/  /'
        echo ""
        echo "ğŸ“Š Top 5 largest dirs:"
        du -sh $(workspaces.source.path)/* 2>/dev/null | sort -hr | head -5 | sed 's/^/  /' || true
        echo ""
        
        # Update pipeline status
        cat > $(workspaces.source.path)/.pipeline-metadata/pipeline-status.json <<EOF
        {
          "stage": "image_built",
          "timestamp_utc": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
          "image_tag": "${IMAGE_TAG}",
          "image_id": "${IMAGE_ID}",
          "build_time_seconds": ${BUILD_TIME},
          "node": "$(cat /etc/hostname)"
        }
        EOF
        echo "ğŸ“ Pipeline status updated: image_built"
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        
        # Save image tag for next step
        echo -n "$IMAGE_TAG" > /tmp/image-tag

    - name: push-to-quay
      image: registry.redhat.io/rhel9/buildah:latest
      resources:
        requests:
          memory: "2Gi"
          cpu: "500m"
        limits:
          memory: "4Gi"
          cpu: "1"
      script: |
        #!/bin/bash
        set -e
        
        START_TIME=$(date +%s)
        
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        echo "ğŸ“¤ [$(date -u +%Y-%m-%dT%H:%M:%SZ)] Step 3/4: Push to Quay"
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        
        # Runtime context
        echo "ğŸ” DEBUG: Runtime context"
        echo "  Task: build-and-push-to-quay / push-to-quay"
        echo "  Time (UTC): $(date -u +%Y-%m-%dT%H:%M:%SZ)"
        echo "  Node: $(cat /etc/hostname)"
        echo ""
        
        # Re-configure Buildah storage (same as build step)
        export HOME=$(workspaces.source.path)/.buildah-home
        export BUILDAH_STORAGE_ROOT=$(workspaces.source.path)/.buildah-storage
        
        IMAGE_TAG=$(cat /tmp/image-tag)
        echo "ğŸ¯ Push configuration:"
        echo "  Image: $IMAGE_TAG"
        echo "  Auth:  /workspace/quay-auth/config.json"
        echo "  Destination: docker://$IMAGE_TAG"
        echo ""
        
        # Push directly to Quay (no intermediate OCI archive!)
        buildah --root ${BUILDAH_STORAGE_ROOT} --storage-driver=vfs push \
          --authfile /workspace/quay-auth/config.json \
          "$IMAGE_TAG" \
          "docker://$IMAGE_TAG"
        
        # Get digest from Quay and verify
        echo ""
        echo "ğŸ” Verifying push and capturing digest..."
        echo "ğŸ“ Pushed to Quay (skopeo inspect):"
        skopeo inspect --authfile /workspace/quay-auth/config.json \
          "docker://$IMAGE_TAG" | jq '{Name: .Name, Digest: .Digest, Created: .Created, Size: .Size}' | sed 's/^/  /'
        
        DIGEST=$(skopeo inspect --authfile /workspace/quay-auth/config.json \
          "docker://$IMAGE_TAG" | jq -r '.Digest')
        
        # Calculate push time
        END_TIME=$(date +%s)
        PUSH_TIME=$((END_TIME - START_TIME))
        
        echo ""
        echo "âœ… Push Complete! [$(date -u +%Y-%m-%dT%H:%M:%SZ)]"
        echo "   URL:    $IMAGE_TAG"
        echo "   Digest: $DIGEST"
        echo "   Time:   ${PUSH_TIME}s ($(($PUSH_TIME / 60))m $(($PUSH_TIME % 60))s)"
        echo ""
        
        # Update pipeline status with Quay push
        cat > $(workspaces.source.path)/.pipeline-metadata/pipeline-status.json <<EOF
        {
          "stage": "quay_pushed",
          "timestamp_utc": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
          "quay_image": "${IMAGE_TAG}",
          "quay_digest": "${DIGEST}",
          "quay_org": "$(params.quay_org)",
          "quay_repo": "$(params.quay_repo)",
          "quay_tag": "$(params.quay_tag)",
          "push_time_seconds": ${PUSH_TIME},
          "node": "$(cat /etc/hostname)"
        }
        EOF
        echo "ğŸ“ Pipeline status updated: quay_pushed"
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        
        # Save for results
        echo -n "$IMAGE_TAG" > /tmp/image-url
        echo -n "$DIGEST" > /tmp/image-digest

    - name: write-metadata
      image: registry.access.redhat.com/ubi9/ubi-minimal:latest
      resources:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "512Mi"
          cpu: "500m"
      script: |
        #!/bin/bash
        set -e
        
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        echo "ğŸ“ [$(date -u +%Y-%m-%dT%H:%M:%SZ)] Step 4/4: Write Metadata"
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        
        IMAGE_URL=$(cat /tmp/image-url)
        IMAGE_DIGEST=$(cat /tmp/image-digest)
        TIMESTAMP=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
        
        # Create metadata directory with setgid
        install -d -m 2775 -g 1000970000 "$(workspaces.source.path)/.pipeline-metadata"
        
        # Write JSON metadata for downstream tasks
        cat > $(workspaces.source.path)/.pipeline-metadata/publish-metadata.json << EOF
        {
          "image_quay": "${IMAGE_URL}",
          "digest": "${IMAGE_DIGEST}",
          "tag": "$(params.quay_tag)",
          "quay_org": "$(params.quay_org)",
          "quay_repo": "$(params.quay_repo)",
          "hf_repo": "$(params.hf_repo)",
          "hf_revision": "$(params.hf_revision)",
          "pushed_at_utc": "${TIMESTAMP}"
        }
        EOF
        
        echo "âœ… Metadata written to: $(workspaces.source.path)/.pipeline-metadata/publish-metadata.json"
        echo ""
        cat $(workspaces.source.path)/.pipeline-metadata/publish-metadata.json | sed 's/^/  /'
        echo ""
        
        # Final workspace snapshot
        echo "ğŸ“‚ Final workspace state:"
        ls -lh $(workspaces.source.path) | sed 's/^/  /' || true
        echo ""
        echo "ğŸ“Š Final disk usage:"
        df -h /workspace / | sed 's/^/  /'
        echo ""
        echo "ğŸ“Š Final top 5 largest dirs:"
        du -sh $(workspaces.source.path)/* 2>/dev/null | sort -hr | head -5 | sed 's/^/  /' || true
        echo ""
        
        # Single-line machine-readable summary (for easy grepping)
        echo "TASK1_SUMMARY {\"stage\":\"quay_pushed\",\"image\":\"${IMAGE_URL}\",\"digest\":\"${IMAGE_DIGEST}\",\"hf_repo\":\"$(params.hf_repo)\",\"time_utc\":\"${TIMESTAMP}\"}"
        
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        echo "âœ… Task 1 Complete: Model downloaded, image built and pushed to Quay"
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        
        # Write results
        echo -n "$IMAGE_URL" > $(results.IMAGE_URL.path)
        echo -n "$IMAGE_DIGEST" > $(results.IMAGE_DIGEST.path)
        echo -n "$(workspaces.source.path)/.pipeline-metadata/publish-metadata.json" > $(results.METADATA_PATH.path)

