apiVersion: tekton.dev/v1
kind: PipelineRun
metadata:
  generateName: test-mistral-quantized-v2-
  namespace: private-ai-demo
  labels:
    model: mistral-quantized
    pipeline: model-testing-v2
    tekton.dev/pipeline: model-testing-v2
spec:
  pipelineRef:
    name: model-testing-v2
  
  params:
    # InferenceService identification
    - name: inference_service_name
      value: "mistral-24b-quantized"
    - name: namespace
      value: "private-ai-demo"
    
    # Model Registry metadata
    - name: model_name
      value: "Mistral-Small-24B-Instruct"
    - name: version_name
      value: "quant-w4a16"
    
    # vLLM model name
    - name: vllm_model_name
      value: "mistral-24b-quantized"
    
    # Tokenizer (HuggingFace model ID for downloading tokenizer)
    - name: tokenizer_name
      value: "mistralai/Mistral-Small-Instruct-2409"
    
    # Evaluation parameters
    - name: eval_tasks
      value: "arc_easy,hellaswag"
    - name: eval_limit
      value: "100"
    
    # GuideLLM benchmark parameters (Phase 1 Optimization)
    - name: prompt_tokens
      value: "512"      # Same input size for comparison
    - name: generated_tokens
      value: "512"      # Same output size for comparison
    - name: rates
      value: "1 3 5"    # Quantized: 1, 3, 5 rps
  
  workspaces:
    - name: results
      volumeClaimTemplate:
        spec:
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: 2Gi
  
  taskRunTemplate:
    podTemplate:
      securityContext:
        fsGroup: 1001130000  # Within namespace supplemental-groups range for affinity-assistant compatibility
  
  timeouts:
    pipeline: "2h"
    tasks: "1h30m"

