apiVersion: tekton.dev/v1
kind: PipelineRun
metadata:
  generateName: test-mistral-quantized-v2-
  namespace: private-ai-demo
  labels:
    model: mistral-24b-quantized
    pipeline: model-testing-v2
    version: quant-w4a16
  annotations:
    argocd.argoproj.io/sync-wave: "5"
    description: Test quantized Mistral model with TOP 24 property schema and friendly names
spec:
  pipelineRef:
    name: model-testing-v2
  params:
    - name: inference_service_name
      value: mistral-24b-quantized
    - name: namespace
      value: private-ai-demo
    - name: model_name
      value: Mistral-Small-24B-Instruct
    - name: version_name
      value: quant-w4a16
    - name: vllm_model_name
      value: mistral-24b-quantized
    - name: tokenizer_name
      value: mistralai/Mistral-Small-Instruct-2409
    - name: eval_tasks
      value: "arc_easy,hellaswag"
    - name: eval_limit
      value: "100"
    - name: guidellm_target
      value: "http://mistral-24b-quantized-predictor.private-ai-demo.svc.cluster.local/v1"
    - name: guidellm_max_requests
      value: "50"
    # Phase 1 Optimization: Model-specific GuideLLM parameters
    - name: request_count
      value: "150"      # Increased from 100 for better statistical significance
    - name: max_seconds
      value: "180"      # 3 minutes (was 120s)
    - name: prompt_tokens
      value: "256"      # Reduced from 512 (more realistic for 1-GPU)
    - name: generated_tokens
      value: "384"      # Adjusted for quantized model capacity
  timeouts:
    pipeline: "3h"
    tasks: "2h30m"
  workspaces:
    - name: results
      volumeClaimTemplate:
        spec:
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: 10Gi
          storageClassName: gp3-csi

