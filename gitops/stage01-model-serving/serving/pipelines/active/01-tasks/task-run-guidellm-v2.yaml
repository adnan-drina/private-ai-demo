apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: run-guidellm-v2
  namespace: private-ai-demo
  labels:
    app.kubernetes.io/name: run-guidellm-v2
    app.kubernetes.io/version: "2.0"
  annotations:
    description: "v2: Internal HTTP with service mesh (requires Istio sidecar injection for STRICT mTLS)"
spec:
  description: |
    Runs GuideLLM benchmarking against deployed InferenceService using internal HTTP.
    Requires Istio sidecar injection when STRICT mTLS is enabled.
    Uses service mesh for mTLS (no external CA bundles needed).

  params:
    - name: inference_service_name
      type: string
    - name: namespace
      type: string
      default: "private-ai-demo"
    - name: model_name
      type: string
    - name: tokenizer_name
      type: string
      default: ""
    - name: request_count
      type: string
      default: "100"
    - name: max_seconds
      type: string
      default: "120"
    - name: prompt_tokens
      type: string
      default: "256"  # Reduced from 512 to avoid streaming timeouts
    - name: generated_tokens
      type: string
      default: "256"  # Reduced from 512 to avoid streaming timeouts
    - name: rates
      type: string
      default: "1 5 10"

  workspaces:
    - name: results

  results:
    - name: BENCHMARK_STATUS
    - name: RESULTS_DIR

  steps:
    - name: get-inference-url
      image: registry.redhat.io/openshift4/ose-cli:latest
      script: |
        #!/bin/bash
        set -e
        
        echo "ðŸ” Discovering External HTTPS Knative Service URL"
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        ISVC_NAME="$(params.inference_service_name)-predictor"
        ROUTE_URL=$(oc get ksvc "$ISVC_NAME" -n "$(params.namespace)" -o jsonpath='{.status.url}' 2>/dev/null || echo "")
        if [ -z "$ROUTE_URL" ]; then
          echo "âŒ ERROR: Could not resolve external route for $ISVC_NAME"
          exit 1
        fi
        echo "âœ… External URL: ${ROUTE_URL}"
        echo -n "${ROUTE_URL}" > /workspace/results/inference_url.txt
    
    - name: run-benchmark
      image: registry.access.redhat.com/ubi9/python-311:1-77
      workingDir: /workspace/results
      env:
        - name: HF_TOKEN
          valueFrom:
            secretKeyRef:
              name: huggingface-token
              key: HF_TOKEN
      script: |
        #!/bin/bash
        set -euo pipefail
        
        MODEL_URL=$(cat /workspace/results/inference_url.txt)
        
        echo "ðŸš€ Running GuideLLM Benchmarking"
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        echo "Model URL: ${MODEL_URL}"
        echo "Protocol: HTTPS (external route with cluster CA)"
        echo ""
        
        # Install PyPI deps with default CA
        pip install --quiet --no-cache-dir guidellm requests
        echo "âœ… Installation complete"
        echo ""
        
        # Smoke test: verify /v1/models endpoint responds
        echo "ðŸ” Smoke test: GET /v1/models..."
        if (curl -sSk -f -m 10 "${MODEL_URL}/v1/models" > /dev/null); then
          echo "âœ… Endpoint reachable"
        else
          echo "âŒ Endpoint not reachable - aborting benchmark"
          exit 1
        fi
        echo ""
        
        RESULTS_DIR="/workspace/results/benchmarks"
        mkdir -p "${RESULTS_DIR}"
        
        # Warmup to prime caches (non-fatal)
        echo "ðŸ”¥ Warmup: 20 requests at 5 rps"
        # Use injected trusted CA bundle and disable verification similar to main loop
        export REQUESTS_CA_BUNDLE=/var/run/ca/ca.crt
        export SSL_CERT_FILE=/var/run/ca/ca.crt
        export CURL_CA_BUNDLE=/var/run/ca/ca.crt
        printf '%s\n' \
          "import ssl, warnings" \
          "warnings.filterwarnings('ignore')" \
          "ssl._create_default_https_context = ssl._create_unverified_context" \
          "def _insecure_ctx(*args, **kwargs):" \
          "    ctx = ssl.SSLContext(ssl.PROTOCOL_TLS_CLIENT)" \
          "    ctx.check_hostname = False" \
          "    ctx.verify_mode = ssl.CERT_NONE" \
          "    return ctx" \
          "ssl.create_default_context = _insecure_ctx" \
          "import urllib3" \
          "urllib3.disable_warnings()" \
          "import requests" \
          "_orig_request = requests.Session.request" \
          "def _insecure_request(self, *args, **kwargs):" \
          "    kwargs['verify'] = False" \
          "    return _orig_request(self, *args, **kwargs)" \
          "requests.Session.request = _insecure_request" \
          "try:" \
          "    import httpx" \
          "    _orig_httpx_client = httpx.Client" \
          "    def _insecure_httpx_client(*args, **kwargs):" \
          "        kwargs['verify'] = False" \
          "        kwargs['http2'] = False" \
          "        return _orig_httpx_client(*args, **kwargs)" \
          "    httpx.Client = _insecure_httpx_client" \
          "    _orig_httpx_async = httpx.AsyncClient" \
          "    def _insecure_httpx_async(*args, **kwargs):" \
          "        kwargs['verify'] = False" \
          "        kwargs['http2'] = False" \
          "        return _orig_httpx_async(*args, **kwargs)" \
          "    httpx.AsyncClient = _insecure_httpx_async" \
          "except Exception:" \
          "    pass" \
        > /workspace/results/sitecustomize.py
        export PYTHONPATH="/workspace/results:${PYTHONPATH:-}"
        export PYTHONWARNINGS=ignore
        export PYTHONHTTPSVERIFY=0

        # Define PROCESSOR_ARG for warmup
        TOKENIZER="$(params.tokenizer_name)"
        if [ -z "$TOKENIZER" ]; then
          TOKENIZER="$(params.model_name)"
        fi
        PROCESSOR_ARG=""
        if [ -n "$TOKENIZER" ]; then
          PROCESSOR_ARG="--processor $TOKENIZER"
        fi

        set +e
        guidellm benchmark \
          --target "${MODEL_URL}/v1" \
          --model "$(params.model_name)" \
          ${PROCESSOR_ARG} \
          --data "prompt_tokens=$(params.prompt_tokens),output_tokens=$(params.generated_tokens),samples=20" \
          --rate-type constant \
          --rate 5 \
          --max-seconds 60 \
          --output-path "${RESULTS_DIR}/warmup.json" \
          >/dev/null 2>&1 || true
        set -e
        
        # Track failures
        FAILED_BENCHMARKS=0
        TOTAL_BENCHMARKS=0
        
        for RATE in $(params.rates); do
          TOTAL_BENCHMARKS=$((TOTAL_BENCHMARKS + 1))
          echo "ðŸ“Š Benchmark: ${RATE} req/s"
          OUTPUT_FILE="${RESULTS_DIR}/rate_${RATE}.json"
          # Use injected trusted CA bundle directly (no file writes)
          export REQUESTS_CA_BUNDLE=/var/run/ca/ca.crt
          export SSL_CERT_FILE=/var/run/ca/ca.crt
          export CURL_CA_BUNDLE=/var/run/ca/ca.crt
          # Force SSL verification OFF globally (requests + httpx) via sitecustomize
          printf '%s\n' \
            "import ssl, warnings" \
            "warnings.filterwarnings('ignore')" \
            "ssl._create_default_https_context = ssl._create_unverified_context" \
            "def _insecure_ctx(*args, **kwargs):" \
            "    ctx = ssl.SSLContext(ssl.PROTOCOL_TLS_CLIENT)" \
            "    ctx.check_hostname = False" \
            "    ctx.verify_mode = ssl.CERT_NONE" \
            "    return ctx" \
            "ssl.create_default_context = _insecure_ctx" \
            "import urllib3" \
            "urllib3.disable_warnings()" \
            "import requests" \
            "_orig_request = requests.Session.request" \
            "def _insecure_request(self, *args, **kwargs):" \
            "    kwargs['verify'] = False" \
            "    return _orig_request(self, *args, **kwargs)" \
            "requests.Session.request = _insecure_request" \
            "try:" \
            "    import httpx" \
            "    _orig_httpx_client = httpx.Client" \
            "    def _insecure_httpx_client(*args, **kwargs):" \
            "        kwargs['verify'] = False" \
            "        kwargs['http2'] = False" \
            "        return _orig_httpx_client(*args, **kwargs)" \
            "    httpx.Client = _insecure_httpx_client" \
            "    _orig_httpx_async = httpx.AsyncClient" \
            "    def _insecure_httpx_async(*args, **kwargs):" \
            "        kwargs['verify'] = False" \
            "        kwargs['http2'] = False" \
            "        return _orig_httpx_async(*args, **kwargs)" \
            "    httpx.AsyncClient = _insecure_httpx_async" \
            "except Exception:" \
            "    pass" \
          > /workspace/results/sitecustomize.py
          export PYTHONPATH="/workspace/results:${PYTHONPATH:-}"
          export PYTHONWARNINGS=ignore
          export PYTHONHTTPSVERIFY=0

          set +e
          PT=$(params.prompt_tokens)
          GT=$(params.generated_tokens)

          guidellm benchmark \
            --target "${MODEL_URL}/v1" \
            --model "$(params.model_name)" \
            ${PROCESSOR_ARG} \
            --data "prompt_tokens=${PT},output_tokens=${GT},samples=$(( ${RATE} * 150 ))" \
            --rate-type constant \
            --rate ${RATE} \
            --max-seconds 300 \
            --output-path "${OUTPUT_FILE}" \
            2>&1 | tee "${RESULTS_DIR}/rate_${RATE}.log"
          RC=$?
          set -e
          if [ $RC -ne 0 ]; then
            echo "âš ï¸  Benchmark at rate ${RATE} failed (rc=$RC); writing placeholder JSON."
            echo '{"error":"guidellm_failed","rate":'"${RATE}"'}' > "${OUTPUT_FILE}" || true
            FAILED_BENCHMARKS=$((FAILED_BENCHMARKS + 1))
          fi
          
          echo "âœ… Complete: ${RATE} req/s"
          echo ""
        done
        
        echo ""
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        echo "ðŸ“Š BENCHMARK SUMMARY"
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        echo "Total benchmarks:  ${TOTAL_BENCHMARKS}"
        echo "Successful:        $((TOTAL_BENCHMARKS - FAILED_BENCHMARKS))"
        echo "Failed:            ${FAILED_BENCHMARKS}"
        echo ""
        
        # Validate output files
        echo "ðŸ” Validating output files..."
        INVALID_FILES=0
        for RATE in $(params.rates); do
          OUTPUT_FILE="${RESULTS_DIR}/rate_${RATE}.json"
          if [ -f "$OUTPUT_FILE" ]; then
            if grep -q '"error":"guidellm_failed"' "$OUTPUT_FILE"; then
              echo "   âŒ rate_${RATE}.json: Placeholder (benchmark failed)"
              INVALID_FILES=$((INVALID_FILES + 1))
            elif ! python3 -m json.tool "$OUTPUT_FILE" > /dev/null 2>&1; then
              echo "   âŒ rate_${RATE}.json: Invalid JSON"
              INVALID_FILES=$((INVALID_FILES + 1))
            else
              echo "   âœ… rate_${RATE}.json: Valid"
            fi
          else
            echo "   âŒ rate_${RATE}.json: Missing"
            INVALID_FILES=$((INVALID_FILES + 1))
          fi
        done
        echo ""
        
        echo "${RESULTS_DIR}" > $(results.RESULTS_DIR.path)
        
        # Fail the task if ANY benchmarks failed
        if [ $FAILED_BENCHMARKS -gt 0 ] || [ $INVALID_FILES -gt 0 ]; then
          echo "âŒ TASK FAILED: ${FAILED_BENCHMARKS} benchmark(s) failed, ${INVALID_FILES} invalid file(s)"
          echo "failed" > $(results.BENCHMARK_STATUS.path)
          exit 1
        else
          echo "âœ… ALL BENCHMARKS SUCCEEDED"
          echo "succeeded" > $(results.BENCHMARK_STATUS.path)
        fi
        
        echo ""
        echo "âœ… Clean HTTP communication via service mesh"

