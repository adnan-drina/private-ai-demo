apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: publish-test-results
  namespace: private-ai-demo
  labels:
    app.kubernetes.io/name: publish-test-results
    app.kubernetes.io/component: pipeline-task
    app.kubernetes.io/part-of: private-ai-demo
  annotations:
    argocd.argoproj.io/sync-wave: "3"
    tekton.dev/pipelines.minVersion: "0.17.0"
    tekton.dev/tags: "model-registry,results"
    tekton.dev/displayName: "Publish Test Results to Model Registry"
spec:
  description: >-
    Publishes evaluation and benchmark results to Model Registry as custom properties.
    This enables tracking model performance over time and comparing different versions.

  timeout: 10m0s

  params:
    - name: model_name
      type: string
      description: Registered model name in Model Registry
    
    - name: version_name
      type: string
      description: Model version identifier
    
    - name: eval_results_path
      type: string
      description: Path to lm-eval results JSON
      default: ""
    
    - name: benchmark_results_path
      type: string
      description: Path to GuideLLM benchmark results directory
      default: ""

  workspaces:
    - name: results
      description: Workspace containing test results

  results:
    - name: PUBLISH_STATUS
      description: Status of publishing (succeeded/failed)

  steps:
    - name: get-model-registry-endpoint
      image: registry.redhat.io/openshift4/ose-cli:latest
      script: |
        #!/bin/bash
        set -e
        
        echo "ğŸ” Discovering Model Registry endpoint"
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        
        # Get Model Registry route (try both http and https route names)
        MR_ROUTE=$(oc get route private-ai-model-registry-http -n rhoai-model-registries \
          -o jsonpath='{.spec.host}' 2>/dev/null || \
          oc get route private-ai-model-registry -n rhoai-model-registries \
          -o jsonpath='{.spec.host}' 2>/dev/null || echo "")
        
        if [ -z "$MR_ROUTE" ]; then
          echo "âŒ ERROR: Model Registry route not found"
          exit 1
        fi
        
        MR_HOST="http://${MR_ROUTE}"
        echo "âœ… Model Registry: ${MR_HOST}"
        echo -n "${MR_HOST}" > /workspace/results/mr_host.txt
        
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

    - name: publish-results
      image: registry.access.redhat.com/ubi9/python-311:latest
      workingDir: /workspace/results
      script: |
        #!/usr/bin/env python3
        import json
        import os
        import sys
        import urllib.request
        import urllib.error
        import ssl
        
        # Read Model Registry host
        with open('/workspace/results/mr_host.txt', 'r') as f:
            mr_host = f.read().strip()
        
        print("ğŸ”„ Publishing Results to Model Registry")
        print("=" * 60)
        print(f"Model Registry: {mr_host}")
        print(f"Model: $(params.model_name)")
        print(f"Version: $(params.version_name)")
        print("=" * 60)
        print()
        
        # Create SSL context that doesn't verify certificates
        ctx = ssl.create_default_context()
        ctx.check_hostname = False
        ctx.verify_mode = ssl.CERT_NONE
        
        # Helper function to make API calls
        def api_call(method, url, data=None):
            headers = {'Content-Type': 'application/json'}
            req = urllib.request.Request(
                url, 
                data=json.dumps(data).encode() if data else None,
                headers=headers,
                method=method
            )
            try:
                with urllib.request.urlopen(req, context=ctx) as response:
                    return json.loads(response.read().decode())
            except urllib.error.HTTPError as e:
                print(f"âš ï¸  HTTP Error {e.code}: {e.reason}")
                return None
            except Exception as e:
                print(f"âš ï¸  Error: {str(e)}")
                return None
        
        # Find registered model
        print("ğŸ” Finding registered model...")
        models_url = f"{mr_host}/api/model_registry/v1alpha3/registered_models"
        models = api_call('GET', models_url)
        
        if not models:
            print("âŒ Failed to fetch models")
            sys.exit(1)
        
        target_model = None
        for model in models.get('items', []):
            if model.get('name') == '$(params.model_name)':
                target_model = model
                break
        
        if not target_model:
            print(f"âŒ Model '$(params.model_name)' not found in registry")
            sys.exit(1)
        
        model_id = target_model['id']
        print(f"âœ… Found model: {model_id}")
        
        # Find model version
        print(f"ğŸ” Finding model version '$(params.version_name)'...")
        versions_url = f"{mr_host}/api/model_registry/v1alpha3/registered_models/{model_id}/versions"
        versions = api_call('GET', versions_url)
        
        if not versions:
            print("âŒ Failed to fetch versions")
            sys.exit(1)
        
        target_version = None
        for version in versions.get('items', []):
            if version.get('name') == '$(params.version_name)':
                target_version = version
                break
        
        if not target_version:
            print(f"âŒ Version '$(params.version_name)' not found")
            sys.exit(1)
        
        version_id = target_version['id']
        print(f"âœ… Found version: {version_id}")
        print()
        
        # Publish evaluation results
        eval_path = '$(params.eval_results_path)'
        if eval_path and os.path.exists(eval_path):
            print("ğŸ“Š Publishing evaluation results...")
            try:
                with open(eval_path, 'r') as f:
                    eval_data = json.load(f)
                
                # Extract metrics and publish as custom properties
                for task_name, task_results in eval_data.get('results', {}).items():
                    for metric_name, metric_value in task_results.items():
                        if isinstance(metric_value, (int, float)):
                            prop_data = {
                                'customProperties': {
                                    f'eval_{task_name}_{metric_name}': str(metric_value)
                                }
                            }
                            # Update version with property
                            update_url = f"{mr_host}/api/model_registry/v1alpha3/model_versions/{version_id}"
                            api_call('PATCH', update_url, prop_data)
                            print(f"  âœ… {task_name}.{metric_name} = {metric_value}")
                
                print("âœ… Evaluation results published")
            except Exception as e:
                print(f"âš ï¸  Failed to publish eval results: {str(e)}")
        
        # Publish benchmark results
        benchmark_path = '$(params.benchmark_results_path)'
        if benchmark_path and os.path.exists(benchmark_path):
            print()
            print("ğŸ“Š Publishing benchmark results...")
            try:
                # Process each rate scenario
                for rate_file in os.listdir(benchmark_path):
                    if rate_file.endswith('.json'):
                        with open(os.path.join(benchmark_path, rate_file), 'r') as f:
                            bench_data = json.load(f)
                        
                        rate = rate_file.replace('rate_', '').replace('.json', '')
                        prefix = f'benchmark_rate_{rate}'
                        
                        # Extract metrics
                        metrics = {
                            f'{prefix}_throughput': bench_data.get('throughput', 'N/A'),
                            f'{prefix}_ttft_p99': bench_data.get('ttft_p99', 'N/A'),
                            f'{prefix}_latency_p99': bench_data.get('latency_p99', 'N/A'),
                        }
                        
                        for metric_name, metric_value in metrics.items():
                            if metric_value != 'N/A':
                                prop_data = {
                                    'customProperties': {
                                        metric_name: str(metric_value)
                                    }
                                }
                                update_url = f"{mr_host}/api/model_registry/v1alpha3/model_versions/{version_id}"
                                api_call('PATCH', update_url, prop_data)
                                print(f"  âœ… {metric_name} = {metric_value}")
                
                print("âœ… Benchmark results published")
            except Exception as e:
                print(f"âš ï¸  Failed to publish benchmark results: {str(e)}")
        
        print()
        print("=" * 60)
        print("âœ… Results Publishing Complete!")
        print("=" * 60)
        
        # Save status
        with open('$(results.PUBLISH_STATUS.path)', 'w') as f:
            f.write('succeeded')
      
      resources:
        requests:
          memory: "512Mi"
          cpu: "500m"
        limits:
          memory: "1Gi"
          cpu: "1"


