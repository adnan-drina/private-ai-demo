---
apiVersion: v1
kind: ConfigMap
metadata:
  name: llama-stack-playground-sitecustomize
  namespace: private-ai-demo
  labels:
    app.kubernetes.io/name: llama-stack-playground
    app.kubernetes.io/component: rag-ui
    app.kubernetes.io/part-of: serving
data:
  sitecustomize.py: |
    """
    Compatibility shim so the legacy Streamlit playground (built against Llama Stack 0.2)
    can talk to the Llama Stack 0.3.x runtime.
    """
    TARGET_VERSION = "0.3.0"
    from types import SimpleNamespace

    def _ensure_dict(obj):
        try:
            from llama_stack_client._wrappers import DataWrapper
            if isinstance(obj, DataWrapper):
                return obj.data
        except Exception:
            pass
        return obj

    def _is_not_found_error(exc):
        status = getattr(exc, "status_code", None)
        if status == 404:
            return True
        status = getattr(exc, "status", None)
        if status == 404:
            return True
        response = getattr(exc, "response", None)
        if response is not None:
            status = getattr(response, "status_code", None)
            if status == 404:
                return True
        message = str(exc)
        return "Error code: 404" in message or "'detail': 'Not Found'" in message

    def _patch():
        import importlib

        try:
            version_mod = importlib.import_module("llama_stack_client._version")
        except ModuleNotFoundError:
            return

        version_mod.__version__ = TARGET_VERSION
        if not getattr(version_mod, "__title__", None):
            version_mod.__title__ = "llama-stack-client"

        try:
            client_mod = importlib.import_module("llama_stack_client")
        except ModuleNotFoundError:
            client_mod = None
        else:
            client_mod.__version__ = TARGET_VERSION
            try:
                original_init = client_mod.LlamaStackClient.__init__
            except (AttributeError, TypeError):
                original_init = None
            if original_init and not getattr(client_mod.LlamaStackClient, "_compat_init_patched", False):
                def compat_init(self, *args, **kwargs):
                    original_init(self, *args, **kwargs)
                    try:
                        self._custom_headers["X-LlamaStack-Client-Version"] = TARGET_VERSION
                    except Exception:
                        pass
                    try:
                        self.default_headers["X-LlamaStack-Client-Version"] = TARGET_VERSION
                    except Exception:
                        pass
                client_mod.LlamaStackClient.__init__ = compat_init
                client_mod.LlamaStackClient._compat_init_patched = True

        try:
            base_client = importlib.import_module("llama_stack_client._base_client")
        except ModuleNotFoundError:
            return

        BaseClient = base_client.BaseClient
        if getattr(BaseClient, "_compat_patched", False):
            return

        original_prepare_url = BaseClient._prepare_url
        original_build_request = BaseClient._build_request

        path_rewrites = {
            "/v1/inference/chat-completion": "/v1/chat/completions",
            "/v1/inference/chat-completions": "/v1/chat/completions",
            "/v1/vector-dbs": "/v1/vector_stores",
            "/v1/vector-dbs/": "/v1/vector_stores/",
            "/v1/vector-io/query": "/v1/vector-io/query",
            "/v1/vector-io/insert": "/v1/vector-io/insert",
        }

        def compat_prepare_url(self, url):
            if isinstance(url, str):
                for old, new in path_rewrites.items():
                    if url.startswith(old):
                        url = new + url[len(old):]
                        break
            return original_prepare_url(self, url)

        def compat_build_request(self, options, *, retries_taken=0):
            url = options.url or ""
            json_data = _ensure_dict(options.json_data)
            params = options.params

            if isinstance(json_data, dict):
                if url.startswith("/v1/inference/chat-completion") or url.startswith("/v1/chat/completions"):
                    if "model_id" in json_data and "model" not in json_data:
                        json_data["model"] = json_data.pop("model_id")
                if url.startswith("/v1/vector-dbs") or url.startswith("/v1/vector_stores"):
                    if "vector_db_id" in json_data and "vector_store_id" not in json_data:
                        json_data["vector_store_id"] = json_data.pop("vector_db_id")
                    if "provider_vector_db_id" in json_data and "vector_store_id" not in json_data:
                        json_data.setdefault("vector_store_id", json_data["provider_vector_db_id"])
            if isinstance(params, dict):
                if "vector_db_id" in params and "vector_store_id" not in params:
                    params["vector_store_id"] = params.pop("vector_db_id")

            options.json_data = json_data if json_data is not None else options.json_data
            options.params = params
            return original_build_request(self, options, retries_taken=retries_taken)

        BaseClient._prepare_url = compat_prepare_url
        BaseClient._build_request = compat_build_request
        BaseClient._compat_patched = True

        try:
            from llama_stack_client.resources.inference import InferenceResource
        except ModuleNotFoundError:
            return

        if getattr(InferenceResource.chat_completion, "_compat_patched", False):
            return

        original_chat_completion = InferenceResource.chat_completion

        class _LegacyCompletionResponse:
            def __init__(self, raw):
                self._raw = raw
                message_content = ""
                try:
                    if raw and raw.choices:
                        choice0 = raw.choices[0]
                        message = choice0.get("message") if isinstance(choice0, dict) else getattr(choice0, "message", None)
                        if message:
                            message_content = message.get("content") if isinstance(message, dict) else getattr(message, "content", "") or ""
                except Exception:
                    message_content = ""
                self.completion_message = SimpleNamespace(content=message_content)

            def __getattr__(self, item):
                return getattr(self._raw, item)

        class _LegacyStreamWrapper:
            def __init__(self, stream):
                self._stream = stream

            def __iter__(self):
                for chunk in self._stream:
                    try:
                        choice = None
                        if getattr(chunk, "choices", None):
                            choice = chunk.choices[0]
                        delta = {}
                        finish_reason = None
                        if isinstance(choice, dict):
                            delta = choice.get("delta") or {}
                            finish_reason = choice.get("finish_reason") or choice.get("stop_reason")
                        else:
                            delta = getattr(choice, "delta", {}) or {}
                            finish_reason = getattr(choice, "finish_reason", None) or getattr(choice, "stop_reason", None)
                        delta_text = ""
                        if isinstance(delta, dict):
                            delta_text = delta.get("content") or ""
                        else:
                            delta_text = getattr(delta, "content", "") or ""
                        event_type = "progress"
                        if finish_reason:
                            event_type = "complete"
                        event = SimpleNamespace(
                            event_type=event_type,
                            delta=SimpleNamespace(text=delta_text),
                        )
                        yield SimpleNamespace(event=event, raw_chunk=chunk)
                    except Exception:
                        yield SimpleNamespace(event=SimpleNamespace(event_type="error", delta=SimpleNamespace(text="")))

            def __getattr__(self, item):
                return getattr(self._stream, item)

        def compat_chat_completion(self, *args, **kwargs):
            stream_flag = kwargs.get("stream")
            result = original_chat_completion(self, *args, **kwargs)
            if stream_flag:
                return _LegacyStreamWrapper(result)
            return _LegacyCompletionResponse(result)

        compat_chat_completion._compat_patched = True
        InferenceResource.chat_completion = compat_chat_completion

        try:
            from llama_stack_client.resources.vector_dbs import VectorDBsResource
            from llama_stack_client.types.vector_db_list_response import VectorDBListResponseItem
        except ModuleNotFoundError:
            return

        if not getattr(VectorDBsResource.list, "_compat_patched", False):
            original_vector_list = VectorDBsResource.list

            def compat_vector_list(self, *args, **kwargs):
                result = original_vector_list(self, *args, **kwargs)
                try:
                    if result and len(result) > 0:
                        return result
                except Exception:
                    if result:
                        return result

                try:
                    client = getattr(self, "_client", None)
                    if client is None:
                        client = getattr(self, "client", None)
                    if client is None and hasattr(self, "_client"):
                        client = self._client
                    if client is None:
                        return result
                    providers = client.providers.list()
                    fallback = []
                    for provider in providers:
                        if getattr(provider, "api", None) != "vector_io":
                            continue
                        cfg = getattr(provider, "config", {}) or {}
                        collections = cfg.get("collections") or []
                        embedding_dimension = int(cfg.get("embedding_dimension") or 768)
                        embedding_model = (
                            cfg.get("embedding_model")
                            or getattr(provider, "embedding_model", None)
                            or "ibm-granite/granite-embedding-125m-english"
                        )
                        provider_id = getattr(provider, "provider_id", "") or cfg.get("provider_id", "")
                        for coll in collections:
                            if isinstance(coll, dict):
                                identifier = coll.get("vector_db_id")
                                provider_resource_id = coll.get("provider_vector_db_id") or identifier
                            else:
                                identifier = getattr(coll, "vector_db_id", None)
                                provider_resource_id = getattr(coll, "provider_vector_db_id", None) or identifier
                            if not identifier:
                                continue
                            ensure_kwargs = dict(
                                vector_db_id=identifier,
                                embedding_model=embedding_model,
                                embedding_dimension=embedding_dimension,
                                provider_id=provider_id or "milvus-shared",
                                provider_vector_db_id=provider_resource_id or identifier,
                            )
                            try:
                                self.retrieve(identifier)
                            except Exception:
                                try:
                                    self.register(**ensure_kwargs)
                                except Exception:
                                    pass
                            fallback.append(
                                VectorDBListResponseItem(
                                    identifier=identifier,
                                    provider_id=provider_id or "vector_io",
                                    provider_resource_id=provider_resource_id or identifier,
                                    embedding_dimension=embedding_dimension or 0,
                                    embedding_model=embedding_model or "",
                                    type="vector_db",
                                )
                            )
                    if fallback:
                        return fallback
                except Exception:
                    return result

                return result

            compat_vector_list._compat_patched = True
            VectorDBsResource.list = compat_vector_list

        optional_resources = [
            ("scoring_functions", "ScoringFunctionsResource"),
            ("datasets", "DatasetsResource"),
            ("benchmarks", "BenchmarksResource"),
        ]

        for module_name, class_name in optional_resources:
            try:
                module = importlib.import_module(f"llama_stack_client.resources.{module_name}")
                resource_cls = getattr(module, class_name)
            except (ModuleNotFoundError, AttributeError):
                continue

            original_list = resource_cls.list
            if getattr(original_list, "_compat_optional_patched", False):
                continue

            def _make_optional_wrapper(func):
                def compat_list(self, *args, **kwargs):
                    try:
                        return func(self, *args, **kwargs)
                    except Exception as exc:  # noqa: BLE001
                        if _is_not_found_error(exc):
                            return []
                        raise

                compat_list._compat_optional_patched = True
                return compat_list

            resource_cls.list = _make_optional_wrapper(original_list)

    _patch()

