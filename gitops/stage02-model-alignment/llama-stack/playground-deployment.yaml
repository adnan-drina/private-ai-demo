---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llama-stack-playground
  namespace: private-ai-demo
  labels:
    app: llama-stack-playground
    app.kubernetes.io/name: llama-stack-playground
    app.kubernetes.io/component: rag-ui
    app.kubernetes.io/part-of: serving
spec:
  replicas: 1
  selector:
    matchLabels:
      app: llama-stack-playground
  template:
    metadata:
      labels:
        app: llama-stack-playground
        app.kubernetes.io/name: llama-stack-playground
        app.kubernetes.io/component: rag-ui
        app.kubernetes.io/part-of: serving
        app.openshift.io/runtime: python
      annotations:
        # Topology: Connects to LlamaStack API
        app.openshift.io/connects-to: '[{"apiVersion":"v1","kind":"Service","name":"llama-stack-service"}]'
        app.openshift.io/vcs-uri: https://github.com/adnan-drina/private-ai-demo
        app.openshift.io/vcs-ref: feature/stage2-implementation
        # No Istio sidecar needed for UI
        sidecar.istio.io/inject: "false"
    spec:
      serviceAccountName: llama-stack
      initContainers:
        # Pre-warm LlamaStack collections to avoid first-query segment load latency
        - name: prewarm-collections
          image: registry.access.redhat.com/ubi9/ubi-minimal:9.5
          command:
            - /bin/sh
            - -c
            - |
              echo "Pre-warming LlamaStack vector collections..."
              for collection in red_hat_docs acme_corporate eu_ai_act; do
                echo "  Querying: $collection"
                curl -sf -X POST http://llama-stack-service.private-ai-demo.svc:8321/v1/vector-io/query \
                  -H 'Content-Type: application/json' \
                  -d "{\"vector_db_id\":\"$collection\",\"query\":\"test\",\"params\":{\"top_k\":1}}" \
                  > /dev/null && echo "    ✅ $collection ready" || echo "    ⚠️  $collection unavailable (will retry on first UI query)"
              done
              echo "Pre-warm complete"
      containers:
        - name: playground
          # Pinned to specific digest for reproducibility (verified 2025-11-07)
          image: quay.io/rh-aiservices-bu/llama-stack-playground@sha256:56be9a862f2b9152ec698f763d762fe426eb8b1c211980a5dd5d7c501b5c25d1
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 8501
              protocol: TCP
          env:
            # LlamaStack API endpoints (internal for SSR, external for browser)
            - name: LLAMA_STACK_BASE_URL
              value: http://llama-stack-service.private-ai-demo.svc:8321
            - name: LLAMA_STACK_URL
              value: http://llama-stack-service.private-ai-demo.svc:8321
            - name: LLAMA_STACK_ENDPOINT
              value: http://llama-stack-service.private-ai-demo.svc:8321
            # Public-facing endpoint for browser
            - name: NEXT_PUBLIC_LLAMA_STACK_URL
              value: https://llamastack-private-ai-demo.apps.cluster-gmgrr.gmgrr.sandbox5294.opentlc.com
            # Streamlit server configuration (Option A: serve at root)
            - name: STREAMLIT_SERVER_HEADLESS
              value: "true"
            - name: STREAMLIT_SERVER_ENABLE_CORS
              value: "false"
            - name: STREAMLIT_SERVER_ENABLE_XSRF_PROTECTION
              value: "false"
            # NOTE: No baseUrlPath - serving at root (/)
            # NOTE: No RAG_DEFAULT_VECTOR_DB_ID - Playground UI will explicitly select from /v1/vector-dbs
          resources:
            requests:
              cpu: 200m
              memory: 512Mi
            limits:
              cpu: 500m
              memory: 1Gi
          readinessProbe:
            httpGet:
              path: /
              port: http
            initialDelaySeconds: 10
            periodSeconds: 5
          livenessProbe:
            httpGet:
              path: /
              port: http
            initialDelaySeconds: 30
            periodSeconds: 10
---
apiVersion: v1
kind: Service
metadata:
  name: llama-stack-playground
  namespace: private-ai-demo
  labels:
    app: llama-stack-playground
    app.kubernetes.io/name: llama-stack-playground
    app.kubernetes.io/component: rag-ui
    app.kubernetes.io/part-of: serving
spec:
  selector:
    app: llama-stack-playground
  ports:
    - name: http
      port: 8501
      targetPort: http
      protocol: TCP
  type: ClusterIP
---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: llama-stack-playground
  namespace: private-ai-demo
  labels:
    app: llama-stack-playground
    app.kubernetes.io/name: llama-stack-playground
    app.kubernetes.io/component: rag-ui
    app.kubernetes.io/part-of: serving
spec:
  to:
    kind: Service
    name: llama-stack-playground
  port:
    targetPort: http
  tls:
    termination: edge
    insecureEdgeTerminationPolicy: Redirect

