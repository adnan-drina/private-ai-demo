---
# LlamaStackDistribution for RHOAI 2.25
# Reference: https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/2.25/html-single/working_with_llama_stack/index
apiVersion: llamastack.io/v1alpha1
kind: LlamaStackDistribution
metadata:
  name: llama-stack
  namespace: private-ai-demo
  labels:
    app: llama-stack
    app.kubernetes.io/name: llama-stack
    app.kubernetes.io/component: rag-stack
    app.kubernetes.io/part-of: private-ai-demo
    app.openshift.io/runtime: python
  annotations:
    app.openshift.io/connects-to: '[{"apiVersion":"apps/v1","kind":"Deployment","name":"milvus-standalone"},{"apiVersion":"apps/v1","kind":"Deployment","name":"docling"}]'
    openshift.io/display-name: "Llama Stack (RHOAI 2.25)"
    description: "Llama Stack RAG runtime with vLLM inference, Milvus vector storage, and Docling document processing"
spec:
  replicas: 1
  
  server:
    # Use Red Hat's official RHOAI Llama Stack image without pre-configured distribution
    # This allows environment variables to configure the stack dynamically
    # Using image from operator's RELATED_IMAGE_RH_DISTRIBUTION
    distribution:
      image: "registry.redhat.io/rhoai/odh-llama-stack-core-rhel9@sha256:86f8d82f589b4044ce9ac30cb62b4611951d2c383e669c8e2dc5bc74e69e6c86"
    
    containerSpec:
      name: llamastack
      port: 8321  # Port defined in rh-dev distribution's run.yaml
      
      env:
        # Inference model configuration (required by rh-dev distribution)
        - name: INFERENCE_MODEL
          value: "mistral-24b-quantized"
        - name: INFERENCE_MODEL_ID
          value: "mistral-24b-quantized"
        
        # vLLM endpoint configuration (for remote::vllm provider)
        # Using external HTTPS route as LlamaStack Operator blocks Istio sidecar injection
        # Internal URL would require sidecar: http://mistral-24b-quantized-predictor.private-ai-demo.svc.cluster.local/v1
        - name: VLLM_URL
          value: "https://mistral-24b-quantized-private-ai-demo.apps.cluster-gmgrr.gmgrr.sandbox5294.opentlc.com/v1"
        - name: VLLM_API_TOKEN
          value: "fake"
        - name: VLLM_TLS_VERIFY
          value: "false"
        
        # Milvus connection (for remote::milvus provider)
        - name: MILVUS_HOST
          value: "milvus-standalone.private-ai-demo.svc.cluster.local"
        - name: MILVUS_PORT
          value: "19530"
        - name: MILVUS_URI
          value: "tcp://milvus-standalone.private-ai-demo.svc.cluster.local:19530"
        
        # Embedding model configuration (for inline::sentence-transformers provider)
        - name: EMBEDDING_MODEL
          value: "sentence-transformers/all-MiniLM-L6-v2"
        
        # Docling document processing service (for RAG document preparation)
        # Reference: RHOAI 2.25 - Section 3.10: Preparing documents with Docling for Llama Stack retrieval
        - name: DOCLING_URL
          value: "http://docling.private-ai-demo.svc:8080"
        
        # Data persistence
        - name: SQLITE_STORE_DIR
          value: "/data"
        
        # Logging level
        - name: LLAMA_STACK_LOG_LEVEL
          value: "INFO"
        
        # HuggingFace token for model downloads (if needed)
        - name: HF_TOKEN
          valueFrom:
            secretKeyRef:
              name: huggingface-secret
              key: token
              optional: true
      
      resources:
        requests:
          memory: "2Gi"
          cpu: "1"
          ephemeral-storage: "5Gi"
        limits:
          memory: "4Gi"
          cpu: "2"
          ephemeral-storage: "10Gi"
    
    # Pod-level configuration
    podOverrides:
      serviceAccountName: rag-workload-sa
      
      volumes:
        - name: data
          persistentVolumeClaim:
            claimName: llamastack-data
        - name: config
          configMap:
            name: llamastack-config
      
      volumeMounts:
        - name: data
          mountPath: /data
        - name: config
          mountPath: /opt/app-root/run.yaml
          subPath: run.yaml
          readOnly: true
