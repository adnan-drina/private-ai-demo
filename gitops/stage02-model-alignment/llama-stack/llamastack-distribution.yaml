---
# LlamaStackDistribution for RHOAI 2.25
# Reference: https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/2.25/html-single/working_with_llama_stack/index
apiVersion: llamastack.io/v1alpha1
kind: LlamaStackDistribution
metadata:
  name: llama-stack
  namespace: private-ai-demo
  labels:
    app: llama-stack
    app.kubernetes.io/name: llama-stack
    app.kubernetes.io/component: rag-stack
    app.kubernetes.io/part-of: serving
    app.openshift.io/runtime: python
  annotations:
    app.openshift.io/connects-to: '[{"apiVersion":"apps/v1","kind":"Deployment","name":"milvus-standalone"},{"apiVersion":"apps/v1","kind":"Deployment","name":"docling"}]'
    openshift.io/display-name: "Llama Stack (RHOAI 2.25)"
    description: "Llama Stack RAG runtime with vLLM inference, Milvus vector storage, and Docling document processing"
spec:
  replicas: 1
  
  server:
    # Use Red Hat Emerging Technologies image with built-in Granite embeddings
    # This image has inline Granite embeddings, avoiding on-demand model loading delays
    distribution:
      image: "quay.io/redhat-et/llama:vllm-milvus-granite-0.2.8"
    
    containerSpec:
      name: llamastack
      port: 8321
      args:
        - --yaml-config
        - /opt/app-root/run.yaml
      
      env:
        # Config and writable homes
        - name: LLAMA_STACK_CONFIG
          value: "/opt/app-root/run.yaml"
        - name: LLAMA_STACK_HOME
          value: "/data/.llama"
        # Note: HF_HOME is set by operator to /opt/app-root/src/.llama/distributions/rh/
        # TRANSFORMERS_CACHE for Granite embedding weights
        - name: TRANSFORMERS_CACHE
          value: "/data/hf_home"

        # vLLM endpoint (external HTTPS Route)
        - name: VLLM_URL
          value: "https://mistral-24b-quantized-private-ai-demo.apps.cluster-gmgrr.gmgrr.sandbox5294.opentlc.com/v1"
        - name: VLLM_API_TOKEN
          value: "fake"
        - name: VLLM_TLS_VERIFY
          value: "false"

        # Milvus (gRPC)
        - name: MILVUS_URI
          value: "tcp://milvus-standalone.private-ai-demo.svc.cluster.local:19530"

        # Files (MinIO) for Files API provider
        - name: FILES_S3_ENDPOINT
          value: "http://minio.model-storage.svc:9000"
        - name: FILES_S3_BUCKET
          value: "llama-files"
        - name: FILES_S3_REGION
          value: "us-east-1"
        - name: FILES_S3_TLS_VERIFY
          value: "false"
        - name: FILES_S3_FORCE_PATH_STYLE
          value: "true"
        - name: FILES_S3_ACCESS_KEY_ID
          valueFrom:
            secretKeyRef:
              name: llama-files-credentials
              key: accesskey
        - name: FILES_S3_SECRET_ACCESS_KEY
          valueFrom:
            secretKeyRef:
              name: llama-files-credentials
              key: secretkey
      
      resources:
        requests:
          memory: "2Gi"
          cpu: "4"  # Increased from 2 to 4 cores for faster embedding computation
          ephemeral-storage: "5Gi"
        limits:
          memory: "4Gi"
          cpu: "4"  # Set limit to 4 cores to handle embedding workload
          ephemeral-storage: "10Gi"
    
    # Pod-level configuration
    podOverrides:
      serviceAccountName: rag-workload-sa
      
      volumes:
        - name: data
          persistentVolumeClaim:
            claimName: llamastack-data
        - name: config
          configMap:
            name: llamastack-config
      
      volumeMounts:
        - name: data
          mountPath: /data
        - name: config
          mountPath: /opt/app-root/run.yaml
          subPath: run.yaml
          readOnly: true
