---
# LlamaStackDistribution for RHOAI 2.25
# Reference: https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/2.25/html-single/working_with_llama_stack/index
apiVersion: llamastack.io/v1alpha1
kind: LlamaStackDistribution
metadata:
  name: llama-stack
  namespace: private-ai-demo
  labels:
    app: llama-stack
    app.kubernetes.io/name: llama-stack
    app.kubernetes.io/component: rag-stack
    app.kubernetes.io/part-of: private-ai-demo
    app.openshift.io/runtime: python
  annotations:
    app.openshift.io/connects-to: '[{"apiVersion":"apps/v1","kind":"Deployment","name":"milvus-standalone"},{"apiVersion":"apps/v1","kind":"Deployment","name":"docling"}]'
    openshift.io/display-name: "Llama Stack (RHOAI 2.25)"
    description: "Llama Stack RAG runtime with vLLM inference, Milvus vector storage, and Docling document processing"
spec:
  replicas: 1
  
  server:
    # Use Red Hat's official RHOAI Llama Stack distribution
    # Using pre-configured "rh-dev" distribution with built-in run.yaml
    # The operator will use registry.redhat.io/rhoai/odh-llama-stack-core-rhel9
    distribution:
      name: "rh-dev"
    
    containerSpec:
      name: llamastack
      port: 8080  # Standard OpenAI-compatible port
      
      env:
        # Inference model configuration (required by rh-dev distribution)
        - name: INFERENCE_MODEL
          value: "mistral-24b-quantized"
        - name: INFERENCE_MODEL_ID
          value: "mistral-24b-quantized"
        
        # vLLM endpoint configuration (for remote::vllm provider)
        - name: VLLM_URL
          value: "http://mistral-24b-quantized-predictor.private-ai-demo.svc.cluster.local/v1"
        
        # Milvus connection (for remote::milvus provider)
        - name: MILVUS_HOST
          value: "milvus-standalone.private-ai-demo.svc.cluster.local"
        - name: MILVUS_PORT
          value: "19530"
        - name: MILVUS_URI
          value: "tcp://milvus-standalone.private-ai-demo.svc.cluster.local:19530"
        
        # Embedding model configuration (for inline::sentence-transformers provider)
        - name: EMBEDDING_MODEL
          value: "sentence-transformers/all-MiniLM-L6-v2"
        
        # Docling document processing service (for RAG document preparation)
        # Reference: RHOAI 2.25 - Section 3.10: Preparing documents with Docling for Llama Stack retrieval
        - name: DOCLING_URL
          value: "http://docling.private-ai-demo.svc:8080"
        
        # Data persistence
        - name: SQLITE_STORE_DIR
          value: "/data"
        
        # Logging level
        - name: LLAMA_STACK_LOG_LEVEL
          value: "INFO"
        
        # HuggingFace token for model downloads (if needed)
        - name: HF_TOKEN
          valueFrom:
            secretKeyRef:
              name: huggingface-secret
              key: token
              optional: true
      
      resources:
        requests:
          memory: "2Gi"
          cpu: "1"
          ephemeral-storage: "5Gi"
        limits:
          memory: "4Gi"
          cpu: "2"
          ephemeral-storage: "10Gi"
    
    # Pod-level configuration
    podOverrides:
      serviceAccountName: rag-workload-sa
      
      volumes:
        - name: data
          persistentVolumeClaim:
            claimName: llamastack-data
      
      volumeMounts:
        - name: data
          mountPath: /data
