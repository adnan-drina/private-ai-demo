---
# LlamaStackDistribution for RHOAI 2.25
# Reference: https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/2.25/html-single/working_with_llama_stack/index
apiVersion: llamastack.io/v1alpha1
kind: LlamaStackDistribution
metadata:
  name: llama-stack
  namespace: private-ai-demo
  labels:
    app: llama-stack
    app.kubernetes.io/name: llama-stack
    app.kubernetes.io/component: rag-stack
    app.kubernetes.io/part-of: serving
    app.openshift.io/runtime: python
  annotations:
    app.openshift.io/connects-to: '[{"apiVersion":"apps/v1","kind":"Deployment","name":"milvus-standalone"},{"apiVersion":"apps/v1","kind":"Deployment","name":"docling"}]'
    openshift.io/display-name: "Llama Stack (RHOAI 2.25)"
    description: "Llama Stack RAG runtime with vLLM inference, Milvus vector storage, and Docling document processing"
spec:
  replicas: 1
  
  server:
    # Use latest Tech Preview CPU image (Python 3.12 +
    # ready for TrustyAI integrations)
    distribution:
      image: "registry.redhat.io/rhoai/odh-llama-stack-core-rhel9:0.0-1762357606"
    
    containerSpec:
      name: llamastack
      port: 8321
      command:
        - /bin/sh
        - -c
      args:
        - |
          set -e
          mkdir -p /external-providers
          mkdir -p /data/sqlite
          mkdir -p /data/files
          python3 -m pip install --no-cache-dir --upgrade --target /external-providers git+https://github.com/trustyai-explainability/llama-stack-provider-trustyai-fms.git
          export PYTHONPATH="/external-providers:${PYTHONPATH}"
          export LLAMA_STACK_EXTERNAL_PROVIDERS_DIR="/external-providers/llama_stack_provider_trustyai_fms/providers.d"
          exec llama stack run /opt/app-root/run.yaml
      env:
        # Config and writable homes
        - name: LLAMA_STACK_CONFIG
          value: "/opt/app-root/run.yaml"
        - name: LLAMA_STACK_HOME
          value: "/data/.llama"
        # Note: HF_HOME is set by operator to /opt/app-root/src/.llama/distributions/rh/
        # TRANSFORMERS_CACHE for Granite embedding weights
        - name: TRANSFORMERS_CACHE
          value: "/data/hf_home"

        # vLLM endpoint (external HTTPS Route)
        - name: VLLM_URL
          value: "https://mistral-24b-quantized-private-ai-demo.apps.cluster-gmgrr.gmgrr.sandbox5294.opentlc.com/v1"
        - name: VLLM_API_TOKEN
          value: "fake"
        - name: VLLM_TLS_VERIFY
          value: "false"

        # Milvus (gRPC)
        - name: MILVUS_URI
          value: "tcp://milvus-standalone.private-ai-demo.svc.cluster.local:19530"
        # External providers bootstrap
        - name: PYTHONPATH
          value: "/external-providers:/opt/app-root/src/.local/lib/python3.12/site-packages:/opt/app-root/lib/python3.12/site-packages"
        - name: LLAMA_STACK_EXTERNAL_PROVIDERS_DIR
          value: "/external-providers/llama_stack_provider_trustyai_fms/providers.d"

        # Files (MinIO) for Files API provider
        - name: FILES_S3_ENDPOINT
          value: "http://minio.model-storage.svc:9000"
        - name: FILES_S3_BUCKET
          value: "llama-files"
        - name: FILES_S3_REGION
          value: "us-east-1"
        - name: FILES_S3_TLS_VERIFY
          value: "false"
        - name: FILES_S3_FORCE_PATH_STYLE
          value: "true"
        - name: FILES_S3_ACCESS_KEY_ID
          valueFrom:
            secretKeyRef:
              name: llama-files-credentials
              key: accesskey
        - name: FILES_S3_SECRET_ACCESS_KEY
          valueFrom:
            secretKeyRef:
              name: llama-files-credentials
              key: secretkey
        - name: AWS_ACCESS_KEY_ID
          valueFrom:
            secretKeyRef:
              name: llama-files-credentials
              key: accesskey
        - name: AWS_SECRET_ACCESS_KEY
          valueFrom:
            secretKeyRef:
              name: llama-files-credentials
              key: secretkey
        - name: AWS_DEFAULT_REGION
          value: "us-east-1"
        - name: AWS_ENDPOINT_URL
          value: "http://minio.model-storage.svc:9000"
        - name: AWS_S3_FORCE_PATH_STYLE
          value: "true"
        - name: AWS_EC2_METADATA_DISABLED
          value: "true"
        - name: AWS_VERIFY_SSL
          value: "false"
      
      resources:
        requests:
          memory: "2Gi"
          cpu: "4"  # Increased from 2 to 4 cores for faster embedding computation
          ephemeral-storage: "5Gi"
        limits:
          memory: "4Gi"
          cpu: "4"  # Set limit to 4 cores to handle embedding workload
          ephemeral-storage: "10Gi"
    
    # Pod-level configuration
    podOverrides:
      serviceAccountName: rag-workload-sa
      
      volumes:
        - name: data
          persistentVolumeClaim:
            claimName: llamastack-data
        - name: config
          configMap:
            name: llamastack-config
        - name: external-providers
          emptyDir: {}
      
      volumeMounts:
        - name: data
          mountPath: /data
        - name: config
          mountPath: /opt/app-root/run.yaml
          subPath: run.yaml
          readOnly: true
        - name: external-providers
          mountPath: /external-providers
