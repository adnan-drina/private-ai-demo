---
apiVersion: v1
kind: ConfigMap
metadata:
  name: llamastack-config
  namespace: private-ai-demo
  labels:
    app: llama-stack
    component: rag
data:
  run.yaml: |
    version: '2'
    image_name: rh
    
    # RAG-enabled APIs - Following OpenDataHub pattern
    # Files API disabled: no provider available in quay.io/redhat-et/llama:vllm-0.2.7
    # Note: embeddings accessed through inference API (not standalone)
    apis:
      - inference
      - agents
      - safety
      - telemetry
      - tool_runtime
      - vector_io
      # - files  # DISABLED: No provider available in Red Hat ET image
      # - embeddings  # NOT SUPPORTED in this image (quay.io/redhat-et/llama:vllm-0.2.7)
    
    providers:
      inference:
        # Quantized Mistral 24B from Stage 1 (1 GPU, cost-efficient)
        # NOTE: URL is dynamically injected by deploy.sh using envsubst
        - provider_id: mistral-24b-quantized
          provider_type: remote::vllm
          config:
            url: "https://mistral-24b-quantized-private-ai-demo.apps.cluster-gmgrr.gmgrr.sandbox5294.opentlc.com/v1"
            max_tokens: 4096
            api_token: fake
            tls_verify: false
        
        # Full Precision Mistral 24B from Stage 1 (4 GPUs, maximum quality)
        - provider_id: mistral-24b-full
          provider_type: remote::vllm
          config:
            url: "https://mistral-24b-private-ai-demo.apps.cluster-gmgrr.gmgrr.sandbox5294.opentlc.com/v1"
            max_tokens: 4096
            api_token: fake
            tls_verify: false
        
        # Sentence Transformers for embeddings (inline, model cached after first load)
        # NOTE: First request after pod restart triggers 500MB model download/load (2-3 min)
        # Subsequent requests use cached model and are fast
        # LIMITATION: Cannot use standalone granite-embedding service because:
        #   - remote::openai provider requires 'litellm' module
        #   - Red Hat ET image (quay.io/redhat-et/llama:vllm-milvus-0.2.8) doesn't include litellm
        # WORKAROUND: Model is now pre-loaded, so blocking only happens on first pod start
        - provider_id: sentence-transformers
          provider_type: inline::sentence-transformers
          config: {}
      
      vector_io:
        # Milvus in private-ai-demo - optimized with HNSW index
        # Reference: https://llama-stack.readthedocs.io/en/v0.2.11/providers/vector_io/milvus.html
        # Reference: https://milvus.io/docs/schema-hands-on.md
        # Schema: Int64 PK (auto_id=true), vector (HNSW indexed), dynamic fields
        - provider_id: milvus-shared
          provider_type: remote::milvus
          config:
            uri: "tcp://milvus-standalone.private-ai-demo.svc.cluster.local:19530"
            # Field mappings - LET PROVIDER OWN THE SCHEMA
            # Provider creates: chunk_id (PK), vector, chunk_content (JSON)
            # Do NOT override text_field or id_field - provider manages these
            embedding_dimension: 768
            metric_type: "L2"
            # Search params aligned with HNSW index (M=16, efConstruction=200)
            search_params:
              metric_type: "L2"
              params:
                ef: 64
            # Collection mappings (scenario-specific)
            collections:
              - vector_db_id: acme_corporate
                provider_vector_db_id: acme_corporate
              - vector_db_id: red_hat_docs
                provider_vector_db_id: red_hat_docs
              - vector_db_id: eu_ai_act
                provider_vector_db_id: eu_ai_act
      
      agents:
        # Inline agent provider for RAG
        - provider_id: inline-agent
          provider_type: inline::meta-reference
          config:
            persistence_store:
              type: sqlite
              namespace: null
              db_path: /data/agents_store.db
            responses_store:
              type: sqlite
              namespace: null
              db_path: /data/responses_store.db
      
      safety:
        # Inline safety provider (required by agents)
        - provider_id: inline-safety
          provider_type: inline::llama-guard
          config: {}
      
      telemetry:
        # SQLite telemetry for local storage
        - provider_id: meta-reference
          provider_type: inline::meta-reference
          config:
            sqlite_db_path: /data/telemetry.db
        
        # OpenTelemetry for distributed tracing and metrics
        # Note: Requires OpenTelemetry instrumentation in the Llama Stack image
        # This is configured via OTEL_* environment variables in the deployment
        # - provider_id: opentelemetry
        #   provider_type: inline::opentelemetry
        #   config:
        #     service_name: llama-stack
        #     exporter_type: otlp
        #     endpoint: http://tempo-distributor.openshift-distributed-tracing.svc:4317
      
      tool_runtime:
        # RAG runtime tool provider (required for agent RAG)
        # RAG runtime uses vector_io provider to access Milvus
        # NOTE: Playground explicitly selects vector_db from UI; no default needed
        - provider_id: rag-runtime
          provider_type: inline::rag-runtime
          config:
            # default_vector_db_id: ""                # Empty - Playground selects explicitly
            top_k: 5                                  # retrieve top 5 chunks
            similarity_threshold: 0.0                # no filtering (start permissive)
        
        # Model Context Protocol provider for external tools
        - provider_id: model-context-protocol
          provider_type: remote::model-context-protocol
          config: {}
      
      # files:
        # Files API not available in Red Hat ET image (quay.io/redhat-et/llama:vllm-0.2.7)
        # Neither remote::s3-files nor inline::files providers are available
        # - provider_id: local-files
        #   provider_type: inline::files
        #   config:
        #     storage_dir: "/data/files"
    
    models:
      # Quantized model - cost-efficient (1 GPU)
      - metadata:
          llama_model: RedHatAI/Mistral-Small-24B-Instruct-2501-quantized.w4a16
          description: "Quantized Mistral 24B from Stage 1 (compressed-tensors)"
          quantization: "compressed-tensors"
          gpu_count: 1
        model_id: mistral-24b-quantized
        model_type: llm
        provider_id: mistral-24b-quantized
        provider_model_id: mistral-24b-quantized
      
      # Full precision model - maximum quality (4 GPUs)
      - metadata:
          llama_model: mistralai/Mistral-Small-Instruct-2501
          description: "Full precision Mistral 24B from Stage 1"
          quantization: "none"
          gpu_count: 4
        model_id: mistral-24b-full
        model_type: llm
        provider_id: mistral-24b-full
        provider_model_id: mistral-24b
      
      # IBM Granite embedding model (768 dimensions)
      # Uses inline::sentence-transformers provider (model cached after first load)
      - metadata:
          embedding_dimension: 768
        model_id: ibm-granite/granite-embedding-125m-english
        model_type: embedding
        provider_id: sentence-transformers
        provider_model_id: ibm-granite/granite-embedding-125m-english
    
    vector_databases:
      # Scenario 1: Red Hat documentation
      - vector_db_id: red_hat_docs
        embedding_model: ibm-granite/granite-embedding-125m-english
        embedding_dimension: 768
        provider_id: milvus-shared
      
      # Scenario 2: ACME corporate documents
      - vector_db_id: acme_corporate
        embedding_model: ibm-granite/granite-embedding-125m-english
        embedding_dimension: 768
        provider_id: milvus-shared
      
      # Scenario 3: EU AI Act regulation
      - vector_db_id: eu_ai_act
        embedding_model: ibm-granite/granite-embedding-125m-english
        embedding_dimension: 768
        provider_id: milvus-shared
    
    tool_groups:
      # Built-in RAG tool group (required for Agent RAG)
      - toolgroup_id: builtin::rag
        provider_id: rag-runtime
      
      # OpenShift MCP Server - ENABLED (SSE-ready)
      # Provides cluster troubleshooting tools via kubernetes-mcp-server
      - toolgroup_id: mcp::openshift
        provider_id: model-context-protocol
        mcp_endpoint:
          uri: "http://openshift-mcp.private-ai-demo.svc.cluster.local:8000/sse"
      
      # Database MCP and Slack MCP - DISABLED
      # TODO: Add SSE support to enable these MCP servers
      # Current implementation uses HTTP POST /execute (not SSE)
      # LlamaStack model-context-protocol provider requires SSE endpoints
      
      # - toolgroup_id: mcp::database
      #   provider_id: model-context-protocol
      #   mcp_endpoint:
      #     uri: "http://database-mcp.private-ai-demo.svc.cluster.local:8080/sse"
      
      # - toolgroup_id: mcp::slack
      #   provider_id: model-context-protocol
      #   mcp_endpoint:
      #     uri: "http://slack-mcp.private-ai-demo.svc.cluster.local:8080/sse"
    
    vector_dbs:
      # Scenario-specific vector databases for RAG
      # All use IBM Granite embeddings (768 dimensions)
      
      # Scenario 1: Red Hat Documentation
      - vector_db_id: red_hat_docs
        provider_id: milvus-shared
        provider_vector_db_id: red_hat_docs
        embedding_model: ibm-granite/granite-embedding-125m-english
        embedding_dimension: 768
      
      # Scenario 2: ACME Corporate (default for Playground)
      - vector_db_id: acme_corporate
        provider_id: milvus-shared
        provider_vector_db_id: acme_corporate
        embedding_model: ibm-granite/granite-embedding-125m-english
        embedding_dimension: 768
      
      # Scenario 3: EU AI Act Regulation
      - vector_db_id: eu_ai_act
        provider_id: milvus-shared
        provider_vector_db_id: eu_ai_act
        embedding_model: ibm-granite/granite-embedding-125m-english
        embedding_dimension: 768
    
    server:
      port: 8321
