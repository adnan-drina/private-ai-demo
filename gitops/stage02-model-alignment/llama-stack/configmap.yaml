---
apiVersion: v1
kind: ConfigMap
metadata:
  name: llamastack-config
  namespace: private-ai-demo
  labels:
    app: llama-stack
    component: rag
data:
  run.yaml: |
    version: '2'
    image_name: remote-vllm-mistral-rag
    
    # Metadata store configuration - use writable /data volume
    metadata_store:
      type: sqlite
      db_path: /data/dist_registry.db
    
    # RAG-enabled APIs - Following OpenDataHub pattern
    # Note: eval API disabled until TrustyAI provider is available
    apis:
      - inference
      - agents
      - safety
      - telemetry
      - tool_runtime
      - vector_io
      - files  # Required by rag-runtime tool_runtime provider
      # - eval  # DISABLED: Provider not available in rh-dev distribution
    
    providers:
      inference:
        # Quantized Mistral 24B from Stage 1 (1 GPU, cost-efficient)
        # Using external HTTPS route as LlamaStack Operator blocks Istio sidecar injection
        # Internal URL would require sidecar: http://mistral-24b-quantized-predictor.private-ai-demo.svc.cluster.local/v1
        - provider_id: mistral-24b-quantized
          provider_type: remote::vllm
          config:
            url: "https://mistral-24b-quantized-private-ai-demo.apps.cluster-gmgrr.gmgrr.sandbox5294.opentlc.com/v1"
            max_tokens: 4096
            api_token: fake
            tls_verify: false
        
        # Full Precision Mistral 24B from Stage 1 (4 GPUs, maximum quality)
        # Using external HTTPS route as LlamaStack Operator blocks Istio sidecar injection  
        # Internal URL would require sidecar: http://mistral-24b-predictor.private-ai-demo.svc.cluster.local/v1
        - provider_id: mistral-24b-full
          provider_type: remote::vllm
          config:
            url: "https://mistral-24b-private-ai-demo.apps.cluster-gmgrr.gmgrr.sandbox5294.opentlc.com/v1"
            max_tokens: 4096
            api_token: fake
            tls_verify: false
        
        # Embedding provider: Granite Embedding (OpenAI-compatible API)
        - provider_id: granite-embedding
          provider_type: remote::openai
          config:
            url: "http://granite-embedding.ai-infrastructure.svc.cluster.local/v1"
            tls_verify: false
      
      vector_io:
        # Milvus in private-ai-demo (dedicated for this demo)
        - provider_id: milvus-shared
          provider_type: remote::milvus
          config:
            uri: "tcp://milvus-standalone.private-ai-demo.svc.cluster.local:19530"
            token: ""  # Empty token for local Milvus without authentication
            collection_name: "rag_documents"
            kvstore:
              type: sqlite
              db_path: /data/milvus_registry.db
      
      agents:
        # Inline agent provider for RAG
        - provider_id: inline-agent
          provider_type: inline::meta-reference
          config:
            persistence_store:
              type: sqlite
              namespace: null
              db_path: /data/agents_store.db
      
      safety:
        # Inline safety provider (required by agents)
        - provider_id: inline-safety
          provider_type: inline::llama-guard
          config: {}
      
      telemetry:
        # SQLite telemetry for local storage
        - provider_id: meta-reference
          provider_type: inline::meta-reference
          config:
            sqlite_db_path: /data/telemetry.db
        
        # OpenTelemetry for distributed tracing and metrics
        # Note: Requires OpenTelemetry instrumentation in the Llama Stack image
        # This is configured via OTEL_* environment variables in the deployment
        # - provider_id: opentelemetry
        #   provider_type: inline::opentelemetry
        #   config:
        #     service_name: llama-stack
        #     exporter_type: otlp
        #     endpoint: http://tempo-distributor.openshift-distributed-tracing.svc:4317
      
      files:
        # S3/MinIO files provider for RAG document storage
        # Reads credentials from process env (set in LlamaStackDistribution)
        - provider_id: s3-files
          provider_type: remote::s3-files
          config:
            endpoint_url: "http://minio.model-storage.svc:9000"
            bucket: "llama-files"
            region: "us-east-1"
            tls_verify: false
            s3_force_path_style: true
            # Credentials read from environment variables
            credentials:
              access_key_id_from_env: "FILES_S3_ACCESS_KEY_ID"
              secret_access_key_from_env: "FILES_S3_SECRET_ACCESS_KEY"
      
      tool_runtime:
        # RAG runtime tool provider (required for agent RAG)
        - provider_id: rag-runtime
          provider_type: inline::rag-runtime
          config: {}
        
        # Model Context Protocol provider for external tools
        - provider_id: model-context-protocol
          provider_type: remote::model-context-protocol
          config: {}
      
      # eval:
      #   # TrustyAI LM-Eval provider for model quality assessment
      #   # Phase 2: Llama Stack Integration - DISABLED: Provider not available
      #   - provider_id: trustyai-lmeval
      #     provider_type: remote::trustyai-lmeval
      #     config:
      #       url: "http://trustyai-lmeval-provider.private-ai-demo.svc:8080"
      #       namespace: "private-ai-demo"
      #       # Model configurations for evaluation
      #       models:
      #         - model_id: mistral-24b-quantized
      #           endpoint: "http://mistral-24b-quantized-predictor.private-ai-demo.svc.cluster.local/v1/completions"
      #           tokenizer: "RedHatAI/Mistral-Small-24B-Instruct-2501-quantized.w4a16"
      #           num_concurrent: 2
      #         - model_id: mistral-24b-full
      #           endpoint: "http://mistral-24b-predictor.private-ai-demo.svc.cluster.local/v1/completions"
      #           tokenizer: "mistralai/Mistral-Small-24B-Instruct-2501"
      #           num_concurrent: 4
      #       # Default evaluation settings
      #       default_sample_limit: 100
      #       default_batch_size: 1
      #       allow_online: true
      #       allow_code_execution: true
    
    models:
      # Quantized model - cost-efficient (1 GPU)
      - metadata:
          llama_model: RedHatAI/Mistral-Small-24B-Instruct-2501-quantized.w4a16
          description: "Quantized Mistral 24B from Stage 1 (compressed-tensors)"
          quantization: "compressed-tensors"
          gpu_count: 1
        model_id: mistral-24b-quantized
        model_type: llm
        provider_id: mistral-24b-quantized
        provider_model_id: mistral-24b-quantized
      
      # Full precision model - maximum quality (4 GPUs)
      - metadata:
          llama_model: mistralai/Mistral-Small-Instruct-2501
          description: "Full precision Mistral 24B from Stage 1"
          quantization: "none"
          gpu_count: 4
        model_id: mistral-24b-full
        model_type: llm
        provider_id: mistral-24b-full
        provider_model_id: mistral-24b
      
      # IBM Granite embedding model (768 dimensions)
      - metadata:
          embedding_dimension: 768
        model_id: ibm-granite/granite-embedding-125m-english
        model_type: embedding
        provider_id: granite-embedding
        provider_model_id: ibm-granite/granite-embedding-125m-english
    
    tool_groups:
      # Built-in RAG tool group (required for Agent RAG)
      - toolgroup_id: builtin::rag
        provider_id: rag-runtime
      
      # MCP Servers - TEMPORARILY DISABLED
      # TODO: Enable after implementing SSE endpoints in MCP servers
      # Current MCP servers use simple HTTP, but Llama Stack requires SSE
      # Will be fixed in Phase 3
      
      # - toolgroup_id: mcp::database
      #   provider_id: model-context-protocol
      #   mcp_endpoint:
      #     uri: "http://database-mcp.private-ai-demo.svc.cluster.local:8080/sse"
      
      # - toolgroup_id: mcp::slack
      #   provider_id: model-context-protocol
      #   mcp_endpoint:
      #     uri: "http://slack-mcp.private-ai-demo.svc.cluster.local:8080/sse"
    
    vector_dbs:
      # Register vector database with IBM Granite embeddings (768 dimensions)
      - vector_db_id: rag_documents
        provider_id: milvus-shared
        embedding_model: ibm-granite/granite-embedding-125m-english
        embedding_dimension: 768
    
    server:
      port: 8321
