---
apiVersion: v1
kind: ConfigMap
metadata:
  name: llamastack-config
  namespace: private-ai-demo
  labels:
    app: llama-stack
    component: rag
data:
  run.yaml: |
    version: '2'
    image_name: rh
    
    # RAG-enabled APIs - Following OpenDataHub pattern
    # Files API disabled: no provider available in quay.io/redhat-et/llama:vllm-0.2.7
    # Note: embeddings accessed through inference API (not standalone)
    apis:
      - inference
      - agents
      - safety
      - telemetry
      - tool_runtime
      - vector_io
      # - files  # DISABLED: No provider available in Red Hat ET image
      # - embeddings  # NOT SUPPORTED in this image (quay.io/redhat-et/llama:vllm-0.2.7)
    
    providers:
      inference:
        # Quantized Mistral 24B from Stage 1 (1 GPU, cost-efficient)
        # NOTE: URL is dynamically injected by deploy.sh using envsubst
        - provider_id: mistral-24b-quantized
          provider_type: remote::vllm
          config:
            url: "https://mistral-24b-quantized-private-ai-demo.apps.cluster-gmgrr.gmgrr.sandbox5294.opentlc.com/v1"
            max_tokens: 4096
            api_token: fake
            tls_verify: false
        
        # Full Precision Mistral 24B from Stage 1 (4 GPUs, maximum quality)
        - provider_id: mistral-24b-full
          provider_type: remote::vllm
          config:
            url: "https://mistral-24b-private-ai-demo.apps.cluster-gmgrr.gmgrr.sandbox5294.opentlc.com/v1"
            max_tokens: 4096
            api_token: fake
            tls_verify: false
        
        # Sentence Transformers for embeddings (inline, model cached after first load)
        # NOTE: First request after pod restart triggers 500MB model download/load (2-3 min)
        # Subsequent requests use cached model and are fast
        # LIMITATION: Cannot use standalone granite-embedding service because:
        #   - remote::openai provider requires 'litellm' module
        #   - Red Hat ET image (quay.io/redhat-et/llama:vllm-milvus-0.2.8) doesn't include litellm
        # WORKAROUND: Model is now pre-loaded, so blocking only happens on first pod start
        - provider_id: sentence-transformers
          provider_type: inline::sentence-transformers
          config: {}
      
      vector_io:
        # Milvus in private-ai-demo (minimal config per official docs)
        # Reference: https://llama-stack.readthedocs.io/en/v0.2.11/providers/vector_io/milvus.html
        - provider_id: milvus-shared
          provider_type: remote::milvus
          config:
            uri: "tcp://milvus-standalone.private-ai-demo.svc.cluster.local:19530"
            # NO OTHER PARAMETERS - provider will use defaults
      
      agents:
        # Inline agent provider for RAG
        - provider_id: inline-agent
          provider_type: inline::meta-reference
          config:
            persistence_store:
              type: sqlite
              namespace: null
              db_path: /data/agents_store.db
            responses_store:
              type: sqlite
              namespace: null
              db_path: /data/responses_store.db
      
      safety:
        # Inline safety provider (required by agents)
        - provider_id: inline-safety
          provider_type: inline::llama-guard
          config: {}
      
      telemetry:
        # SQLite telemetry for local storage
        - provider_id: meta-reference
          provider_type: inline::meta-reference
          config:
            sqlite_db_path: /data/telemetry.db
        
        # OpenTelemetry for distributed tracing and metrics
        # Note: Requires OpenTelemetry instrumentation in the Llama Stack image
        # This is configured via OTEL_* environment variables in the deployment
        # - provider_id: opentelemetry
        #   provider_type: inline::opentelemetry
        #   config:
        #     service_name: llama-stack
        #     exporter_type: otlp
        #     endpoint: http://tempo-distributor.openshift-distributed-tracing.svc:4317
      
      tool_runtime:
        # RAG runtime tool provider (required for agent RAG)
        # Fixed: Added defaults for vector DB queries
        - provider_id: rag-runtime
          provider_type: inline::rag-runtime
          config:
            default_vector_db_id: acme_corporate     # default corpus for RAG (has data)
            top_k: 5                                  # retrieve top 5 chunks
            similarity_threshold: 0.0                # no filtering (start permissive)
        
        # Model Context Protocol provider for external tools
        - provider_id: model-context-protocol
          provider_type: remote::model-context-protocol
          config: {}
      
      # files:
        # Files API not available in Red Hat ET image (quay.io/redhat-et/llama:vllm-0.2.7)
        # Neither remote::s3-files nor inline::files providers are available
        # - provider_id: local-files
        #   provider_type: inline::files
        #   config:
        #     storage_dir: "/data/files"
    
    models:
      # Quantized model - cost-efficient (1 GPU)
      - metadata:
          llama_model: RedHatAI/Mistral-Small-24B-Instruct-2501-quantized.w4a16
          description: "Quantized Mistral 24B from Stage 1 (compressed-tensors)"
          quantization: "compressed-tensors"
          gpu_count: 1
        model_id: mistral-24b-quantized
        model_type: llm
        provider_id: mistral-24b-quantized
        provider_model_id: mistral-24b-quantized
      
      # Full precision model - maximum quality (4 GPUs)
      - metadata:
          llama_model: mistralai/Mistral-Small-Instruct-2501
          description: "Full precision Mistral 24B from Stage 1"
          quantization: "none"
          gpu_count: 4
        model_id: mistral-24b-full
        model_type: llm
        provider_id: mistral-24b-full
        provider_model_id: mistral-24b
      
      # IBM Granite embedding model (768 dimensions)
      # Uses inline::sentence-transformers provider (model cached after first load)
      - metadata:
          embedding_dimension: 768
        model_id: ibm-granite/granite-embedding-125m-english
        model_type: embedding
        provider_id: sentence-transformers
        provider_model_id: ibm-granite/granite-embedding-125m-english
    
    tool_groups:
      # Built-in RAG tool group (required for Agent RAG)
      - toolgroup_id: builtin::rag
        provider_id: rag-runtime
      
      # MCP Servers - TEMPORARILY DISABLED
      # TODO: Enable after implementing SSE endpoints in MCP servers
      # Current MCP servers use simple HTTP, but Llama Stack requires SSE
      # Will be fixed in Phase 3
      
      # - toolgroup_id: mcp::database
      #   provider_id: model-context-protocol
      #   mcp_endpoint:
      #     uri: "http://database-mcp.private-ai-demo.svc.cluster.local:8080/sse"
      
      # - toolgroup_id: mcp::slack
      #   provider_id: model-context-protocol
      #   mcp_endpoint:
      #     uri: "http://slack-mcp.private-ai-demo.svc.cluster.local:8080/sse"
    
    vector_dbs:
      # Register vector databases created by KFP ingestion pipelines
      # All use IBM Granite embeddings (768 dimensions)
      # CRITICAL FIX: Added provider_vector_db_id to map to actual Milvus collections
      
      # KFP default collection used by store_in_milvus step
      - vector_db_id: rag_documents
        provider_id: milvus-shared
        provider_vector_db_id: rag_documents
        embedding_model: ibm-granite/granite-embedding-125m-english
        embedding_dimension: 768
      
      # ACME Corporate collection (default for RAG queries)
      - vector_db_id: acme_corporate
        provider_id: milvus-shared
        provider_vector_db_id: acme_corporate          # exact Milvus collection name
        embedding_model: ibm-granite/granite-embedding-125m-english
        embedding_dimension: 768
      
      # Red Hat Documentation collection
      - vector_db_id: red_hat_docs
        provider_id: milvus-shared
        provider_vector_db_id: red_hat_docs            # exact Milvus collection name
        embedding_model: ibm-granite/granite-embedding-125m-english
        embedding_dimension: 768
      
      # EU AI Act collection
      - vector_db_id: eu_ai_act
        provider_id: milvus-shared
        provider_vector_db_id: eu_ai_act               # exact Milvus collection name
        embedding_model: ibm-granite/granite-embedding-125m-english
        embedding_dimension: 768
    
    server:
      port: 8321
