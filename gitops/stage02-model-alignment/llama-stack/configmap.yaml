---
apiVersion: v1
kind: ConfigMap
metadata:
  name: llamastack-config
  namespace: private-ai-demo
  labels:
    app: llama-stack
    component: rag
data:
  run.yaml: |
    version: '2'
    image_name: remote-vllm-mistral-rag
    
    # RAG-enabled APIs - Following OpenDataHub pattern
    # + Evaluation API (Phase 2: TrustyAI Integration)
    apis:
      - inference
      - agents
      - safety
      - telemetry
      - tool_runtime
      - vector_io
      - eval  # NEW: Enable evaluation API via TrustyAI provider
    
    providers:
      inference:
        # Quantized Mistral 24B from Stage 1 (1 GPU, cost-efficient)
        # NOTE: URL is dynamically injected by deploy.sh using envsubst
        - provider_id: mistral-24b-quantized
          provider_type: remote::vllm
          config:
            url: "https://mistral-24b-quantized-private-ai-demo.apps.cluster-qtvt5.qtvt5.sandbox2082.opentlc.com/v1"
            max_tokens: 4096
            api_token: fake
            tls_verify: false
        
        # Full Precision Mistral 24B from Stage 1 (4 GPUs, maximum quality)
        - provider_id: mistral-24b-full
          provider_type: remote::vllm
          config:
            url: "https://mistral-24b-private-ai-demo.apps.cluster-qtvt5.qtvt5.sandbox2082.opentlc.com/v1"
            max_tokens: 4096
            api_token: fake
            tls_verify: false
        
        # Sentence Transformers for embeddings (IBM Granite)
        - provider_id: sentence-transformers
          provider_type: inline::sentence-transformers
          config: {}
      
      vector_io:
        # Milvus in private-ai-demo (dedicated for this demo)
        - provider_id: milvus-shared
          provider_type: remote::milvus
          config:
            uri: "http://milvus-standalone.private-ai-demo.svc.cluster.local:19530"
            collection_name: "rag_documents"
      
      agents:
        # Inline agent provider for RAG
        - provider_id: inline-agent
          provider_type: inline::meta-reference
          config:
            persistence_store:
              type: sqlite
              namespace: null
              db_path: /data/agents_store.db
      
      safety:
        # Inline safety provider (required by agents)
        - provider_id: inline-safety
          provider_type: inline::llama-guard
          config: {}
      
      telemetry:
        # SQLite telemetry for local storage
        - provider_id: meta-reference
          provider_type: inline::meta-reference
          config:
            sqlite_db_path: /data/telemetry.db
        
        # OpenTelemetry for distributed tracing and metrics
        # Note: Requires OpenTelemetry instrumentation in the Llama Stack image
        # This is configured via OTEL_* environment variables in the deployment
        # - provider_id: opentelemetry
        #   provider_type: inline::opentelemetry
        #   config:
        #     service_name: llama-stack
        #     exporter_type: otlp
        #     endpoint: http://tempo-distributor.openshift-distributed-tracing.svc:4317
      
      tool_runtime:
        # RAG runtime tool provider (required for agent RAG)
        - provider_id: rag-runtime
          provider_type: inline::rag-runtime
          config: {}
        
        # Model Context Protocol provider for external tools
        - provider_id: model-context-protocol
          provider_type: remote::model-context-protocol
          config: {}
      
      eval:
        # TrustyAI LM-Eval provider for model quality assessment
        # Phase 2: Llama Stack Integration
        - provider_id: trustyai-lmeval
          provider_type: remote::trustyai-lmeval
          config:
            url: "http://trustyai-lmeval-provider.private-ai-demo.svc:8080"
            namespace: "private-ai-demo"
            # Model configurations for evaluation
            models:
              - model_id: mistral-24b-quantized
                endpoint: "https://mistral-24b-quantized-private-ai-demo.apps.cluster-qtvt5.qtvt5.sandbox2082.opentlc.com/v1/completions"
                tokenizer: "RedHatAI/Mistral-Small-24B-Instruct-2501-quantized.w4a16"
                num_concurrent: 2
              - model_id: mistral-24b-full
                endpoint: "https://mistral-24b-private-ai-demo.apps.cluster-qtvt5.qtvt5.sandbox2082.opentlc.com/v1/completions"
                tokenizer: "mistralai/Mistral-Small-24B-Instruct-2501"
                num_concurrent: 4
            # Default evaluation settings
            default_sample_limit: 100
            default_batch_size: 1
            allow_online: true
            allow_code_execution: true
    
    models:
      # Quantized model - cost-efficient (1 GPU)
      - metadata:
          llama_model: RedHatAI/Mistral-Small-24B-Instruct-2501-quantized.w4a16
          description: "Quantized Mistral 24B from Stage 1 (compressed-tensors)"
          quantization: "compressed-tensors"
          gpu_count: 1
        model_id: mistral-24b-quantized
        model_type: llm
        provider_id: mistral-24b-quantized
        provider_model_id: mistral-24b-quantized
      
      # Full precision model - maximum quality (4 GPUs)
      - metadata:
          llama_model: mistralai/Mistral-Small-Instruct-2501
          description: "Full precision Mistral 24B from Stage 1"
          quantization: "none"
          gpu_count: 4
        model_id: mistral-24b-full
        model_type: llm
        provider_id: mistral-24b-full
        provider_model_id: mistral-24b
      
      # IBM Granite embedding model (768 dimensions)
      - metadata:
          embedding_dimension: 768
        model_id: ibm-granite/granite-embedding-125m-english
        model_type: embedding
        provider_id: sentence-transformers
        provider_model_id: ibm-granite/granite-embedding-125m-english
    
    tool_groups:
      # Built-in RAG tool group (required for Agent RAG)
      - toolgroup_id: builtin::rag
        provider_id: rag-runtime
      
      # MCP Servers - TEMPORARILY DISABLED
      # TODO: Enable after implementing SSE endpoints in MCP servers
      # Current MCP servers use simple HTTP, but Llama Stack requires SSE
      # Will be fixed in Phase 3
      
      # - toolgroup_id: mcp::database
      #   provider_id: model-context-protocol
      #   mcp_endpoint:
      #     uri: "http://database-mcp.private-ai-demo.svc.cluster.local:8080/sse"
      
      # - toolgroup_id: mcp::slack
      #   provider_id: model-context-protocol
      #   mcp_endpoint:
      #     uri: "http://slack-mcp.private-ai-demo.svc.cluster.local:8080/sse"
    
    vector_dbs:
      # Register vector database with IBM Granite embeddings (768 dimensions)
      - vector_db_id: rag_documents
        provider_id: milvus-shared
        embedding_model: ibm-granite/granite-embedding-125m-english
        embedding_dimension: 768
    
    server:
      port: 8321
