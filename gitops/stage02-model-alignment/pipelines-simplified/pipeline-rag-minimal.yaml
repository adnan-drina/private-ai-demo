apiVersion: tekton.dev/v1
kind: Pipeline
metadata:
  name: rag-ingestion-minimal
  namespace: private-ai-demo
spec:
  params:
    - name: milvus_uri
      type: string
      default: "tcp://milvus-standalone.private-ai-demo.svc.cluster.local:19530"
    - name: milvus_collection
      type: string
      default: "rag_documents_test"
    - name: min_entities
      type: string
      default: "5"
  
  tasks:
    - name: ingest-test-data
      params:
        - name: milvus_uri
          value: $(params.milvus_uri)
        - name: milvus_collection
          value: $(params.milvus_collection)
        - name: min_entities
          value: $(params.min_entities)
      taskSpec:
        params:
          - name: milvus_uri
          - name: milvus_collection
          - name: min_entities
        steps:
          - name: ingest-and-verify
            image: registry.access.redhat.com/ubi9/python-311:latest
            script: |
              #!/bin/bash
              set -e
              
              echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
              echo "RAG INGESTION TEST (Minimal)"
              echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
              
              pip install --quiet pymilvus sentence-transformers
              
              python3 << 'PYEOF'
              from pymilvus import connections, Collection, CollectionSchema, FieldSchema, DataType, utility
              from sentence_transformers import SentenceTransformer
              
              # Test data - same content as uploaded PDF
              test_chunks = [
                  "This is a sample document for testing the RAG (Retrieval-Augmented Generation) pipeline.",
                  "Purpose: Validate document ingestion and retrieval in the Private AI Demo system on OpenShift.",
                  "The RAG pipeline processes documents through several stages including document processing, embedding generation, vector storage, retrieval, and generation.",
                  "Docling converts PDFs to structured markdown for processing.",
                  "The Granite embedding model creates vector representations of text chunks.",
                  "Milvus stores embeddings with metadata for efficient similarity search.",
                  "Query vectors are used to find relevant document chunks from the vector database.",
                  "vLLM generates responses using the retrieved context from relevant documents.",
                  "When asked about the purpose of this document, the system should retrieve this section and respond that it is for validating document ingestion.",
                  "The embedding model creates high-dimensional vectors that capture semantic meaning."
              ]
              
              milvus_uri = "$(params.milvus_uri)"
              collection_name = "$(params.milvus_collection)"
              min_entities = int("$(params.min_entities)")
              
              print(f"\nğŸ“Š Test data: {len(test_chunks)} chunks")
              
              print("\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
              print("STEP 1: Generate embeddings")
              print("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
              
              print("Loading embedding model...")
              model = SentenceTransformer('all-MiniLM-L6-v2')
              
              embeddings = model.encode(test_chunks)
              print(f"âœ… Generated {len(embeddings)} embeddings (dim={embeddings.shape[1]})")
              
              print("\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
              print("STEP 2: Connect to Milvus")
              print("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
              
              print(f"Connecting to: {milvus_uri}")
              connections.connect(uri=milvus_uri, timeout=30)
              print("âœ… Connected")
              
              print("\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
              print("STEP 3: Create/verify collection")
              print("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
              
              if utility.has_collection(collection_name):
                  print(f"Collection '{collection_name}' already exists")
                  collection = Collection(collection_name)
                  print(f"Current entities: {collection.num_entities}")
              else:
                  print(f"Creating collection: {collection_name}")
                  fields = [
                      FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
                      FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=65535),
                      FieldSchema(name="source", dtype=DataType.VARCHAR, max_length=512),
                      FieldSchema(name="chunk_id", dtype=DataType.INT64),
                      FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=embeddings.shape[1])
                  ]
                  schema = CollectionSchema(fields, description="RAG test collection")
                  collection = Collection(name=collection_name, schema=schema)
                  
                  # Create index
                  index_params = {"metric_type": "L2", "index_type": "IVF_FLAT", "params": {"nlist": 128}}
                  collection.create_index("embedding", index_params)
                  print("âœ… Collection created with index")
              
              print("\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
              print("STEP 4: Insert data")
              print("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
              
              texts = test_chunks
              sources = ["test://rag-demo"] * len(test_chunks)
              chunk_ids = list(range(len(test_chunks)))
              embedding_list = embeddings.tolist()
              
              collection.insert([texts, sources, chunk_ids, embedding_list])
              collection.flush()
              
              num_entities = collection.num_entities
              print(f"âœ… Inserted {len(test_chunks)} chunks")
              print(f"   Total entities: {num_entities}")
              
              print("\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
              print("STEP 5: Verify ingestion")
              print("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
              
              success = num_entities >= min_entities
              if success:
                  print(f"âœ… VERIFICATION PASSED: {num_entities} >= {min_entities}")
              else:
                  print(f"âŒ VERIFICATION FAILED: {num_entities} < {min_entities}")
                  exit(1)
              
              print("\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
              print("STEP 6: Test retrieval")
              print("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
              
              collection.load()
              
              query_text = "What is the purpose of this document?"
              print(f"Query: '{query_text}'")
              
              query_embedding = model.encode([query_text])[0].tolist()
              
              results = collection.search(
                  data=[query_embedding],
                  anns_field="embedding",
                  param={"metric_type": "L2", "params": {"nprobe": 10}},
                  limit=3,
                  output_fields=["text", "source", "chunk_id"]
              )
              
              print(f"\nTop 3 results:")
              for i, hit in enumerate(results[0]):
                  print(f"\n{i+1}. Distance: {hit.distance:.4f}")
                  print(f"   Chunk ID: {hit.entity.get('chunk_id')}")
                  print(f"   Text: {hit.entity.get('text')}")
              
              print("\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
              print("âœ… PIPELINE COMPLETED SUCCESSFULLY!")
              print("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
              print(f"\nSummary:")
              print(f"  â€¢ Collection: {collection_name}")
              print(f"  â€¢ Total entities: {num_entities}")
              print(f"  â€¢ Retrieval test: PASSED")
              PYEOF
