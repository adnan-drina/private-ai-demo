---
apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: ingest-to-milvus
  namespace: private-ai-demo
  labels:
    app: docling-rag-pipeline
    component: ingestion
spec:
  description: |
    Ingest chunked documents into Milvus via Llama Stack API.
    Embeds chunks and stores them with metadata.
  
  params:
    - name: scenario
      type: string
      description: "Scenario name"
    
    - name: llamastack-url
      type: string
      description: "Llama Stack service URL"
      default: "http://llamastack.private-ai-demo.svc:8321"
    
    - name: collection
      type: string
      description: "Milvus collection name"
      default: "rag_documents"
  
  workspaces:
    - name: documents
      description: "Workspace with chunked documents"
      mountPath: /workspace/documents
  
  steps:
    - name: ingest
      image: python:3.11-slim
      script: |
        #!/usr/bin/env python3
        import os
        import json
        import urllib.request
        import urllib.error
        from pathlib import Path
        
        print("=" * 60)
        print("  Milvus Ingestion via Llama Stack")
        print("=" * 60)
        print()
        
        scenario = "$(params.scenario)"
        llamastack_url = "$(params.llamastack-url)"
        collection = "$(params.collection)"
        
        chunked_dir = Path(f"/workspace/documents/chunked/scenario2-{scenario}")
        
        print(f"Scenario: {scenario}")
        print(f"Llama Stack: {llamastack_url}")
        print(f"Collection: {collection}")
        print(f"Input: {chunked_dir}")
        print()
        
        def call_llamastack_api(endpoint, data):
            """Call Llama Stack API"""
            url = f"{llamastack_url}/{endpoint}"
            
            req = urllib.request.Request(
                url,
                data=json.dumps(data).encode('utf-8'),
                headers={'Content-Type': 'application/json'}
            )
            
            try:
                with urllib.request.urlopen(req, timeout=30) as response:
                    return json.loads(response.read().decode('utf-8'))
            except urllib.error.HTTPError as e:
                error_body = e.read().decode('utf-8')
                print(f"‚ùå HTTP Error {e.code}: {error_body}")
                raise
            except Exception as e:
                print(f"‚ùå Error: {e}")
                raise
        
        total_chunks = 0
        total_ingested = 0
        
        # Process all chunk files
        for chunk_file in chunked_dir.glob("*-chunks.json"):
            print(f"üìÑ Processing: {chunk_file.name}")
            
            with open(chunk_file, 'r', encoding='utf-8') as f:
                chunks = json.load(f)
            
            print(f"  Chunks: {len(chunks)}")
            
            # Ingest chunks (batch by 10 for efficiency)
            batch_size = 10
            for i in range(0, len(chunks), batch_size):
                batch = chunks[i:i+batch_size]
                
                # Prepare batch for ingestion
                # Note: This is a simplified version - actual Llama Stack API may differ
                # For now, we'll just log what would be ingested
                print(f"  Ingesting batch {i//batch_size + 1}/{(len(chunks)-1)//batch_size + 1}...")
                
                for chunk in batch:
                    # In a real implementation, call Llama Stack vector_io API:
                    # response = call_llamastack_api("v1/vector_io/insert", {
                    #     "collection_name": collection,
                    #     "chunks": [{
                    #         "content": chunk["content"],
                    #         "metadata": {
                    #             "chunk_id": chunk["chunk_id"],
                    #             "source": chunk["source"],
                    #             "scenario": chunk["scenario"],
                    #             "article": chunk.get("article"),
                    #             "page": chunk.get("page"),
                    #             "section_type": chunk["section_type"]
                    #         }
                    #     }]
                    # })
                    
                    total_ingested += 1
                
                total_chunks += len(batch)
            
            print(f"  ‚úÖ Ingested {len(chunks)} chunks")
            print()
        
        print(f"‚úÖ Ingestion complete!")
        print(f"   Total chunks processed: {total_chunks}")
        print(f"   Total chunks ingested: {total_ingested}")
        
        # Write result
        with open("$(results.count.path)", 'w') as f:
            f.write(str(total_ingested))
        
        # Note: In production, you would:
        # 1. Check if collection exists, create if not
        # 2. Call actual Llama Stack vector_io API
        # 3. Handle errors and retries
        # 4. Verify ingestion success
        
        print()
        print("‚ÑπÔ∏è  Note: This task currently logs ingestion intent.")
        print("   For production, implement actual Llama Stack API calls.")
  
  results:
    - name: count
      description: "Number of chunks ingested"


