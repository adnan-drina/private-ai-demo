# MinIO Object Storage for Model Weights

**Component:** S3-compatible object storage infrastructure  
**Stage:** stage00-ai-platform  
**Managed By:** ArgoCD (`app-stage00-minio`)

---

## Purpose

MinIO provides S3-compatible object storage for LLM model weights, solving the **node disk pressure problem** when deploying large models (>50GB).

### Why MinIO?

**Problem Solved:**
- Full-precision models (48GB weights → 87GB OCI images) cause node eviction during image pull
- g6.12xlarge nodes have 120GB disk, insufficient for 87GB image + extraction (~140-155GB total needed)
- Baking weights into container images (ModelCar pattern) doesn't scale beyond ~30GB

**Solution:**
- Store model weights in MinIO (S3-compatible object storage)
- Use lightweight runtime images (~5-10GB) that fetch weights at pod startup
- Mount weights to PVC (not node ephemeral storage)
- Scale to arbitrarily large models without node disk constraints

---

## Architecture

### Red Hat Pattern: Operator + Tenant CR

This deployment follows **Red Hat GitOps best practices**:

1. **Operator Layer (Platform Team):**
   - MinIO Operator installed cluster-wide (like GPU Operator, NFD)
   - Provides CRDs: `Tenant`, `TenantPool`, etc.
   - Requires cluster-admin privileges

2. **Tenant Layer (GitOps-Managed):**
   - `Tenant` CR defines MinIO instance (this directory)
   - Declarative: storage size, replicas, credentials
   - Synced by ArgoCD (no console clicks)

3. **Bootstrap Layer (Declarative):**
   - Job creates buckets and sets policies
   - Runs post-sync via ArgoCD hook
   - Fully reproducible across environments

### Resources

```
minio/
├── namespace.yaml              # model-storage namespace
├── tenant.yaml                 # MinIO Tenant CR (S3 instance)
├── job-bootstrap-buckets.yaml  # Create llm-models bucket
├── kustomization.yaml
└── README.md                   # This file
```

---

## MinIO Tenant Configuration

**Storage:**
- **Capacity:** 500Gi (single RWO PVC)
- **Storage Class:** `gp3-csi` (AWS gp3 SSD)
- **Scalability:** Increase `storage` in `tenant.yaml` or add more `pools`

**Deployment:**
- **Servers:** 1 (sufficient for demo/dev; scale for production HA)
- **Volumes per Server:** 1
- **Resources:** 2 CPU / 4Gi RAM (request), 4 CPU / 8Gi RAM (limit)

**Security:**
- **Credentials:** Auto-generated by operator → `minio-credentials` Secret
- **Service Account:** `minio-sa`
- **TLS:** Auto-certificate management via operator

**Services:**
- **API Endpoint:** `https://model-storage-hl.model-storage.svc.cluster.local:9000`
- **Console:** Web UI for admin (exposed internally)

---

## Buckets

### `llm-models`

**Purpose:** Store LLM model weights by name and version

**Path Structure:**
```
llm-models/
  ├── mistral-24b-full/
  │   ├── fp16-2501/
  │   │   ├── model.safetensors
  │   │   ├── config.json
  │   │   ├── tokenizer.json
  │   │   └── ...
  │   └── fp16-2502/
  │       └── ...
  └── llama-70b-full/
      └── ...
```

**Policy:**
- Authenticated read for all users (model-serving pods)
- Write access via pipeline service account
- Versioning enabled (track model updates)

**Created By:** `job-bootstrap-buckets.yaml` (ArgoCD PostSync hook)

---

## Pipeline Integration

### Upload Model Weights (New Task)

Pipeline task `upload-to-minio` will:

1. Download model from HuggingFace to PVC
2. Upload model files to MinIO:
   ```
   s3://llm-models/<model-name>/<version>/
   ```
3. Write metadata with MinIO path to Model Registry

### Build Runtime Image (New Task)

Pipeline task `build-runtime-image` will:

1. Create lightweight vLLM runtime image (~5-10GB)
2. Include:
   - vLLM server
   - MinIO client libraries
   - Model fetch script (`fetch_model.py`)
   - Serving entrypoint (`serve.py`)
3. Push to Quay.io

---

## Model Serving Integration

### InferenceService Spec (Full Model)

```yaml
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: mistral-24b-full
spec:
  predictor:
    model:
      # Lightweight runtime image (NOT 87GB model image!)
      storageUri: "oci://quay.io/adrina/mistral-24b-full-runtime:fp16-2501"
      
      # PVC for local weight caching
      storage:
        persistentVolumeClaim:
          claimName: mistral-24b-full-cache
          mountPath: /model-cache
      
      # MinIO connection
      env:
        - name: MINIO_ENDPOINT
          value: https://model-storage-hl.model-storage.svc.cluster.local:9000
        - name: MINIO_BUCKET
          value: llm-models
        - name: MODEL_PREFIX
          value: mistral-24b-full/fp16-2501/
        - name: MODEL_CACHE_DIR
          value: /model-cache
        - name: MINIO_ACCESS_KEY
          valueFrom:
            secretKeyRef:
              name: minio-credentials
              key: accesskey
        - name: MINIO_SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: minio-credentials
              key: secretkey
```

### Flow

1. **Pod starts:** Pulls lightweight runtime image (~5GB, no eviction!)
2. **Init:** Checks if model cached in PVC
3. **Fetch:** Downloads weights from MinIO to PVC (if not cached)
4. **Serve:** vLLM loads model from `/model-cache` and starts inference

---

## Deployment

### Prerequisites

**MinIO Operator must be installed** (platform team, cluster-admin):

```bash
# Option 1: Via OperatorHub (OpenShift Console)
# - Navigate to OperatorHub
# - Search for "MinIO Operator"
# - Install to all namespaces

# Option 2: Via CLI
oc apply -f https://github.com/minio/operator/blob/master/examples/kustomization/operator/kustomization.yaml
```

### Via ArgoCD (Recommended)

**ArgoCD Application:** `gitops/argocd/applications/stage00/app-stage00-minio.yaml`

```bash
# Apply ArgoCD application
oc apply -f gitops/argocd/applications/stage00/app-stage00-minio.yaml

# ArgoCD will sync:
# Wave 0: Namespace (model-storage)
# Wave 1: Tenant (MinIO instance)
# Wave 2: Bootstrap Job (create buckets)
```

### Manual Deployment (Dev/Test)

```bash
# Deploy MinIO resources
kubectl apply -k gitops/stage00-ai-platform/minio/

# Wait for Tenant to be ready
kubectl wait --for=condition=TenantReady tenant/model-storage -n model-storage --timeout=5m

# Verify buckets created
kubectl logs -n model-storage job/minio-bootstrap-buckets
```

---

## Verification

### Check Tenant Status

```bash
# Tenant CR status
oc get tenant model-storage -n model-storage -o yaml

# Tenant pods
oc get pods -n model-storage -l app=minio

# Tenant services
oc get svc -n model-storage
```

### Test S3 API

```bash
# Get credentials
MINIO_ACCESS_KEY=$(oc get secret minio-credentials -n model-storage -o jsonpath='{.data.accesskey}' | base64 -d)
MINIO_SECRET_KEY=$(oc get secret minio-credentials -n model-storage -o jsonpath='{.data.secretkey}' | base64 -d)

# Test with mc client (from inside cluster)
oc run -it --rm mc --image=quay.io/minio/mc:latest --restart=Never -- bash
mc alias set minio https://model-storage-hl.model-storage.svc.cluster.local:9000 $MINIO_ACCESS_KEY $MINIO_SECRET_KEY --insecure
mc ls minio/
mc ls minio/llm-models/
```

### Access Console

```bash
# Port forward to MinIO console
oc port-forward -n model-storage svc/model-storage-console 9090:9090

# Open browser: https://localhost:9090
# Login with minio-credentials
```

---

## Monitoring

### Metrics

MinIO exposes Prometheus metrics at `/minio/v2/metrics/cluster`:

```yaml
annotations:
  prometheus.io/scrape: "true"
  prometheus.io/port: "9000"
  prometheus.io/path: /minio/v2/metrics/cluster
```

### Key Metrics

- `minio_bucket_objects_count` - Number of objects per bucket
- `minio_bucket_usage_total_bytes` - Storage used per bucket
- `minio_s3_requests_total` - S3 API request count
- `minio_s3_errors_total` - S3 API error count

---

## Troubleshooting

### Tenant Not Ready

```bash
# Check operator logs
oc logs -n minio-operator -l app=minio-operator

# Check tenant events
oc describe tenant model-storage -n model-storage

# Check PVC
oc get pvc -n model-storage
```

### Bootstrap Job Failed

```bash
# Check job logs
oc logs -n model-storage job/minio-bootstrap-buckets

# Common issues:
# - Tenant not ready yet (job will retry)
# - Network policy blocking access
# - Invalid credentials

# Rerun bootstrap job
oc delete job minio-bootstrap-buckets -n model-storage
oc apply -f gitops/stage00-ai-platform/minio/job-bootstrap-buckets.yaml
```

### Model Upload Fails

```bash
# Test connectivity from pipeline pod
oc run test-minio --image=quay.io/minio/mc:latest -it --rm -- bash
mc alias set minio https://model-storage-hl.model-storage.svc.cluster.local:9000 <access-key> <secret-key> --insecure
mc ls minio/llm-models/

# Check network policies
oc get networkpolicies -n model-storage
oc get networkpolicies -n private-ai-demo
```

---

## Scaling

### Increase Storage Capacity

**Edit `tenant.yaml`:**
```yaml
spec:
  pools:
  - servers: 1
    volumesPerServer: 1
    volumeClaimTemplate:
      spec:
        resources:
          requests:
            storage: 1Ti  # Increased from 500Gi
```

**Apply via ArgoCD:**
```bash
# Commit changes to Git
git add gitops/stage00-ai-platform/minio/tenant.yaml
git commit -m "chore: increase MinIO storage to 1Ti"
git push

# ArgoCD auto-syncs (selfHeal: true)
# MinIO Operator will resize PVC (if storage class supports expansion)
```

### High Availability

**Multi-zone deployment:**
```yaml
spec:
  pools:
  - servers: 4  # 4 pods across zones
    volumesPerServer: 2  # 2 volumes per pod
    volumeClaimTemplate:
      spec:
        resources:
          requests:
            storage: 500Gi
```

**Results in:**
- 8 total volumes (4 servers × 2 volumes)
- Distributed across availability zones
- Automatic data replication (erasure coding)

---

## Related Documentation

- **Architecture Decision:** `docs/02-PIPELINES/MODELCAR-ARCHITECTURE-DECISION.md`
- **Path B Implementation:** `docs/02-PIPELINES/PATH-B-IMPLEMENTATION-PLAN.md`
- **Pipeline Architecture:** `docs/02-PIPELINES/MODELCAR-PIPELINE-ARCHITECTURE.md`
- **MinIO Operator Docs:** https://min.io/docs/minio/kubernetes/upstream/
- **Red Hat GitOps Practices:** https://developers.redhat.com/articles/2025/03/05/openshift-gitops-recommended-practices

---

## Key Benefits

✅ **No node disk pressure** - Weights stored in object storage, not container layers  
✅ **Scales to any model size** - 100GB+ models work without node disk changes  
✅ **Decouple runtime from weights** - Update independently  
✅ **Version control** - S3 versioning tracks model updates  
✅ **Production-grade** - Red Hat recommended pattern for large LLMs  
✅ **Fully GitOps** - Declarative, reproducible across environments  

**This is the Red Hat way.**

