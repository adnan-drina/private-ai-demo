---
apiVersion: trustyai.opendatahub.io/v1alpha1
kind: LMEvalJob
metadata:
  name: eval-mistral-full
  namespace: private-ai-demo
  labels:
    app.kubernetes.io/name: lm-eval
    app.kubernetes.io/component: trustyai-eval
    app.kubernetes.io/part-of: trustyai-evaluation
    model: mistral-24b
  annotations:
    description: "LM-Eval job for Mistral 24B Full Precision model (FP16, 4 GPUs)"
spec:
  # Model configuration (local vLLM endpoint with OpenAI-compatible API)
  # Using local-completions for loglikelihood support (required by arc_easy, hellaswag)
  model: local-completions
  
  modelArgs:
    - name: model
      value: "mistral-24b"
    - name: base_url
      value: "http://mistral-24b-predictor-00043-private.private-ai-demo.svc.cluster.local/v1/completions"
    - name: tokenizer
      value: "mistralai/Mistral-Small-24B-Instruct-2501"
    - name: num_concurrent
      value: "4"  # Optimal for 4 GPUs
    - name: max_retries
      value: "5"
    - name: timeout
      value: "600"
    - name: tokenized_requests
      value: "false"
  
  # Evaluation tasks (All 4 benchmarks)
  taskList:
    taskNames:
      - arc_easy        # General knowledge & reasoning
      - hellaswag       # Commonsense reasoning
      - gsm8k          # Math reasoning
      - truthfulqa_mc2 # Truthfulness assessment
  
  # Sample configuration (fast-track for demo)
  limit: "100"  # Samples per task (production)
  
  # Batching configuration
  batchSize: "1"  # Disabled for stability
  
  # Additional settings
  logSamples: true
  
  # Allow online access and code execution
  allowOnline: true
  allowCodeExecution: true
  
  # Storage configuration (operator-managed PVC)
  outputs:
    pvcManaged:
      size: 2Gi
  
  # Environment configuration (HF token & vLLM timeout only; operator handles online access)
  pod:
    container:
      env:
        - name: HF_TOKEN
          valueFrom:
            secretKeyRef:
              name: huggingface-token
              key: HF_TOKEN
        - name: VLLM_TIMEOUT
          value: "600"
      resources:
        requests:
          cpu: "1"
          memory: 4Gi
        limits:
          cpu: "2"
          memory: 6Gi

