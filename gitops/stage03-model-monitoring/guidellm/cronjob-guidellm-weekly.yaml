---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: guidellm-weekly-comprehensive
  namespace: private-ai-demo
  labels:
    app: guidellm
    schedule: weekly
  annotations:
    argocd.argoproj.io/sync-wave: "2"
    description: "Weekly comprehensive GuideLLM benchmarks (1-hour duration)"
spec:
  schedule: "0 0 * * 0"  # Run weekly on Sundays at midnight
  timeZone: "America/New_York"
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 4  # Keep last 4 weeks
  failedJobsHistoryLimit: 2
  jobTemplate:
    metadata:
      labels:
        app: guidellm
        schedule: weekly
    spec:
      backoffLimit: 2
      ttlSecondsAfterFinished: 2592000  # Keep for 30 days
      template:
        metadata:
          annotations:
            sidecar.istio.io/inject: "true"
          labels:
            app: guidellm
        spec:
          restartPolicy: OnFailure
          serviceAccountName: guidellm-runner
          
          initContainers:
          # Get URLs for both models
          - name: get-inference-urls
            image: registry.redhat.io/openshift4/ose-cli:latest
            command:
            - /bin/bash
            - -c
            - |
                echo "ðŸ” Writing canonical service URLs for GuideLLM weekly benchmarks"
                QUANTIZED_URL="http://mistral-24b-quantized.private-ai-demo.svc.cluster.local"
                FULL_URL="http://mistral-24b.private-ai-demo.svc.cluster.local"
                echo "âœ… Quantized: ${QUANTIZED_URL}"
                echo -n "${QUANTIZED_URL}" > /shared/quantized-url.txt
                echo "âœ… Full: ${FULL_URL}"
                echo -n "${FULL_URL}" > /shared/full-url.txt
            volumeMounts:
            - name: shared
              mountPath: /shared
          
          containers:
          # Comprehensive benchmark for Quantized Model
          - name: guidellm-quantized
            image: ghcr.io/vllm-project/guidellm:latest
            command:
            - /bin/bash
            - -c
            - |
              RATE=${GUIDELLM_RATE:-20}
              RATE_TYPE=${GUIDELLM_RATE_TYPE:-sweep}
              PROMPT_TOKENS=${GUIDELLM_PROMPT_TOKENS:-512}
              OUTPUT_TOKENS=${GUIDELLM_OUTPUT_TOKENS:-256}
              SAMPLES=${GUIDELLM_SAMPLES:-500}
              MAX_SECONDS=${GUIDELLM_MAX_SECONDS:-2400}

              MODEL_URL=$(cat /shared/quantized-url.txt)
              echo "ðŸš€ Running GuideLLM Comprehensive Benchmark (Quantized)"
              echo "Target: ${MODEL_URL}"
              
              TIMESTAMP=$(date +%Y%m%d)
              OUTPUT_FILE="/results/weekly-quantized-${TIMESTAMP}.json"
              guidellm benchmark \
                --target "${MODEL_URL}" \
                --model "mistral-24b-quantized" \
                --backend-type openai_http \
                --processor "mistralai/Mistral-Small-Instruct-2409" \
                --rate-type "${RATE_TYPE}" \
                --rate "${RATE}" \
                --max-seconds "${MAX_SECONDS}" \
                --data "prompt_tokens=${PROMPT_TOKENS},output_tokens=${OUTPUT_TOKENS},samples=${SAMPLES}" \
                --output-path "${OUTPUT_FILE}"

              HTML_FILE="/results/weekly-quantized-${TIMESTAMP}.html"
              BENCHMARK_FILE="${OUTPUT_FILE}" \
              BENCHMARK_MODEL="mistral-24b-quantized" \
              BENCHMARK_TYPE="weekly" \
              BENCHMARK_TIMESTAMP="${TIMESTAMP}" \
              HTML_FILE="${HTML_FILE}" \
              python - <<'PY'
                import json
                import os
                import pathlib
                import subprocess
                import sys
                from datetime import datetime
                from html import escape
                from textwrap import dedent
                from textwrap import dedent

                benchmark_file = os.environ["BENCHMARK_FILE"]
                model = os.environ.get("BENCHMARK_MODEL", "unknown")
                bench_type = os.environ.get("BENCHMARK_TYPE", "manual")
                timestamp = os.environ.get("BENCHMARK_TIMESTAMP", "")
                html_file = os.environ.get("HTML_FILE")
                pushgateway_url = os.environ.get(
                    "PUSHGATEWAY_URL",
                    "http://prometheus-pushgateway.private-ai-demo.svc.cluster.local:9091",
                )

                try:
                    with open(benchmark_file, "r", encoding="utf-8") as fh:
                        data = json.load(fh)
                except Exception as exc:  # noqa: BLE001
                    print(f"Unable to read benchmark output {benchmark_file}: {exc}")
                    sys.exit(0)

                benchmarks = data.get("benchmarks") or []
                if not benchmarks:
                    print(f"No benchmarks found in {benchmark_file}, skipping metrics export and HTML generation.")
                    sys.exit(0)

                metrics = benchmarks[0].get("metrics", {}) or {}

                def select_section(metric_dict):
                    if not isinstance(metric_dict, dict):
                        return None
                    section = metric_dict.get("total")
                    if isinstance(section, dict):
                        return section
                    section = metric_dict.get("successful")
                    if isinstance(section, dict):
                        return section
                    return metric_dict if isinstance(metric_dict, dict) else None

                def get_metric(key, percentile=None, scale=1.0):
                    metric = metrics.get(key)
                    section = select_section(metric)
                    if not section:
                        return None
                    value = None
                    if percentile:
                        percentiles = section.get("percentiles")
                        if isinstance(percentiles, dict):
                            value = percentiles.get(percentile)
                        if value is None:
                            value = section.get(percentile)
                    if value is None:
                        value = section.get("mean")
                    if value is None:
                        return None
                    try:
                        return float(value) * scale
                    except (TypeError, ValueError):
                        return None

                def compute_success_rate():
                    metric = metrics.get("requests_per_second")
                    if not isinstance(metric, dict):
                        return None
                    total = metric.get("total")
                    successful = metric.get("successful")
                    if isinstance(total, dict) and isinstance(successful, dict):
                        total_count = total.get("count")
                        success_count = successful.get("count")
                        if total_count:
                            return (success_count or 0) / total_count * 100.0
                    return None

                def compute_total_requests():
                    for candidate in ("request_latency", "requests_per_second"):
                        metric = metrics.get(candidate)
                        section = select_section(metric)
                        if isinstance(section, dict):
                            count = section.get("count")
                            if isinstance(count, (int, float)):
                                return int(count)
                    return None

                try:
                    from prometheus_client import CollectorRegistry, Gauge, push_to_gateway
                except ImportError:  # pragma: no cover
                    subprocess.check_call(
                        [sys.executable, "-m", "pip", "install", "--quiet", "prometheus-client"]
                    )
                    from prometheus_client import CollectorRegistry, Gauge, push_to_gateway

                registry = CollectorRegistry()

                def record(name: str, description: str, value):
                    gauge = Gauge(name, description, ["model", "benchmark_type"], registry=registry)
                    gauge.labels(model=model, benchmark_type=bench_type).set(value if value is not None else 0)

                prom_metrics = [
                    ("guidellm_request_latency_p50_seconds", "P50 request latency", get_metric("request_latency", "p50")),
                    ("guidellm_request_latency_p95_seconds", "P95 request latency", get_metric("request_latency", "p95")),
                    ("guidellm_request_latency_p99_seconds", "P99 request latency", get_metric("request_latency", "p99")),
                    ("guidellm_ttft_p50_seconds", "P50 time to first token", get_metric("time_to_first_token_ms", "p50", scale=0.001)),
                    ("guidellm_ttft_p95_seconds", "P95 time to first token", get_metric("time_to_first_token_ms", "p95", scale=0.001)),
                    ("guidellm_throughput_requests_per_second", "Requests per second", get_metric("requests_per_second")),
                    ("guidellm_throughput_tokens_per_second", "Tokens per second", get_metric("tokens_per_second") or get_metric("output_tokens_per_second")),
                    ("guidellm_request_success_rate", "Request success rate", compute_success_rate()),
                    ("guidellm_total_requests", "Total requests", compute_total_requests()),
                ]

                for metric_name, desc, value in prom_metrics:
                    record(metric_name, desc, value)

                try:
                    push_to_gateway(pushgateway_url, job="guidellm", registry=registry)
                    print(f"âœ… Exported metrics for {model} ({bench_type}) to {pushgateway_url}")
                except Exception as exc:  # noqa: BLE001
                    print(f"âš ï¸ Failed to push metrics to {pushgateway_url}: {exc}")

                token_rate = get_metric("tokens_per_second") or get_metric("output_tokens_per_second")

                table_rows = [
                    ("P50 request latency (s)", get_metric("request_latency", "p50")),
                    ("P95 request latency (s)", get_metric("request_latency", "p95")),
                    ("P99 request latency (s)", get_metric("request_latency", "p99")),
                    ("P50 time to first token (s)", get_metric("time_to_first_token_ms", "p50", scale=0.001)),
                    ("P95 time to first token (s)", get_metric("time_to_first_token_ms", "p95", scale=0.001)),
                    ("Requests per second", get_metric("requests_per_second")),
                    ("Tokens per second", token_rate),
                    ("Request success rate (%)", compute_success_rate()),
                    ("Total requests", compute_total_requests()),
                ]

                if html_file:
                    timestamp_display = timestamp or datetime.utcnow().strftime("%Y-%m-%d %H:%M:%SZ")
                    rows_html = "".join(
                        f"<tr><th>{escape(label)}</th><td>{escape('-' if value is None else f'{value:.4f}' if isinstance(value, float) else str(value))}</td></tr>"
                        for label, value in table_rows
                    )
                    html_doc = dedent(
                        f"""\
                        <!DOCTYPE html>
                        <html lang="en">
                          <head>
                            <meta charset="utf-8" />
                            <title>GuideLLM Benchmark Report - {escape(model)}</title>
                            <style>
                              body {{ font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif; margin: 2rem; }}
                              h1 {{ margin-bottom: 0.25rem; }}
                              table {{ border-collapse: collapse; margin-top: 1.5rem; width: 100%; max-width: 640px; }}
                              th, td {{ border: 1px solid #444; padding: 0.4rem 0.6rem; text-align: left; }}
                              th {{ background: #f0f0f0; width: 60%; }}
                              pre {{ background: #f8f8f8; padding: 1rem; overflow-x: auto; }}
                              footer {{ margin-top: 2rem; font-size: 0.85rem; color: #666; }}
                            </style>
                          </head>
                          <body>
                            <h1>GuideLLM Benchmark Report</h1>
                            <p><strong>Model:</strong> {escape(model)}</p>
                            <p><strong>Benchmark Type:</strong> {escape(bench_type)}</p>
                            <p><strong>Timestamp:</strong> {escape(timestamp_display)}</p>
                            <table>
                              <tbody>
                                {rows_html}
                              </tbody>
                            </table>
                            <h2>Raw Benchmark Payload</h2>
                            <pre>{escape(json.dumps(data, indent=2))}</pre>
                            <footer>Generated automatically by the GuideLLM automation pipeline.</footer>
                          </body>
                        </html>
                        """
                    )
                    pathlib.Path(html_file).write_text(html_doc, encoding="utf-8")
                    print(f"ðŸ“„ HTML report written to {html_file}")
              PY
            env:
            - name: GUIDELLM__ENV
              value: "prod"
            - name: GUIDELLM__OPENAI__VERIFY
              value: "false"
            - name: GUIDELLM__LOGGING__CONSOLE_LOG_LEVEL
              value: "INFO"
            - name: HF_TOKEN
              valueFrom:
                secretKeyRef:
                  name: huggingface-token
                  key: HF_TOKEN
            - name: PYTHONHTTPSVERIFY
              value: "0"
            - name: HOME
              value: "/tmp"
            - name: HF_HOME
              value: "/tmp/hf"
            - name: PUSHGATEWAY_URL
              value: "http://prometheus-pushgateway.private-ai-demo.svc.cluster.local:9091"
            envFrom:
            - configMapRef:
                name: guidellm-weekly-config
            volumeMounts:
            - name: shared
              mountPath: /shared
            - name: results
              mountPath: /results
            resources:
              requests:
                cpu: "200m"
                memory: "512Mi"
              limits:
                cpu: "1"
                memory: "2Gi"
          
          # Comprehensive benchmark for Full Model
          - name: guidellm-full
            image: ghcr.io/vllm-project/guidellm:latest
            command:
            - /bin/bash
            - -c
            - |
              RATE=${GUIDELLM_RATE:-20}
              RATE_TYPE=${GUIDELLM_RATE_TYPE:-sweep}
              PROMPT_TOKENS=${GUIDELLM_PROMPT_TOKENS:-512}
              OUTPUT_TOKENS=${GUIDELLM_OUTPUT_TOKENS:-256}
              SAMPLES=${GUIDELLM_SAMPLES:-500}
              MAX_SECONDS=${GUIDELLM_MAX_SECONDS:-3600}

              # Wait for quantized to finish
              sleep 120
              
              MODEL_URL=$(cat /shared/full-url.txt)
              echo "ðŸš€ Running GuideLLM Comprehensive Benchmark (Full Precision)"
              echo "Target: ${MODEL_URL}"
              
              TIMESTAMP=$(date +%Y%m%d)
              OUTPUT_FILE="/results/weekly-full-${TIMESTAMP}.json"
              guidellm benchmark \
                --target "${MODEL_URL}" \
                --model "mistral-24b" \
                --backend-type openai_http \
                --processor "mistralai/Mistral-Small-Instruct-2409" \
                --rate-type "${RATE_TYPE}" \
                --rate "${RATE}" \
                --max-seconds "${MAX_SECONDS}" \
                --data "prompt_tokens=${PROMPT_TOKENS},output_tokens=${OUTPUT_TOKENS},samples=${SAMPLES}" \
                --output-path "${OUTPUT_FILE}"

              HTML_FILE="/results/weekly-full-${TIMESTAMP}.html"
              BENCHMARK_FILE="${OUTPUT_FILE}" \
              BENCHMARK_MODEL="mistral-24b" \
              BENCHMARK_TYPE="weekly" \
              BENCHMARK_TIMESTAMP="${TIMESTAMP}" \
              HTML_FILE="${HTML_FILE}" \
              python - <<'PY'
                import json
                import os
                import pathlib
                import subprocess
                import sys
                from datetime import datetime
                from html import escape

                benchmark_file = os.environ["BENCHMARK_FILE"]
                model = os.environ.get("BENCHMARK_MODEL", "unknown")
                bench_type = os.environ.get("BENCHMARK_TYPE", "manual")
                timestamp = os.environ.get("BENCHMARK_TIMESTAMP", "")
                html_file = os.environ.get("HTML_FILE")
                pushgateway_url = os.environ.get(
                    "PUSHGATEWAY_URL",
                    "http://prometheus-pushgateway.private-ai-demo.svc.cluster.local:9091",
                )

                try:
                    with open(benchmark_file, "r", encoding="utf-8") as fh:
                        data = json.load(fh)
                except Exception as exc:  # noqa: BLE001
                    print(f"Unable to read benchmark output {benchmark_file}: {exc}")
                    sys.exit(0)

                benchmarks = data.get("benchmarks") or []
                if not benchmarks:
                    print(f"No benchmarks found in {benchmark_file}, skipping metrics export and HTML generation.")
                    sys.exit(0)

                metrics = benchmarks[0].get("metrics", {}) or {}

                def select_section(metric_dict):
                    if not isinstance(metric_dict, dict):
                        return None
                    section = metric_dict.get("total")
                    if isinstance(section, dict):
                        return section
                    section = metric_dict.get("successful")
                    if isinstance(section, dict):
                        return section
                    return metric_dict if isinstance(metric_dict, dict) else None

                def get_metric(key, percentile=None, scale=1.0):
                    metric = metrics.get(key)
                    section = select_section(metric)
                    if not section:
                        return None
                    value = None
                    if percentile:
                        percentiles = section.get("percentiles")
                        if isinstance(percentiles, dict):
                            value = percentiles.get(percentile)
                        if value is None:
                            value = section.get(percentile)
                    if value is None:
                        value = section.get("mean")
                    if value is None:
                        return None
                    try:
                        return float(value) * scale
                    except (TypeError, ValueError):
                        return None

                def compute_success_rate():
                    metric = metrics.get("requests_per_second")
                    if not isinstance(metric, dict):
                        return None
                    total = metric.get("total")
                    successful = metric.get("successful")
                    if isinstance(total, dict) and isinstance(successful, dict):
                        total_count = total.get("count")
                        success_count = successful.get("count")
                        if total_count:
                            return (success_count or 0) / total_count * 100.0
                    return None

                def compute_total_requests():
                    for candidate in ("request_latency", "requests_per_second"):
                        metric = metrics.get(candidate)
                        section = select_section(metric)
                        if isinstance(section, dict):
                            count = section.get("count")
                            if isinstance(count, (int, float)):
                                return int(count)
                    return None

                try:
                    from prometheus_client import CollectorRegistry, Gauge, push_to_gateway
                except ImportError:  # pragma: no cover
                    subprocess.check_call(
                        [sys.executable, "-m", "pip", "install", "--quiet", "prometheus-client"]
                    )
                    from prometheus_client import CollectorRegistry, Gauge, push_to_gateway

                registry = CollectorRegistry()

                def record(name: str, description: str, value):
                    gauge = Gauge(name, description, ["model", "benchmark_type"], registry=registry)
                    gauge.labels(model=model, benchmark_type=bench_type).set(value if value is not None else 0)

                prom_metrics = [
                    ("guidellm_request_latency_p50_seconds", "P50 request latency", get_metric("request_latency", "p50")),
                    ("guidellm_request_latency_p95_seconds", "P95 request latency", get_metric("request_latency", "p95")),
                    ("guidellm_request_latency_p99_seconds", "P99 request latency", get_metric("request_latency", "p99")),
                    ("guidellm_ttft_p50_seconds", "P50 time to first token", get_metric("time_to_first_token_ms", "p50", scale=0.001)),
                    ("guidellm_ttft_p95_seconds", "P95 time to first token", get_metric("time_to_first_token_ms", "p95", scale=0.001)),
                    ("guidellm_throughput_requests_per_second", "Requests per second", get_metric("requests_per_second")),
                    ("guidellm_throughput_tokens_per_second", "Tokens per second", get_metric("tokens_per_second") or get_metric("output_tokens_per_second")),
                    ("guidellm_request_success_rate", "Request success rate", compute_success_rate()),
                    ("guidellm_total_requests", "Total requests", compute_total_requests()),
                ]

                for metric_name, desc, value in prom_metrics:
                    record(metric_name, desc, value)

                try:
                    push_to_gateway(pushgateway_url, job="guidellm", registry=registry)
                    print(f"âœ… Exported metrics for {model} ({bench_type}) to {pushgateway_url}")
                except Exception as exc:  # noqa: BLE001
                    print(f"âš ï¸ Failed to push metrics to {pushgateway_url}: {exc}")

                token_rate = get_metric("tokens_per_second") or get_metric("output_tokens_per_second")

                table_rows = [
                    ("P50 request latency (s)", get_metric("request_latency", "p50")),
                    ("P95 request latency (s)", get_metric("request_latency", "p95")),
                    ("P99 request latency (s)", get_metric("request_latency", "p99")),
                    ("P50 time to first token (s)", get_metric("time_to_first_token_ms", "p50", scale=0.001)),
                    ("P95 time to first token (s)", get_metric("time_to_first_token_ms", "p95", scale=0.001)),
                    ("Requests per second", get_metric("requests_per_second")),
                    ("Tokens per second", token_rate),
                    ("Request success rate (%)", compute_success_rate()),
                    ("Total requests", compute_total_requests()),
                ]

                if html_file:
                    timestamp_display = timestamp or datetime.utcnow().strftime("%Y-%m-%d %H:%M:%SZ")
                    rows_html = "".join(
                        f"<tr><th>{escape(label)}</th><td>{escape('-' if value is None else f'{value:.4f}' if isinstance(value, float) else str(value))}</td></tr>"
                        for label, value in table_rows
                    )
                    html_doc = dedent(
                        f"""\
                        <!DOCTYPE html>
                        <html lang="en">
                          <head>
                            <meta charset="utf-8" />
                            <title>GuideLLM Benchmark Report - {escape(model)}</title>
                            <style>
                              body {{ font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif; margin: 2rem; }}
                              h1 {{ margin-bottom: 0.25rem; }}
                              table {{ border-collapse: collapse; margin-top: 1.5rem; width: 100%; max-width: 640px; }}
                              th, td {{ border: 1px solid #444; padding: 0.4rem 0.6rem; text-align: left; }}
                              th {{ background: #f0f0f0; width: 60%; }}
                              pre {{ background: #f8f8f8; padding: 1rem; overflow-x: auto; }}
                              footer {{ margin-top: 2rem; font-size: 0.85rem; color: #666; }}
                            </style>
                          </head>
                          <body>
                            <h1>GuideLLM Benchmark Report</h1>
                            <p><strong>Model:</strong> {escape(model)}</p>
                            <p><strong>Benchmark Type:</strong> {escape(bench_type)}</p>
                            <p><strong>Timestamp:</strong> {escape(timestamp_display)}</p>
                            <table>
                              <tbody>
                                {rows_html}
                              </tbody>
                            </table>
                            <h2>Raw Benchmark Payload</h2>
                            <pre>{escape(json.dumps(data, indent=2))}</pre>
                            <footer>Generated automatically by the GuideLLM automation pipeline.</footer>
                          </body>
                        </html>
                        """
                    )
                    pathlib.Path(html_file).write_text(html_doc, encoding="utf-8")
                    print(f"ðŸ“„ HTML report written to {html_file}")
              PY
            env:
            - name: GUIDELLM__ENV
              value: "prod"
            - name: GUIDELLM__OPENAI__VERIFY
              value: "false"
            - name: GUIDELLM__LOGGING__CONSOLE_LOG_LEVEL
              value: "INFO"
            - name: HF_TOKEN
              valueFrom:
                secretKeyRef:
                  name: huggingface-token
                  key: HF_TOKEN
            - name: PYTHONHTTPSVERIFY
              value: "0"
            - name: HOME
              value: "/tmp"
            - name: HF_HOME
              value: "/tmp/hf"
            - name: PUSHGATEWAY_URL
              value: "http://prometheus-pushgateway.private-ai-demo.svc.cluster.local:9091"
            envFrom:
            - configMapRef:
                name: guidellm-weekly-config
            volumeMounts:
            - name: shared
              mountPath: /shared
            - name: results
              mountPath: /results
            resources:
              requests:
                cpu: "200m"
                memory: "512Mi"
              limits:
                cpu: "1"
                memory: "2Gi"
          
          # Upload to MinIO
          - name: s3-uploader
            image: quay.io/minio/mc:latest
            command:
            - /bin/sh
            - -c
            - |
              # Wait for both benchmarks (JSON + HTML for each run)
              while [ $(ls -1 /results/*.json 2>/dev/null | wc -l) -lt 2 ] || \
                    [ $(ls -1 /results/*.html 2>/dev/null | wc -l) -lt 2 ]; do
                echo "Waiting for comprehensive benchmarks..."
                sleep 60
              done
              
              mc alias set minio http://minio.model-storage.svc.cluster.local:9000 \
                $MINIO_ACCESS_KEY $MINIO_SECRET_KEY
              
              mc cp /results/*.json minio/guidellm-results/weekly/
              mc cp /results/*.html minio/guidellm-results/weekly/
              
              echo "âœ… Weekly comprehensive benchmarks uploaded"
            env:
            - name: MINIO_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: llama-files-credentials
                  key: accesskey
            - name: MINIO_SECRET_KEY
              valueFrom:
                secretKeyRef:
                  name: llama-files-credentials
                  key: secretkey
            volumeMounts:
            - name: results
              mountPath: /results
            resources:
              requests:
                cpu: "100m"
                memory: "128Mi"
              limits:
                cpu: "500m"
                memory: "512Mi"
          
          volumes:
          - name: shared
            emptyDir: {}  # Shared volume for passing URLs between containers
          - name: results
            emptyDir: {}  # Ephemeral storage, results uploaded to MinIO S3

