---
apiVersion: v1
kind: ConfigMap
metadata:
  name: guidellm-metrics-exporter
  namespace: private-ai-demo
  labels:
    app: guidellm
data:
  export_metrics.py: |
    #!/usr/bin/env python3
    """
    GuideLLM Metrics Exporter
    
    Parses GuideLLM JSON output and pushes metrics to Prometheus Pushgateway.
    """
    import json
    import glob
    import time
    import os
    import sys
    from datetime import datetime
    
    try:
        from prometheus_client import CollectorRegistry, Gauge, push_to_gateway
    except ImportError:
        print("Installing prometheus_client...")
        os.system("pip install --quiet prometheus-client")
        from prometheus_client import CollectorRegistry, Gauge, push_to_gateway
    
    PUSHGATEWAY_URL = os.getenv('PUSHGATEWAY_URL', 'http://prometheus-pushgateway.private-ai-demo.svc.cluster.local:9091')
    RESULTS_DIR = os.getenv('RESULTS_DIR', '/results')
    
    # Create Prometheus metrics
    registry = CollectorRegistry()
    
    # Latency metrics
    latency_p50 = Gauge('guidellm_request_latency_p50_seconds', 
                        'P50 request latency', ['model', 'benchmark_type'], registry=registry)
    latency_p95 = Gauge('guidellm_request_latency_p95_seconds', 
                        'P95 request latency', ['model', 'benchmark_type'], registry=registry)
    latency_p99 = Gauge('guidellm_request_latency_p99_seconds', 
                        'P99 request latency', ['model', 'benchmark_type'], registry=registry)
    
    # TTFT (Time to First Token)
    ttft_p50 = Gauge('guidellm_ttft_p50_seconds', 
                     'P50 time to first token', ['model', 'benchmark_type'], registry=registry)
    ttft_p95 = Gauge('guidellm_ttft_p95_seconds', 
                     'P95 time to first token', ['model', 'benchmark_type'], registry=registry)
    
    # Throughput metrics
    throughput_rps = Gauge('guidellm_throughput_requests_per_second', 
                           'Requests per second', ['model', 'benchmark_type'], registry=registry)
    throughput_tps = Gauge('guidellm_throughput_tokens_per_second', 
                           'Tokens per second', ['model', 'benchmark_type'], registry=registry)
    
    # Success rate
    success_rate = Gauge('guidellm_request_success_rate', 
                         'Request success rate (0-1)', ['model', 'benchmark_type'], registry=registry)
    
    # Total requests
    total_requests = Gauge('guidellm_total_requests', 
                           'Total requests in benchmark', ['model', 'benchmark_type'], registry=registry)
    
    def parse_guidellm_output(json_file):
        """Parse GuideLLM JSON output and extract metrics."""
        try:
            with open(json_file, 'r') as f:
                data = json.load(f)
            
            # Extract model name and benchmark type
            model = data.get('model', 'unknown')
            benchmark_type = 'unknown'
            
            if 'quantized' in json_file:
                model = 'mistral-24b-quantized'
            elif 'full' in json_file:
                model = 'mistral-24b-full'
            
            if 'daily' in json_file:
                benchmark_type = 'daily'
            elif 'weekly' in json_file:
                benchmark_type = 'weekly'
            else:
                benchmark_type = 'manual'
            
            # Extract metrics from benchmarks array
            benchmarks = data.get('benchmarks', [])
            if not benchmarks:
                print(f"No benchmarks found in {json_file}")
                return None
            
            # Use the first benchmark's metrics
            bench = benchmarks[0]
            metrics = bench.get('metrics', {})
            
            return {
                'model': model,
                'benchmark_type': benchmark_type,
                'latency_p50': metrics.get('request_latency_p50', 0),
                'latency_p95': metrics.get('request_latency_p95', 0),
                'latency_p99': metrics.get('request_latency_p99', 0),
                'ttft_p50': metrics.get('time_to_first_token_p50', 0),
                'ttft_p95': metrics.get('time_to_first_token_p95', 0),
                'throughput_rps': metrics.get('request_throughput', 0),
                'throughput_tps': metrics.get('token_throughput', 0),
                'success_rate': metrics.get('success_rate', 1.0),
                'total_requests': metrics.get('total_requests', 0)
            }
        
        except Exception as e:
            print(f"Error parsing {json_file}: {e}")
            return None
    
    def export_metrics(metrics_data):
        """Export metrics to Prometheus Pushgateway."""
        try:
            model = metrics_data['model']
            bench_type = metrics_data['benchmark_type']
            
            # Set metric values
            latency_p50.labels(model=model, benchmark_type=bench_type).set(metrics_data['latency_p50'])
            latency_p95.labels(model=model, benchmark_type=bench_type).set(metrics_data['latency_p95'])
            latency_p99.labels(model=model, benchmark_type=bench_type).set(metrics_data['latency_p99'])
            ttft_p50.labels(model=model, benchmark_type=bench_type).set(metrics_data['ttft_p50'])
            ttft_p95.labels(model=model, benchmark_type=bench_type).set(metrics_data['ttft_p95'])
            throughput_rps.labels(model=model, benchmark_type=bench_type).set(metrics_data['throughput_rps'])
            throughput_tps.labels(model=model, benchmark_type=bench_type).set(metrics_data['throughput_tps'])
            success_rate.labels(model=model, benchmark_type=bench_type).set(metrics_data['success_rate'])
            total_requests.labels(model=model, benchmark_type=bench_type).set(metrics_data['total_requests'])
            
            # Push to Pushgateway
            push_to_gateway(PUSHGATEWAY_URL, job='guidellm', registry=registry)
            
            print(f"‚úÖ Exported metrics for {model} ({bench_type}) to Pushgateway")
            
        except Exception as e:
            print(f"Error exporting metrics: {e}")
    
    def main():
        """Main loop to watch for new JSON files and export metrics."""
        print(f"üöÄ GuideLLM Metrics Exporter started")
        print(f"üìÇ Watching directory: {RESULTS_DIR}")
        print(f"üìä Pushgateway URL: {PUSHGATEWAY_URL}")
        
        processed_files = set()
        
        while True:
            try:
                # Find all JSON files
                json_files = glob.glob(f"{RESULTS_DIR}/*.json")
                
                for json_file in json_files:
                    if json_file in processed_files:
                        continue
                    
                    if json_file.endswith('.processed'):
                        continue
                    
                    print(f"üìÑ Processing: {json_file}")
                    
                    # Parse and export metrics
                    metrics_data = parse_guidellm_output(json_file)
                    if metrics_data:
                        export_metrics(metrics_data)
                        processed_files.add(json_file)
                        
                        # Mark as processed (optional)
                        try:
                            os.rename(json_file, json_file + '.processed')
                        except:
                            pass
                
                # Sleep for 1 minute
                time.sleep(60)
            
            except KeyboardInterrupt:
                print("\nüëã Metrics exporter stopped")
                sys.exit(0)
            except Exception as e:
                print(f"‚ùå Error in main loop: {e}")
                time.sleep(60)
    
    if __name__ == '__main__':
        main()

