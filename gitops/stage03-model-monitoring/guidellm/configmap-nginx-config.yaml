---
apiVersion: v1
kind: ConfigMap
metadata:
  name: guidellm-nginx-config
  namespace: private-ai-demo
  labels:
    app: guidellm-reports
data:
  nginx.conf: |
    worker_processes auto;
    error_log /var/log/nginx/error.log warn;
    pid /tmp/nginx.pid;
    
    events {
        worker_connections 1024;
    }
    
    http {
        include /etc/nginx/mime.types;
        default_type application/octet-stream;
        
        log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                        '$status $body_bytes_sent "$http_referer" '
                        '"$http_user_agent" "$http_x_forwarded_for"';
        
        access_log /var/log/nginx/access.log main;
        
        sendfile on;
        tcp_nopush on;
        tcp_nodelay on;
        keepalive_timeout 65;
        types_hash_max_size 2048;
        
        # Temp paths for non-root user
        client_body_temp_path /tmp/client_temp;
        proxy_temp_path /tmp/proxy_temp;
        fastcgi_temp_path /tmp/fastcgi_temp;
        uwsgi_temp_path /tmp/uwsgi_temp;
        scgi_temp_path /tmp/scgi_temp;
        
        server {
            listen 8080;
            server_name _;
            
            # Root directory for serving files
            root /usr/share/nginx/html;
            index index.html;
            
            # Enable directory listing for reports
            location /reports/ {
                alias /usr/share/nginx/html/;
                autoindex on;
                autoindex_exact_size off;
                autoindex_format html;
                autoindex_localtime on;
            }
            
            # Serve index page
            location = / {
                try_files /index.html =404;
            }
            
            # Proxy GuideLLM UI assets from CDN (redirect v0.3.0 to latest)
            location ~ ^/guidellm/ui/ {
                # Rewrite v0.3.0 to latest
                rewrite ^/guidellm/ui/v[0-9.]+/(.*)$ /guidellm/ui/latest/$1 break;
                proxy_pass https://blog.vllm.ai;
                proxy_ssl_verify off;
                proxy_set_header Host blog.vllm.ai;
                proxy_set_header X-Real-IP $remote_addr;
                proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
                proxy_set_header X-Forwarded-Proto $scheme;
            }
            
            # Serve HTML reports directly
            location ~ \.html$ {
                add_header Content-Type text/html;
                add_header Cache-Control "no-cache, no-store, must-revalidate";
                add_header X-Frame-Options "ALLOWALL" always;
                add_header Access-Control-Allow-Origin "*" always;
                add_header Content-Security-Policy "default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; script-src * 'unsafe-inline' 'unsafe-eval'; style-src * 'unsafe-inline';" always;
                
                # Rewrite CDN URLs in HTML to use local proxy
                sub_filter_types text/html;
                sub_filter 'https://blog.vllm.ai/guidellm/ui/' '/guidellm/ui/';
               sub_filter_once off;
                
                # NOTE: GuideLLM embeds data in window.runInfo, window.workloadDetails, window.benchmarks
                # The Next.js UI framework from CDN should read these automatically
            }
            
            # Serve JSON data
            location ~ \.json$ {
                add_header Content-Type application/json;
                add_header Cache-Control "no-cache, no-store, must-revalidate";
            }
            
            # Health check endpoint
            location /healthz {
                access_log off;
                return 200 "healthy\n";
                add_header Content-Type text/plain;
            }
        }
    }

