---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: guidellm-daily-benchmark
  namespace: private-ai-demo
  labels:
    app: guidellm
    schedule: daily
  annotations:
    argocd.argoproj.io/sync-wave: "2"
    description: "Daily GuideLLM benchmarks for all models"
spec:
  schedule: "0 2 * * *"  # Run daily at 2 AM
  timeZone: "America/New_York"
  concurrencyPolicy: Forbid  # Don't run multiple benchmarks simultaneously
  successfulJobsHistoryLimit: 7  # Keep last 7 successful runs
  failedJobsHistoryLimit: 3  # Keep last 3 failed runs
  jobTemplate:
    metadata:
      labels:
        app: guidellm
        schedule: daily
    spec:
      backoffLimit: 2
      ttlSecondsAfterFinished: 604800  # Keep completed jobs for 7 days
      template:
        metadata:
          annotations:
            sidecar.istio.io/inject: "true"
          labels:
            app: guidellm
        spec:
          restartPolicy: OnFailure
          serviceAccountName: guidellm-runner
          
          initContainers:
          # Get URLs for both models
          - name: get-inference-urls
            image: registry.redhat.io/openshift4/ose-cli:latest
            command:
            - /bin/bash
            - -c
            - |
                echo "ðŸ” Writing canonical service URLs for GuideLLM benchmarks"
                QUANTIZED_URL="http://mistral-24b-quantized.private-ai-demo.svc.cluster.local"
                FULL_URL="http://mistral-24b.private-ai-demo.svc.cluster.local"
                echo "âœ… Quantized: ${QUANTIZED_URL}"
                echo -n "${QUANTIZED_URL}" > /shared/quantized-url.txt
                echo "âœ… Full: ${FULL_URL}"
                echo -n "${FULL_URL}" > /shared/full-url.txt
            volumeMounts:
            - name: shared
              mountPath: /shared
          
          containers:
          # Benchmark Quantized Model
          - name: guidellm-quantized
            image: ghcr.io/vllm-project/guidellm:latest
            command:
            - /bin/bash
            - -c
            - |
              RATE=${GUIDELLM_RATE:-10}
              RATE_TYPE=${GUIDELLM_RATE_TYPE:-sweep}
              PROMPT_TOKENS=${GUIDELLM_PROMPT_TOKENS:-256}
              OUTPUT_TOKENS=${GUIDELLM_OUTPUT_TOKENS:-128}
              SAMPLES=${GUIDELLM_SAMPLES:-100}
              MAX_SECONDS=${GUIDELLM_MAX_SECONDS:-1800}

              MODEL_URL=$(cat /shared/quantized-url.txt)
              echo "ðŸš€ Running GuideLLM Benchmark (Quantized)"
              echo "Target: ${MODEL_URL}"
              
              TIMESTAMP=$(date +%Y%m%d-%H%M%S)
              OUTPUT_FILE="/results/daily-quantized-${TIMESTAMP}.json"
              guidellm benchmark \
                --target "${MODEL_URL}" \
                --model "mistral-24b-quantized" \
                --backend-type openai_http \
                --processor "mistralai/Mistral-Small-Instruct-2409" \
                --rate-type "${RATE_TYPE}" \
                --rate "${RATE}" \
                --max-seconds "${MAX_SECONDS}" \
                --data "prompt_tokens=${PROMPT_TOKENS},output_tokens=${OUTPUT_TOKENS},samples=${SAMPLES}" \
                --output-path "${OUTPUT_FILE}"

              HTML_FILE="/results/daily-quantized-${TIMESTAMP}.html"
              BENCHMARK_FILE="${OUTPUT_FILE}" \
              BENCHMARK_MODEL="mistral-24b-quantized" \
              BENCHMARK_TYPE="daily" \
              BENCHMARK_TIMESTAMP="${TIMESTAMP}" \
              HTML_FILE="${HTML_FILE}" \
              python - <<'PY'
                import json
                import os
                import pathlib
                import subprocess
                import sys
                from datetime import datetime
                from html import escape
                from textwrap import dedent
                from textwrap import dedent

                benchmark_file = os.environ["BENCHMARK_FILE"]
                model = os.environ.get("BENCHMARK_MODEL", "unknown")
                bench_type = os.environ.get("BENCHMARK_TYPE", "manual")
                timestamp = os.environ.get("BENCHMARK_TIMESTAMP", "")
                html_file = os.environ.get("HTML_FILE")
                pushgateway_url = os.environ.get(
                    "PUSHGATEWAY_URL",
                    "http://prometheus-pushgateway.private-ai-demo.svc.cluster.local:9091",
                )

                try:
                    with open(benchmark_file, "r", encoding="utf-8") as fh:
                        data = json.load(fh)
                except Exception as exc:  # noqa: BLE001
                    print(f"Unable to read benchmark output {benchmark_file}: {exc}")
                    sys.exit(0)

                benchmarks = data.get("benchmarks") or []
                if not benchmarks:
                    print(f"No benchmarks found in {benchmark_file}, skipping metrics export and HTML generation.")
                    sys.exit(0)

                metrics = benchmarks[0].get("metrics", {})

                try:
                    from prometheus_client import CollectorRegistry, Gauge, push_to_gateway
                except ImportError:  # pragma: no cover
                    subprocess.check_call(
                        [sys.executable, "-m", "pip", "install", "--quiet", "prometheus-client"]
                    )
                    from prometheus_client import CollectorRegistry, Gauge, push_to_gateway

                registry = CollectorRegistry()

                def record(name: str, description: str, value):
                    gauge = Gauge(name, description, ["model", "benchmark_type"], registry=registry)
                    gauge.labels(model=model, benchmark_type=bench_type).set(value if value is not None else 0)

                metric_map = [
                    ("guidellm_request_latency_p50_seconds", "P50 request latency", "request_latency_p50"),
                    ("guidellm_request_latency_p95_seconds", "P95 request latency", "request_latency_p95"),
                    ("guidellm_request_latency_p99_seconds", "P99 request latency", "request_latency_p99"),
                    ("guidellm_ttft_p50_seconds", "P50 time to first token", "time_to_first_token_p50"),
                    ("guidellm_ttft_p95_seconds", "P95 time to first token", "time_to_first_token_p95"),
                    ("guidellm_throughput_requests_per_second", "Requests per second", "request_throughput"),
                    ("guidellm_throughput_tokens_per_second", "Tokens per second", "token_throughput"),
                    ("guidellm_request_success_rate", "Request success rate", "success_rate"),
                    ("guidellm_total_requests", "Total requests", "total_requests"),
                ]

                for metric_name, desc, key in metric_map:
                    record(metric_name, desc, metrics.get(key))

                try:
                    push_to_gateway(pushgateway_url, job="guidellm", registry=registry)
                    print(f"âœ… Exported metrics for {model} ({bench_type}) to {pushgateway_url}")
                except Exception as exc:  # noqa: BLE001
                    print(f"âš ï¸ Failed to push metrics to {pushgateway_url}: {exc}")

                if html_file:
                    rows = []
                    for _metric_name, friendly, key in metric_map:
                        value = metrics.get(key)
                        if isinstance(value, float):
                            value_str = f"{value:.4f}"
                        else:
                            value_str = "-" if value in (None, "") else str(value)
                        rows.append(f"<tr><th>{escape(friendly)}</th><td>{escape(value_str)}</td></tr>")

                    timestamp_display = timestamp or datetime.utcnow().strftime("%Y-%m-%d %H:%M:%SZ")
                    html_doc = dedent(
                        f"""\
                        <!DOCTYPE html>
                        <html lang="en">
                          <head>
                            <meta charset="utf-8" />
                            <title>GuideLLM Benchmark Report - {escape(model)}</title>
                            <style>
                              body {{ font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif; margin: 2rem; }}
                              h1 {{ margin-bottom: 0.25rem; }}
                              table {{ border-collapse: collapse; margin-top: 1.5rem; width: 100%; max-width: 640px; }}
                              th, td {{ border: 1px solid #444; padding: 0.4rem 0.6rem; text-align: left; }}
                              th {{ background: #f0f0f0; width: 60%; }}
                              pre {{ background: #f8f8f8; padding: 1rem; overflow-x: auto; }}
                              footer {{ margin-top: 2rem; font-size: 0.85rem; color: #666; }}
                            </style>
                          </head>
                          <body>
                            <h1>GuideLLM Benchmark Report</h1>
                            <p><strong>Model:</strong> {escape(model)}</p>
                            <p><strong>Benchmark Type:</strong> {escape(bench_type)}</p>
                            <p><strong>Timestamp:</strong> {escape(timestamp_display)}</p>

                            <table>
                              <tbody>
                                {"".join(rows)}
                              </tbody>
                            </table>

                            <h2>Raw Benchmark Payload</h2>
                            <pre>{escape(json.dumps(data, indent=2))}</pre>

                            <footer>Generated automatically by the GuideLLM automation pipeline.</footer>
                          </body>
                        </html>
                        """
                    )
                    pathlib.Path(html_file).write_text(html_doc, encoding="utf-8")
                    print(f"ðŸ“„ HTML report written to {html_file}")
              PY
            env:
            - name: GUIDELLM__ENV
              value: "prod"
            - name: GUIDELLM__OPENAI__VERIFY
              value: "false"
            - name: HF_TOKEN
              valueFrom:
                secretKeyRef:
                  name: huggingface-token
                  key: HF_TOKEN
            - name: PYTHONHTTPSVERIFY
              value: "0"
            - name: HOME
              value: "/tmp"
            - name: HF_HOME
              value: "/tmp/hf"
            - name: PUSHGATEWAY_URL
              value: "http://prometheus-pushgateway.private-ai-demo.svc.cluster.local:9091"
            envFrom:
            - configMapRef:
                name: guidellm-daily-config
            volumeMounts:
            - name: shared
              mountPath: /shared
            - name: results
              mountPath: /results
            resources:
              requests:
                cpu: "200m"
                memory: "512Mi"
              limits:
                cpu: "1"
                memory: "2Gi"
          
          # Benchmark Full Precision Model
          - name: guidellm-full
            image: ghcr.io/vllm-project/guidellm:latest
            command:
            - /bin/bash
            - -c
            - |
              RATE=${GUIDELLM_RATE:-10}
              RATE_TYPE=${GUIDELLM_RATE_TYPE:-sweep}
              PROMPT_TOKENS=${GUIDELLM_PROMPT_TOKENS:-256}
              OUTPUT_TOKENS=${GUIDELLM_OUTPUT_TOKENS:-128}
              SAMPLES=${GUIDELLM_SAMPLES:-100}
              MAX_SECONDS=${GUIDELLM_MAX_SECONDS:-1800}

              # Wait for quantized benchmark to finish
              sleep 60
              
              MODEL_URL=$(cat /shared/full-url.txt)
              echo "ðŸš€ Running GuideLLM Benchmark (Full Precision)"
              echo "Target: ${MODEL_URL}"
              
              TIMESTAMP=$(date +%Y%m%d-%H%M%S)
              OUTPUT_FILE="/results/daily-full-${TIMESTAMP}.json"
              guidellm benchmark \
                --target "${MODEL_URL}" \
                --model "mistral-24b" \
                --backend-type openai_http \
                --processor "mistralai/Mistral-Small-Instruct-2409" \
                --rate-type "${RATE_TYPE}" \
                --rate "${RATE}" \
                --max-seconds "${MAX_SECONDS}" \
                --data "prompt_tokens=${PROMPT_TOKENS},output_tokens=${OUTPUT_TOKENS},samples=${SAMPLES}" \
                --output-path "${OUTPUT_FILE}"

              HTML_FILE="/results/daily-full-${TIMESTAMP}.html"
              BENCHMARK_FILE="${OUTPUT_FILE}" \
              BENCHMARK_MODEL="mistral-24b" \
              BENCHMARK_TYPE="daily" \
              BENCHMARK_TIMESTAMP="${TIMESTAMP}" \
              HTML_FILE="${HTML_FILE}" \
              python - <<'PY'
                import json
                import os
                import pathlib
                import subprocess
                import sys
                from datetime import datetime
                from html import escape

                benchmark_file = os.environ["BENCHMARK_FILE"]
                model = os.environ.get("BENCHMARK_MODEL", "unknown")
                bench_type = os.environ.get("BENCHMARK_TYPE", "manual")
                timestamp = os.environ.get("BENCHMARK_TIMESTAMP", "")
                html_file = os.environ.get("HTML_FILE")
                pushgateway_url = os.environ.get(
                    "PUSHGATEWAY_URL",
                    "http://prometheus-pushgateway.private-ai-demo.svc.cluster.local:9091",
                )

                try:
                    with open(benchmark_file, "r", encoding="utf-8") as fh:
                        data = json.load(fh)
                except Exception as exc:  # noqa: BLE001
                    print(f"Unable to read benchmark output {benchmark_file}: {exc}")
                    sys.exit(0)

                benchmarks = data.get("benchmarks") or []
                if not benchmarks:
                    print(f"No benchmarks found in {benchmark_file}, skipping metrics export and HTML generation.")
                    sys.exit(0)

                metrics = benchmarks[0].get("metrics", {})

                try:
                    from prometheus_client import CollectorRegistry, Gauge, push_to_gateway
                except ImportError:  # pragma: no cover
                    subprocess.check_call(
                        [sys.executable, "-m", "pip", "install", "--quiet", "prometheus-client"]
                    )
                    from prometheus_client import CollectorRegistry, Gauge, push_to_gateway

                registry = CollectorRegistry()

                def record(name: str, description: str, value):
                    gauge = Gauge(name, description, ["model", "benchmark_type"], registry=registry)
                    gauge.labels(model=model, benchmark_type=bench_type).set(value if value is not None else 0)

                metric_map = [
                    ("guidellm_request_latency_p50_seconds", "P50 request latency", "request_latency_p50"),
                    ("guidellm_request_latency_p95_seconds", "P95 request latency", "request_latency_p95"),
                    ("guidellm_request_latency_p99_seconds", "P99 request latency", "request_latency_p99"),
                    ("guidellm_ttft_p50_seconds", "P50 time to first token", "time_to_first_token_p50"),
                    ("guidellm_ttft_p95_seconds", "P95 time to first token", "time_to_first_token_p95"),
                    ("guidellm_throughput_requests_per_second", "Requests per second", "request_throughput"),
                    ("guidellm_throughput_tokens_per_second", "Tokens per second", "token_throughput"),
                    ("guidellm_request_success_rate", "Request success rate", "success_rate"),
                    ("guidellm_total_requests", "Total requests", "total_requests"),
                ]

                for metric_name, desc, key in metric_map:
                    record(metric_name, desc, metrics.get(key))

                try:
                    push_to_gateway(pushgateway_url, job="guidellm", registry=registry)
                    print(f"âœ… Exported metrics for {model} ({bench_type}) to {pushgateway_url}")
                except Exception as exc:  # noqa: BLE001
                    print(f"âš ï¸ Failed to push metrics to {pushgateway_url}: {exc}")

                if html_file:
                    rows = []
                    for _metric_name, friendly, key in metric_map:
                        value = metrics.get(key)
                        if isinstance(value, float):
                            value_str = f"{value:.4f}"
                        else:
                            value_str = "-" if value in (None, "") else str(value)
                        rows.append(f"<tr><th>{escape(friendly)}</th><td>{escape(value_str)}</td></tr>")

                    timestamp_display = timestamp or datetime.utcnow().strftime("%Y-%m-%d %H:%M:%SZ")
                    html_doc = dedent(
                        f"""\
                        <!DOCTYPE html>
                        <html lang="en">
                          <head>
                            <meta charset="utf-8" />
                            <title>GuideLLM Benchmark Report - {escape(model)}</title>
                            <style>
                              body {{ font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif; margin: 2rem; }}
                              h1 {{ margin-bottom: 0.25rem; }}
                              table {{ border-collapse: collapse; margin-top: 1.5rem; width: 100%; max-width: 640px; }}
                              th, td {{ border: 1px solid #444; padding: 0.4rem 0.6rem; text-align: left; }}
                              th {{ background: #f0f0f0; width: 60%; }}
                              pre {{ background: #f8f8f8; padding: 1rem; overflow-x: auto; }}
                              footer {{ margin-top: 2rem; font-size: 0.85rem; color: #666; }}
                            </style>
                          </head>
                          <body>
                            <h1>GuideLLM Benchmark Report</h1>
                            <p><strong>Model:</strong> {escape(model)}</p>
                            <p><strong>Benchmark Type:</strong> {escape(bench_type)}</p>
                            <p><strong>Timestamp:</strong> {escape(timestamp_display)}</p>

                            <table>
                              <tbody>
                                {"".join(rows)}
                              </tbody>
                            </table>

                            <h2>Raw Benchmark Payload</h2>
                            <pre>{escape(json.dumps(data, indent=2))}</pre>

                            <footer>Generated automatically by the GuideLLM automation pipeline.</footer>
                          </body>
                        </html>
                        """
                    )
                    pathlib.Path(html_file).write_text(html_doc, encoding="utf-8")
                    print(f"ðŸ“„ HTML report written to {html_file}")
              PY
            env:
            - name: GUIDELLM__ENV
              value: "prod"
            - name: GUIDELLM__OPENAI__VERIFY
              value: "false"
            - name: HF_TOKEN
              valueFrom:
                secretKeyRef:
                  name: huggingface-token
                  key: HF_TOKEN
            - name: PYTHONHTTPSVERIFY
              value: "0"
            - name: HOME
              value: "/tmp"
            - name: HF_HOME
              value: "/tmp/hf"
            - name: PUSHGATEWAY_URL
              value: "http://prometheus-pushgateway.private-ai-demo.svc.cluster.local:9091"
            envFrom:
            - configMapRef:
                name: guidellm-daily-config
            volumeMounts:
            - name: shared
              mountPath: /shared
            - name: results
              mountPath: /results
            resources:
              requests:
                cpu: "200m"
                memory: "512Mi"
              limits:
                cpu: "1"
                memory: "2Gi"
          
          # Upload to MinIO
          - name: s3-uploader
            image: quay.io/minio/mc:latest
            command:
            - /bin/sh
            - -c
            - |
              # Wait for both benchmarks to complete (JSON + HTML for each run)
              while [ $(ls -1 /results/*.json 2>/dev/null | wc -l) -lt 2 ] || \
                    [ $(ls -1 /results/*.html 2>/dev/null | wc -l) -lt 2 ]; do
                echo "Waiting for benchmarks to complete..."
                sleep 30
              done
              
              # Configure MinIO
              mc alias set minio http://minio.private-ai-demo.svc.cluster.local:9000 \
                $MINIO_ACCESS_KEY $MINIO_SECRET_KEY
              
              # Upload all results (JSON + HTML artifacts)
              mc cp /results/*.json minio/guidellm-results/daily/
              mc cp /results/*.html minio/guidellm-results/daily/
              
              echo "âœ… Daily benchmarks uploaded successfully"
              mc ls minio/guidellm-results/daily/
            env:
            - name: MINIO_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: minio-credentials
                  key: accesskey
            - name: MINIO_SECRET_KEY
              valueFrom:
                secretKeyRef:
                  name: minio-credentials
                  key: secretkey
            volumeMounts:
            - name: results
              mountPath: /results
            resources:
              requests:
                cpu: "100m"
                memory: "128Mi"
              limits:
                cpu: "500m"
                memory: "512Mi"
          
          volumes:
          - name: shared
            emptyDir: {}  # Shared volume for passing URLs between containers
          - name: results
            emptyDir: {}  # Ephemeral storage, results uploaded to MinIO S3

