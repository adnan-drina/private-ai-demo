---
apiVersion: grafana.integreatly.org/v1beta1
kind: GrafanaDashboard
metadata:
  name: llama-stack-dashboard-enhanced
  namespace: private-ai-demo
spec:
  instanceSelector:
    matchLabels:
      dashboards: "grafana"
  json: |
    {
      "title": "Llama Stack Operations",
      "uid": "llama-ops",
      "timezone": "browser",
      "schemaVersion": 38,
      "version": 2,
      "refresh": "30s",
      "tags": ["llama-stack", "operations"],
      "panels": [
        {
          "id": 1,
          "type": "stat",
          "title": "Request Rate (req/s)",
          "datasource": {"type": "prometheus", "uid": "otel-prometheus"},
          "gridPos": {"h": 4, "w": 6, "x": 0, "y": 0},
          "targets": [
            {
              "refId": "A",
              "expr": "sum(rate(http_request_duration_highr_seconds_count{job=\"private-ai-demo/vllm-predictor-metrics\"}[$__rate_interval]))"
            }
          ],
          "fieldConfig": {"defaults": {"unit": "reqps", "decimals": 2}},
          "options": {
            "reduceOptions": {"calcs": ["lastNotNull"], "values": false},
            "orientation": "horizontal",
            "colorMode": "value",
            "graphMode": "none",
            "textMode": "value"
          }
        },
        {
          "id": 2,
          "type": "stat",
          "title": "Tokens / second",
          "datasource": {"type": "prometheus", "uid": "otel-prometheus"},
          "gridPos": {"h": 4, "w": 6, "x": 6, "y": 0},
          "targets": [
            {
              "refId": "A",
              "expr": "sum(rate(vllm:generation_tokens_total{job=\"private-ai-demo/vllm-predictor-metrics\"}[$__rate_interval]))"
            }
          ],
          "fieldConfig": {"defaults": {"unit": "ops", "decimals": 1}},
          "options": {
            "reduceOptions": {"calcs": ["lastNotNull"], "values": false},
            "orientation": "horizontal",
            "colorMode": "value",
            "graphMode": "none",
            "textMode": "value"
          }
        },
        {
          "id": 3,
          "type": "stat",
          "title": "Guardrail Error Rate",
          "datasource": {"type": "prometheus", "uid": "otel-prometheus"},
          "gridPos": {"h": 4, "w": 6, "x": 12, "y": 0},
          "targets": [
            {
              "refId": "A",
              "expr": "100 * sum(sum_over_time(client_request_duration_count{job=\"fms_guardrails_orchestr8\",status!=\"200 OK\"}[5m])) / clamp_min(sum(sum_over_time(client_request_duration_count{job=\"fms_guardrails_orchestr8\"}[5m])), 1)"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "percent",
              "thresholds": {
                "mode": "absolute",
                "steps": [
                  {"value": 0, "color": "green"},
                  {"value": 2, "color": "yellow"},
                  {"value": 5, "color": "red"}
                ]
              }
            }
          },
          "options": {
            "reduceOptions": {"calcs": ["lastNotNull"], "values": false},
            "orientation": "horizontal",
            "colorMode": "value",
            "graphMode": "none",
            "textMode": "value"
          }
        },
        {
          "id": 4,
          "type": "stat",
          "title": "Guardrail Throughput",
          "datasource": {"type": "prometheus", "uid": "otel-prometheus"},
          "gridPos": {"h": 4, "w": 6, "x": 18, "y": 0},
          "targets": [
            {
              "refId": "A",
              "expr": "sum(sum_over_time(client_request_duration_count{job=\"fms_guardrails_orchestr8\"}[5m])) / 300"
            }
          ],
          "fieldConfig": {"defaults": {"unit": "reqps", "decimals": 2}},
          "options": {
            "reduceOptions": {"calcs": ["lastNotNull"], "values": false},
            "orientation": "horizontal",
            "colorMode": "value",
            "graphMode": "none",
            "textMode": "value"
          }
        },
        {
          "id": 5,
          "type": "timeseries",
          "title": "Prefill Latency (ms)",
          "datasource": {"type": "prometheus", "uid": "otel-prometheus"},
          "gridPos": {"h": 8, "w": 12, "x": 0, "y": 4},
          "targets": [
            {
              "refId": "A",
              "expr": "histogram_quantile(0.50, sum(rate(vllm:request_prefill_time_seconds_bucket{job=\"private-ai-demo/vllm-predictor-metrics\"}[$__rate_interval])) by (le)) * 1000",
              "legendFormat": "P50"
            },
            {
              "refId": "B",
              "expr": "histogram_quantile(0.95, sum(rate(vllm:request_prefill_time_seconds_bucket{job=\"private-ai-demo/vllm-predictor-metrics\"}[$__rate_interval])) by (le)) * 1000",
              "legendFormat": "P95"
            }
          ],
          "fieldConfig": {"defaults": {"unit": "ms", "custom": {"drawStyle": "line", "fillOpacity": 15}}}
        },
        {
          "id": 6,
          "type": "timeseries",
          "title": "Time to First Token (ms)",
          "datasource": {"type": "prometheus", "uid": "otel-prometheus"},
          "gridPos": {"h": 8, "w": 12, "x": 12, "y": 4},
          "targets": [
            {
              "refId": "A",
              "expr": "histogram_quantile(0.50, sum(rate(vllm:time_to_first_token_seconds_bucket{job=\"private-ai-demo/vllm-predictor-metrics\"}[$__rate_interval])) by (le)) * 1000",
              "legendFormat": "P50"
            },
            {
              "refId": "B",
              "expr": "histogram_quantile(0.95, sum(rate(vllm:time_to_first_token_seconds_bucket{job=\"private-ai-demo/vllm-predictor-metrics\"}[$__rate_interval])) by (le)) * 1000",
              "legendFormat": "P95"
            }
          ],
          "fieldConfig": {"defaults": {"unit": "ms", "custom": {"drawStyle": "line", "fillOpacity": 15}}}
        },
        {
          "id": 7,
          "type": "timeseries",
          "title": "Queue Time (P95)",
          "datasource": {"type": "prometheus", "uid": "otel-prometheus"},
          "gridPos": {"h": 8, "w": 12, "x": 0, "y": 12},
          "targets": [
            {
              "refId": "A",
              "expr": "histogram_quantile(0.95, sum(rate(vllm:request_queue_time_seconds_bucket{job=\"private-ai-demo/vllm-predictor-metrics\"}[$__rate_interval])) by (le)) * 1000",
              "legendFormat": "P95"
            }
          ],
          "fieldConfig": {"defaults": {"unit": "ms"}}
        },
        {
          "id": 8,
          "type": "timeseries",
          "title": "Tokens / s (per model)",
          "datasource": {"type": "prometheus", "uid": "otel-prometheus"},
          "gridPos": {"h": 8, "w": 12, "x": 12, "y": 12},
          "targets": [
            {
              "refId": "A",
              "expr": "sum by (model_name) (rate(vllm:generation_tokens_total{job=\"private-ai-demo/vllm-predictor-metrics\"}[$__rate_interval]))",
              "legendFormat": "{{model_name}}"
            }
          ],
          "fieldConfig": {"defaults": {"unit": "ops", "custom": {"drawStyle": "line", "fillOpacity": 10}}}
        },
        {
          "id": 9,
          "type": "timeseries",
          "title": "Per-Pod Request Rate",
          "datasource": {"type": "prometheus", "uid": "otel-prometheus"},
          "gridPos": {"h": 8, "w": 12, "x": 0, "y": 20},
          "targets": [
            {
              "refId": "A",
              "expr": "sum by (pod) (rate(http_request_duration_highr_seconds_count{job=\"private-ai-demo/vllm-predictor-metrics\"}[$__rate_interval]))",
              "legendFormat": "{{pod}}"
            }
          ],
          "fieldConfig": {"defaults": {"unit": "reqps", "custom": {"drawStyle": "line", "fillOpacity": 10}}}
        },
        {
          "id": 10,
          "type": "timeseries",
          "title": "Guardrail Latency (ms)",
          "datasource": {"type": "prometheus", "uid": "otel-prometheus"},
          "gridPos": {"h": 8, "w": 12, "x": 12, "y": 20},
          "targets": [
            {
              "refId": "A",
              "expr": "quantile_over_time(0.50, (client_request_duration_sum{job=\"fms_guardrails_orchestr8\"}/client_request_duration_count{job=\"fms_guardrails_orchestr8\"})[5m])",
              "legendFormat": "P50"
            },
            {
              "refId": "B",
              "expr": "quantile_over_time(0.95, (client_request_duration_sum{job=\"fms_guardrails_orchestr8\"}/client_request_duration_count{job=\"fms_guardrails_orchestr8\"})[5m])",
              "legendFormat": "P95"
            }
          ],
          "fieldConfig": {"defaults": {"unit": "ms", "custom": {"drawStyle": "line", "fillOpacity": 10}}}
        },
        {
          "id": 11,
          "type": "timeseries",
          "title": "GPU Utilization",
          "datasource": {"type": "prometheus", "uid": "otel-prometheus"},
          "gridPos": {"h": 8, "w": 12, "x": 0, "y": 28},
          "targets": [
            {
              "refId": "A",
              "expr": "DCGM_FI_DEV_GPU_UTIL{job=\"private-ai-demo/nvidia-dcgm-metrics\",exported_namespace=\"private-ai-demo\"}",
              "legendFormat": "{{exported_pod}} GPU {{gpu}}"
            }
          ],
          "fieldConfig": {"defaults": {"unit": "percent", "custom": {"drawStyle": "line", "fillOpacity": 8}}}
        },
        {
          "id": 12,
          "type": "timeseries",
          "title": "GPU Memory (MiB)",
          "datasource": {"type": "prometheus", "uid": "otel-prometheus"},
          "gridPos": {"h": 8, "w": 12, "x": 12, "y": 28},
          "targets": [
            {
              "refId": "A",
              "expr": "DCGM_FI_DEV_FB_USED{job=\"private-ai-demo/nvidia-dcgm-metrics\",exported_namespace=\"private-ai-demo\"} / 1024",
              "legendFormat": "{{exported_pod}} GPU {{gpu}}"
            }
          ],
          "fieldConfig": {"defaults": {"unit": "mebibytes", "custom": {"drawStyle": "line", "fillOpacity": 8}}}
        }
      ]
    }

