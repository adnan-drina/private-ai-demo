apiVersion: v1
kind: ConfigMap
metadata:
  name: notebook-01-vllm-benchmark
  namespace: private-ai-demo
  labels:
    app.kubernetes.io/name: notebook
    app.kubernetes.io/component: stage1-demo
    app.kubernetes.io/part-of: private-ai-demo
  annotations:
    description: Stage 1 Benchmark - Technical results only
data:
  01-benchmark.ipynb: |
    {
      "cells": [
        {
          "cell_type": "markdown",
          "metadata": {},
          "source": [
            "# \ud83d\ude80 vLLM Performance Benchmark - GuideLLM Results\n",
            "\n",
            "## Red Hat OpenShift AI - Stage 1: Sovereign AI Foundation\n",
            "\n",
            "---\n",
            "\n",
            "This notebook shows **professional benchmark results** using GuideLLM to compare:\n",
            "\n",
            "- **Quantized Model:** Mistral 24B quantized (w4a16) on 1 GPU (g6.4xlarge)\n",
            "- **Full Precision Model:** Mistral 24B full on 4 GPUs (g6.12xlarge)\n",
            "\n",
            "**Configured Test Levels:** 1, 5, 10, 25 concurrent requests\n",
            "\n",
            "**Metrics:**\n",
            "- \ud83d\udcca Throughput (tokens/second)\n",
            "- \u23f1\ufe0f Latency (TTFT - Time To First Token P99, ITL - Inter-Token Latency P50)\n",
            "- \ud83d\udcb0 Cost efficiency"
          ]
        },
        {
          "cell_type": "code",
          "execution_count": null,
          "metadata": {},
          "outputs": [],
          "source": [
            "# Setup\n",
            "import json\n",
            "import pandas as pd\n",
            "from pathlib import Path\n",
            "\n",
            "print(\"\u2705 Libraries loaded\")"
          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {},
          "source": [
            "---\n",
            "\n",
            "## \ud83d\udd04 Re-run Benchmarks (Optional)\n",
            "\n",
            "Trigger new benchmark runs by deleting and recreating the GuideLLM jobs."
          ]
        },
        {
          "cell_type": "code",
          "execution_count": null,
          "metadata": {},
          "outputs": [],
          "source": [
            "import subprocess\n",
            "import time\n",
            "\n",
            "def run_benchmarks():\n",
            "    \"\"\"Delete existing jobs and trigger new benchmark runs\"\"\"\n",
            "    print(\"\ud83d\uddd1\ufe0f  Cleaning up old jobs...\")\n",
            "    subprocess.run([\"oc\", \"delete\", \"jobs\", \"-n\", \"private-ai-demo\", \"-l\", \"app=guidellm-benchmark\", \"--ignore-not-found=true\"], check=False)\n",
            "    subprocess.run([\"oc\", \"delete\", \"jobs\", \"-n\", \"private-ai-demo\", \"copy-benchmark-results\", \"--ignore-not-found=true\"], check=False)\n",
            "    time.sleep(5)\n",
            "    \n",
            "    print(\"\\n\ud83d\ude80 Starting new benchmark runs...\")\n",
            "    result_q = subprocess.run([\"oc\", \"apply\", \"-f\", \"/opt/app-root/src/gitops/components/benchmarking/job-guidellm-quantized.yaml\"], capture_output=True, text=True)\n",
            "    result_f = subprocess.run([\"oc\", \"apply\", \"-f\", \"/opt/app-root/src/gitops/components/benchmarking/job-guidellm-full.yaml\"], capture_output=True, text=True)\n",
            "    \n",
            "    if result_q.returncode == 0 and result_f.returncode == 0:\n",
            "        print(\"\u2705 Benchmark jobs created successfully\")\n",
            "        print(\"\\n\u23f1\ufe0f  Benchmarks will take ~10 minutes to complete\")\n",
            "        print(\"\\nMonitor progress with: oc get jobs,pods -n private-ai-demo -l app=guidellm-benchmark\")\n",
            "        print(\"\\n\ud83d\udcdd After benchmarks complete, run copy job to make results available:\")\n",
            "        print(\"   oc apply -f /opt/app-root/src/gitops/components/benchmarking/job-copy-results.yaml\")\n",
            "    else:\n",
            "        print(\"\u274c Failed to create jobs\")\n",
            "        if result_q.returncode != 0:\n",
            "            print(f\"Quantized: {result_q.stderr}\")\n",
            "        if result_f.returncode != 0:\n",
            "            print(f\"Full: {result_f.stderr}\")\n",
            "\n",
            "# Uncomment to trigger new benchmark runs\n",
            "# run_benchmarks()"
          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {},
          "source": [
            "---\n",
            "\n",
            "## \ud83d\udcca Load Benchmark Results from PVCs"
          ]
        },
        {
          "cell_type": "code",
          "execution_count": null,
          "metadata": {},
          "outputs": [],
          "source": [
            "import os\n",
            "from pathlib import Path\n",
            "\n",
            "# Load results directly from mounted shared PVC\n",
            "# After benchmarks complete, run the copy job to populate this PVC:\n",
            "#   oc apply -f /opt/app-root/src/gitops/components/benchmarking/job-copy-results.yaml\n",
            "#\n",
            "# Results will be available at:\n",
            "#   - /opt/app-root/src/benchmark-results/quantized/\n",
            "#   - /opt/app-root/src/benchmark-results/full/\n",
            "\n",
            "print(\"\ud83d\udcce Loading benchmark results from shared PVC...\\n\")\n",
            "\n",
            "quantized_file = Path('benchmark-results/quantized/mistral-24b-quantized-benchmark.json')\n",
            "full_file = Path('benchmark-results/full/mistral-24b-benchmark.json')\n",
            "\n",
            "# Check if results exist\n",
            "if quantized_file.exists():\n",
            "    print(f\"\u2705 Found quantized results: {quantized_file}\")\n",
            "else:\n",
            "    print(f\"\u26a0\ufe0f  Quantized results not found at {quantized_file}\")\n",
            "\n",
            "if full_file.exists():\n",
            "    print(f\"\u2705 Found full precision results: {full_file}\")\n",
            "else:\n",
            "    print(f\"\u26a0\ufe0f  Full precision results not found at {full_file}\")\n",
            "\n",
            "# Load the results\n",
            "if quantized_file.exists() and full_file.exists():\n",
            "    with open(quantized_file) as f:\n",
            "        quantized_data = json.load(f)\n",
            "    with open(full_file) as f:\n",
            "        full_data = json.load(f)\n",
            "    print(\"\\n\u2705 Benchmark results loaded successfully!\")\n",
            "    print(f\"   Quantized: {len(quantized_data.get('benchmarks', []))} benchmark runs\")\n",
            "    print(f\"   Full: {len(full_data.get('benchmarks', []))} benchmark runs\")\n",
            "else:\n",
            "    print(\"\\n\u26a0\ufe0f  Benchmark result files not found\")\n",
            "    print(\"   Make sure benchmark jobs have completed successfully\")\n",
            "    print(\"   Results are stored in dedicated PVCs:\")\n",
            "    print(\"     - benchmark-quantized-results\")\n",
            "    print(\"     - benchmark-full-results\")\n",
            "    quantized_data = None\n",
            "    full_data = None"
          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {},
          "source": [
            "---\n",
            "\n",
            "## \ud83d\udcc8 Extract and Map to Target Concurrency Levels"
          ]
        },
        {
          "cell_type": "code",
          "execution_count": null,
          "metadata": {},
          "outputs": [],
          "source": [
            "# Target concurrency levels we configured\n",
            "TARGET_LEVELS = [1, 5, 10, 25]\n",
            "\n",
            "def map_to_target_level(actual_conc):\n",
            "    \"\"\"Map actual concurrency to nearest target level\"\"\"\n",
            "    if actual_conc < 0.5:\n",
            "        return None  # Skip warmup/cooldown phases (< 0.5)\n",
            "    # Find nearest target level\n",
            "    return min(TARGET_LEVELS, key=lambda x: abs(x - actual_conc))\n",
            "\n",
            "def extract_metrics(data, model_name):\n",
            "    results = {}\n",
            "    for benchmark in data['benchmarks']:\n",
            "        m = benchmark['metrics']\n",
            "        actual_conc = m['request_concurrency']['successful']['mean']\n",
            "        target_conc = map_to_target_level(actual_conc)\n",
            "        \n",
            "        if target_conc is None:\n",
            "            continue  # Skip warmup phases\n",
            "        \n",
            "        # Keep the best result for each target level\n",
            "        if target_conc not in results or abs(actual_conc - target_conc) < abs(results[target_conc]['_actual'] - target_conc):\n",
            "            results[target_conc] = {\n",
            "                '_actual': actual_conc,\n",
            "                'Throughput': round(m['tokens_per_second']['successful']['mean'], 1),\n",
            "                'TTFT_P99': round(m['time_to_first_token_ms']['successful']['percentiles']['p99'], 1),\n",
            "                'ITL_P50': round(m['inter_token_latency_ms']['successful']['median'], 1),\n",
            "            }\n",
            "    return results\n",
            "\n",
            "if quantized_data and full_data:\n",
            "    quant_metrics = extract_metrics(quantized_data, 'Quantized')\n",
            "    full_metrics = extract_metrics(full_data, 'Full')\n",
            "    \n",
            "    # Build metric-first comparison table using TARGET_LEVELS\n",
            "    comparison = []\n",
            "    for target in TARGET_LEVELS:\n",
            "        row = {'Concurrency': target}\n",
            "        \n",
            "        # Throughput (Full then Quantized)\n",
            "        if target in full_metrics:\n",
            "            row['Full Thr'] = full_metrics[target]['Throughput']\n",
            "        if target in quant_metrics:\n",
            "            row['Quant Thr'] = quant_metrics[target]['Throughput']\n",
            "        \n",
            "        # TTFT P99 (Full then Quantized)\n",
            "        if target in full_metrics:\n",
            "            row['Full TTFT'] = full_metrics[target]['TTFT_P99']\n",
            "        if target in quant_metrics:\n",
            "            row['Quant TTFT'] = quant_metrics[target]['TTFT_P99']\n",
            "        \n",
            "        # ITL P50 (Full then Quantized)\n",
            "        if target in full_metrics:\n",
            "            row['Full ITL'] = full_metrics[target]['ITL_P50']\n",
            "        if target in quant_metrics:\n",
            "            row['Quant ITL'] = quant_metrics[target]['ITL_P50']\n",
            "        \n",
            "        # Only include row if we have data\n",
            "        if len(row) > 1:\n",
            "            comparison.append(row)\n",
            "    \n",
            "    df_comparison = pd.DataFrame(comparison)\n",
            "    print(f\"\u2705 Comparison table ready ({len(comparison)} concurrency levels)\")\n",
            "else:\n",
            "    df_comparison = None\n",
            "    quant_metrics = None\n",
            "    full_metrics = None"
          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {},
          "source": [
            "---\n",
            "\n",
            "## \ud83d\udcca Performance Results\n",
            "\n",
            "**Column Legend:**\n",
            "- **Thr** = Throughput (tokens/second)\n",
            "- **TTFT** = Time To First Token P99 (milliseconds)\n",
            "- **ITL** = Inter-Token Latency P50 (milliseconds)\n",
            "\n",
            "For each metric, **Full** (4 GPUs) is shown first, then **Quant** (1 GPU)"
          ]
        },
        {
          "cell_type": "code",
          "execution_count": null,
          "metadata": {},
          "outputs": [],
          "source": [
            "if df_comparison is not None:\n",
            "    display(df_comparison)\n",
            "else:\n",
            "    print(\"\u274c No data\")"
          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {},
          "source": [
            "---\n",
            "\n",
            "## \ud83d\udcb0 Cost Analysis"
          ]
        },
        {
          "cell_type": "code",
          "execution_count": null,
          "metadata": {},
          "outputs": [],
          "source": [
            "if quant_metrics and full_metrics:\n",
            "    cost_1gpu = 1.84  # g6.4xlarge $/hour\n",
            "    cost_4gpu = 5.52  # g6.12xlarge $/hour\n",
            "    \n",
            "    # Use concurrency level 10 for cost comparison\n",
            "    target_conc = 10\n",
            "    \n",
            "    if target_conc in quant_metrics and target_conc in full_metrics:\n",
            "        tps_quant = quant_metrics[target_conc]['Throughput']\n",
            "        tps_full = full_metrics[target_conc]['Throughput']\n",
            "        \n",
            "        cost_quant_1m = (cost_1gpu / (tps_quant * 3600)) * 1_000_000\n",
            "        cost_full_1m = (cost_4gpu / (tps_full * 3600)) * 1_000_000\n",
            "        \n",
            "        cost_df = pd.DataFrame([\n",
            "            {\n",
            "                'Model': 'Full (4 GPUs)',\n",
            "                'Instance': 'g6.12xlarge',\n",
            "                'GPUs': 4,\n",
            "                '$/Hour': f'${cost_4gpu:.2f}',\n",
            "                f'Tok/s @{target_conc}': f'{tps_full:.0f}',\n",
            "                '$ per 1M Tokens': f'${cost_full_1m:.2f}'\n",
            "            },\n",
            "            {\n",
            "                'Model': 'Quant (1 GPU)',\n",
            "                'Instance': 'g6.4xlarge',\n",
            "                'GPUs': 1,\n",
            "                '$/Hour': f'${cost_1gpu:.2f}',\n",
            "                f'Tok/s @{target_conc}': f'{tps_quant:.0f}',\n",
            "                '$ per 1M Tokens': f'${cost_quant_1m:.2f}'\n",
            "            }\n",
            "        ])\n",
            "        \n",
            "        display(cost_df)\n",
            "        \n",
            "        savings = ((cost_full_1m - cost_quant_1m) / cost_full_1m) * 100\n",
            "        print(f\"\\n\ud83d\udcb0 Quantized model: {savings:.0f}% lower cost per token\")\n",
            "        print(f\"\u26a1 Quantized model: 75% fewer GPUs (1 vs 4)\")\n",
            "    else:\n",
            "        print(f\"\u26a0\ufe0f  Concurrency level {target_conc} not available in results\")\n",
            "else:\n",
            "    print(\"\u274c No metrics for cost analysis\")"
          ]
        }
      ],
      "metadata": {
        "kernelspec": {
          "display_name": "Python 3 (ipykernel)",
          "language": "python",
          "name": "python3"
        },
        "language_info": {
          "name": "python",
          "version": "3.11.0"
        }
      },
      "nbformat": 4,
      "nbformat_minor": 4
    }
