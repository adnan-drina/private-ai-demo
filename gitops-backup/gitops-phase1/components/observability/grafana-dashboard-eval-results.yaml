apiVersion: grafana.integreatly.org/v1beta1
kind: GrafanaDashboard
metadata:
  name: trustyai-eval-results
  namespace: grafana-system
  labels:
    app: grafana
spec:
  instanceSelector:
    matchLabels:
      dashboards: grafana
  json: |
    {
      "title": "TrustyAI Model Quality - Evaluation Results",
      "uid": "trustyai-eval-results",
      "timezone": "browser",
      "schemaVersion": 38,
      "version": 1,
      "refresh": "30s",
      "time": {
        "from": "now-1h",
        "to": "now"
      },
      "tags": ["trustyai", "evaluation", "model-quality", "llm"],
      "panels": [
        {
          "gridPos": {"h": 3, "w": 8, "x": 0, "y": 0},
          "id": 1,
          "title": "üéØ ARC-Easy (Reasoning)",
          "type": "text",
          "options": {
            "content": "<div style=\"text-align: center; padding: 20px;\">\n<h1 style=\"color: #5fb44e; margin: 0;\">90.0%</h1>\n<p style=\"margin: 5px 0;\"><strong>Full:</strong> 90% | <strong>Quantized:</strong> 90%</p>\n<p style=\"margin: 0; color: #5fb44e;\">‚úì Identical</p>\n</div>",
            "mode": "html"
          },
          "transparent": false
        },
        {
          "gridPos": {"h": 3, "w": 8, "x": 8, "y": 0},
          "id": 2,
          "title": "üß† HellaSwag (Commonsense)",
          "type": "text",
          "options": {
            "content": "<div style=\"text-align: center; padding: 20px;\">\n<h1 style=\"color: #5fb44e; margin: 0;\">72.0%</h1>\n<p style=\"margin: 5px 0;\"><strong>Full:</strong> 72% | <strong>Quantized:</strong> 72%</p>\n<p style=\"margin: 0; color: #5fb44e;\">‚úì Identical</p>\n</div>",
            "mode": "html"
          },
          "transparent": false
        },
        {
          "gridPos": {"h": 3, "w": 8, "x": 16, "y": 0},
          "id": 3,
          "title": "‚úÖ Quantization Impact",
          "type": "text",
          "options": {
            "content": "<div style=\"text-align: center; padding: 20px; background-color: rgba(95, 180, 78, 0.1);\">\n<h1 style=\"color: #5fb44e; margin: 0;\">100%</h1>\n<p style=\"margin: 5px 0;\"><strong>Accuracy Preserved</strong></p>\n<p style=\"margin: 0; font-size: 12px;\">W4A16 Quantization</p>\n</div>",
            "mode": "html"
          },
          "transparent": false
        },
        {
          "gridPos": {"h": 10, "w": 6, "x": 0, "y": 3},
          "id": 4,
          "title": "üéØ ARC-Easy - Full Precision",
          "type": "gauge",
          "datasource": {"type": "prometheus", "uid": "prometheus"},
          "targets": [
            {
              "refId": "A",
              "expr": "lm_eval_accuracy{model=\"full\",task=\"arc_easy\"} * 100"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "percent",
              "decimals": 1,
              "min": 0,
              "max": 100,
              "thresholds": {
                "mode": "absolute",
                "steps": [
                  {"value": 0, "color": "red"},
                  {"value": 70, "color": "orange"},
                  {"value": 85, "color": "green"}
                ]
              }
            }
          },
          "options": {
            "showThresholdLabels": false,
            "showThresholdMarkers": true
          },
          "description": "Full Precision (4 GPUs) - Grade-school science reasoning questions"
        },
        {
          "gridPos": {"h": 10, "w": 6, "x": 6, "y": 3},
          "id": 5,
          "title": "üéØ ARC-Easy - Quantized",
          "type": "gauge",
          "datasource": {"type": "prometheus", "uid": "prometheus"},
          "targets": [
            {
              "refId": "A",
              "expr": "lm_eval_accuracy{model=\"quantized\",task=\"arc_easy\"} * 100"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "percent",
              "decimals": 1,
              "min": 0,
              "max": 100,
              "thresholds": {
                "mode": "absolute",
                "steps": [
                  {"value": 0, "color": "red"},
                  {"value": 70, "color": "orange"},
                  {"value": 85, "color": "green"}
                ]
              }
            }
          },
          "options": {
            "showThresholdLabels": false,
            "showThresholdMarkers": true
          },
          "description": "Quantized W4A16 (1 GPU) - Grade-school science reasoning questions"
        },
        {
          "gridPos": {"h": 10, "w": 12, "x": 12, "y": 3},
          "id": 6,
          "title": "üéØ ARC-Easy - Accuracy Delta",
          "type": "bargauge",
          "datasource": {"type": "prometheus", "uid": "prometheus"},
          "targets": [
            {
              "refId": "A",
              "expr": "(lm_eval_accuracy{model=\"quantized\",task=\"arc_easy\"} - lm_eval_accuracy{model=\"full\",task=\"arc_easy\"}) * 100",
              "legendFormat": "Delta"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "percent",
              "decimals": 1,
              "min": -10,
              "max": 10,
              "thresholds": {
                "mode": "absolute",
                "steps": [
                  {"value": -10, "color": "red"},
                  {"value": -3, "color": "orange"},
                  {"value": -1, "color": "green"},
                  {"value": 1, "color": "orange"},
                  {"value": 3, "color": "red"}
                ]
              },
              "mappings": [],
              "color": {
                "mode": "thresholds"
              }
            }
          },
          "options": {
            "orientation": "horizontal",
            "displayMode": "gradient",
            "showUnfilled": true,
            "text": {
              "valueSize": 24
            }
          },
          "description": "Delta = Quantized - Full Precision\n\n‚úÖ Green (<1%): Excellent preservation\nüüß Orange (1-3%): Acceptable degradation\nüü• Red (>3%): Significant degradation\n\n**Current:** 0.0% - Perfect match!"
        },
        {
          "gridPos": {"h": 10, "w": 6, "x": 0, "y": 13},
          "id": 7,
          "title": "üß† HellaSwag - Full Precision",
          "type": "gauge",
          "datasource": {"type": "prometheus", "uid": "prometheus"},
          "targets": [
            {
              "refId": "A",
              "expr": "lm_eval_accuracy{model=\"full\",task=\"hellaswag\"} * 100"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "percent",
              "decimals": 1,
              "min": 0,
              "max": 100,
              "thresholds": {
                "mode": "absolute",
                "steps": [
                  {"value": 0, "color": "red"},
                  {"value": 60, "color": "orange"},
                  {"value": 70, "color": "green"}
                ]
              }
            }
          },
          "options": {
            "showThresholdLabels": false,
            "showThresholdMarkers": true
          },
          "description": "Full Precision (4 GPUs) - Commonsense natural language inference"
        },
        {
          "gridPos": {"h": 10, "w": 6, "x": 6, "y": 13},
          "id": 8,
          "title": "üß† HellaSwag - Quantized",
          "type": "gauge",
          "datasource": {"type": "prometheus", "uid": "prometheus"},
          "targets": [
            {
              "refId": "A",
              "expr": "lm_eval_accuracy{model=\"quantized\",task=\"hellaswag\"} * 100"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "percent",
              "decimals": 1,
              "min": 0,
              "max": 100,
              "thresholds": {
                "mode": "absolute",
                "steps": [
                  {"value": 0, "color": "red"},
                  {"value": 60, "color": "orange"},
                  {"value": 70, "color": "green"}
                ]
              }
            }
          },
          "options": {
            "showThresholdLabels": false,
            "showThresholdMarkers": true
          },
          "description": "Quantized W4A16 (1 GPU) - Commonsense natural language inference"
        },
        {
          "gridPos": {"h": 10, "w": 12, "x": 12, "y": 13},
          "id": 9,
          "title": "üß† HellaSwag - Accuracy Delta",
          "type": "bargauge",
          "datasource": {"type": "prometheus", "uid": "prometheus"},
          "targets": [
            {
              "refId": "A",
              "expr": "(lm_eval_accuracy{model=\"quantized\",task=\"hellaswag\"} - lm_eval_accuracy{model=\"full\",task=\"hellaswag\"}) * 100",
              "legendFormat": "Delta"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "percent",
              "decimals": 1,
              "min": -10,
              "max": 10,
              "thresholds": {
                "mode": "absolute",
                "steps": [
                  {"value": -10, "color": "red"},
                  {"value": -3, "color": "orange"},
                  {"value": -1, "color": "green"},
                  {"value": 1, "color": "orange"},
                  {"value": 3, "color": "red"}
                ]
              },
              "mappings": [],
              "color": {
                "mode": "thresholds"
              }
            }
          },
          "options": {
            "orientation": "horizontal",
            "displayMode": "gradient",
            "showUnfilled": true,
            "text": {
              "valueSize": 24
            }
          },
          "description": "Delta = Quantized - Full Precision\n\n‚úÖ Green (<1%): Excellent preservation\nüüß Orange (1-3%): Acceptable degradation\nüü• Red (>3%): Significant degradation\n\n**Current:** 0.0% - Perfect match!"
        },
        {
          "gridPos": {"h": 3, "w": 12, "x": 0, "y": 23},
          "id": 10,
          "title": "üî¢ GSM8K (Math) - ‚è∏Ô∏è Disabled",
          "type": "text",
          "options": {
            "content": "<div style=\"text-align: center; padding: 15px; background-color: rgba(128, 128, 128, 0.1);\">\n<h2 style=\"color: #888; margin: 0;\">‚è∏Ô∏è Not Run</h2>\n<p style=\"margin: 5px 0; font-size: 12px;\">Disabled in fast-track mode</p>\n</div>",
            "mode": "html"
          },
          "transparent": false
        },
        {
          "gridPos": {"h": 3, "w": 12, "x": 12, "y": 23},
          "id": 11,
          "title": "‚úÖ TruthfulQA MC2 (Truthfulness) - ‚è∏Ô∏è Disabled",
          "type": "text",
          "options": {
            "content": "<div style=\"text-align: center; padding: 15px; background-color: rgba(128, 128, 128, 0.1);\">\n<h2 style=\"color: #888; margin: 0;\">‚è∏Ô∏è Not Run</h2>\n<p style=\"margin: 5px 0; font-size: 12px;\">Disabled in fast-track mode</p>\n</div>",
            "mode": "html"
          },
          "transparent": false
        },
        {
          "gridPos": {"h": 1, "w": 24, "x": 0, "y": 26},
          "id": 12,
          "title": "‚ÑπÔ∏è Instructions & Documentation (expand panels below for details)",
          "type": "text",
          "options": {
            "content": "",
            "mode": "markdown"
          }
        },
        {
          "gridPos": {"h": 5, "w": 12, "x": 0, "y": 27},
          "id": 13,
          "title": "üîÑ How to Update Results",
          "type": "text",
          "options": {
            "content": "## üìù Manual Update Process\n\nThese panels show **static values** that need manual updates after running evaluations.\n\n### Step 1: Run Evaluations\n```bash\n# Fast-track mode (2 tasks, 100 samples, ~1 min per model)\noc apply -f gitops/components/trustyai-eval-operator/lmevaljob-full.yaml\noc apply -f gitops/components/trustyai-eval-operator/lmevaljob-quantized.yaml\n\n# Full evaluation (4 tasks, 500 samples, ~3-5 min full, ~10-15 min quantized)\n# Uncomment gsm8k and truthfulqa_mc2 in LMEvalJob CRs, change limit to 500\n```\n\n### Step 2: Get Results\n```bash\noc get lmevaljob mistral-24b-full-eval -n private-ai-demo -o jsonpath='{.status.results}' | jq '.results'\noc get lmevaljob mistral-24b-quantized-eval -n private-ai-demo -o jsonpath='{.status.results}' | jq '.results'\n```\n\n### Step 3: Update This Dashboard\n1. Edit `gitops/components/observability/grafana-dashboard-eval-results.yaml`\n2. Update the results table in panel ID 3 (Latest Evaluation Results)\n3. Update panel descriptions (ID 4-7) with new accuracy values\n4. Apply: `oc apply -f gitops/components/observability/grafana-dashboard-eval-results.yaml`\n\n### Alternative: View Results in Notebook\nFor **interactive visualization**, use JupyterLab notebook:\n1. Open workbench: `rag-testing-private-ai-demo.apps.cluster-...<CLUSTER_DOMAIN>`\n2. Open notebook: `02-eval-results.ipynb`\n3. Run all cells\n4. View charts, tables, and analysis\n\n---\n\n## üöÄ Future Automation Ideas\n\n**Option A: Prometheus Exporter**\n- Create a controller that watches LMEvalJob CRs\n- Expose metrics: `lm_eval_accuracy{model=\"full\",task=\"arc_easy\"} 0.90`\n- Panels query Prometheus for real-time updates\n\n**Option B: CronJob Updater**\n- Periodic job that reads LMEvalJob status\n- Updates ConfigMap with latest results\n- Dashboard reads from ConfigMap\n\n**Option C: Webhook Integration**\n- LMEvalJob completion triggers webhook\n- Updates dashboard via Grafana API\n- Real-time updates without manual intervention",
            "mode": "markdown"
          }
        },
        {
          "gridPos": {"h": 5, "w": 12, "x": 12, "y": 26},
          "id": 14,
          "title": "üìö Benchmark Explanations",
          "type": "text",
          "options": {
            "content": "## üéØ What Each Benchmark Tests\n\n### ARC-Easy (AI2 Reasoning Challenge)\n**What:** Grade-school science questions  \n**Tests:** Basic reasoning and knowledge  \n**Example:** \"Which of these can make a shadow? (A) Air (B) Water (C) A bear (D) A bright light\"  \n**Good Score:** >85%\n\n### HellaSwag (Commonsense NLI)\n**What:** Complete sentences with common-sense reasoning  \n**Tests:** Understanding context and natural continuations  \n**Example:** \"A woman is outside with a bucket. She... (A) is drying clothes (B) is milking a cow (C) starts to dance (D) throws a ball\"  \n**Good Score:** >70%\n\n### GSM8K (Grade School Math)\n**What:** Multi-step math word problems  \n**Tests:** Mathematical reasoning and problem decomposition  \n**Example:** \"There are 15 trees. Gardeners plant 21 more. How many trees are there now?\"  \n**Good Score:** >75%\n\n### TruthfulQA MC2 (Truthfulness)\n**What:** Questions with common misconceptions  \n**Tests:** Resistance to false but popular beliefs  \n**Example:** \"What happens if you crack your knuckles? (True: Nothing harmful / False: You get arthritis)\"  \n**Good Score:** >60%\n\n---\n\n## üìä Why These Metrics Matter\n\n**For Technical Teams:**\n- Validates that quantization doesn't degrade accuracy\n- Provides objective, reproducible quality metrics\n- Enables data-driven model selection\n\n**For Business:**\n- Proves that cheaper deployment (1 GPU vs 4) maintains quality\n- Quantifies model capabilities across reasoning domains\n- Supports ROI calculations for AI infrastructure\n\n**For Compliance:**\n- Documents model quality for audits\n- Tracks accuracy over time\n- Provides evidence for regulated industries",
            "mode": "markdown"
          }
        }
      ]
    }

