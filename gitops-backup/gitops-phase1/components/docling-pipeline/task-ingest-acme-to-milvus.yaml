---
apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: ingest-acme-to-milvus
  namespace: private-ai-demo
  labels:
    app: docling-pipeline
    scenario: acme
spec:
  description: |
    Ingest ACME chunks into Milvus via Llama Stack API.
    Reuses proven EU AI Act ingestion code.
  
  params:
    - name: doc-name
      type: string
      description: "Document filename"
  
  workspaces:
    - name: documents
      description: "PVC containing documents"
  
  steps:
    - name: ingest
      image: python:3.11-slim
      env:
        - name: PYTHONUNBUFFERED
          value: "1"
      script: |
        #!/usr/bin/env python3
        import json
        import urllib.request
        import urllib.error
        import re
        from pathlib import Path
        
        DOC_NAME = "$(params.doc-name)"
        LLAMASTACK_URL = "http://llama-stack-service.private-ai-demo.svc:8321"
        DOCS_DIR = "/workspace/documents"
        
        print("=" * 70)
        print("  ACME Ingestion via Llama Stack (EU AI Act Pattern)")
        print("=" * 70)
        print()
        
        # Load chunks (ACME-specific path)
        doc_base = DOC_NAME.replace('.pdf', '')
        chunks_path = Path(DOCS_DIR) / 'acme' / 'parsed' / f"{doc_base}_chunks.json"
        
        if not chunks_path.exists():
            print(f"‚ùå Chunks file not found: {chunks_path}")
            exit(1)
        
        with open(chunks_path, 'r') as f:
            chunks = json.load(f)
        
        print(f"üìÑ Document: {DOC_NAME}")
        print(f"üìã Loaded {len(chunks)} chunks")
        print()
        
        def call_llamastack_api(endpoint, data):
            """Call Llama Stack API using urllib (no dependencies)"""
            url = f"{LLAMASTACK_URL}/{endpoint}"
            
            req = urllib.request.Request(
                url,
                data=json.dumps(data).encode('utf-8'),
                headers={'Content-Type': 'application/json'},
                method="POST"
            )
            
            try:
                with urllib.request.urlopen(req, timeout=120) as response:
                    return json.loads(response.read().decode('utf-8'))
            except urllib.error.HTTPError as e:
                error_body = e.read().decode('utf-8')
                print(f"‚ùå HTTP Error {e.code}: {error_body}")
                raise
            except Exception as e:
                print(f"‚ùå Error: {e}")
                raise
        
        # Ingest chunks one at a time (proven EU AI Act pattern)
        total_ingested = 0
        
        for i, chunk in enumerate(chunks, 1):
            try:
                # Extract content
                content = chunk.get('text', '')
                
                # CRITICAL: Strip base64 images to avoid Milvus 65KB limit
                content = re.sub(r'!\[Image\]\(data:image/[^;]+;base64,[^\)]+\)', '[Image removed]', content)
                
                if not content or len(content.strip()) < 10:
                    print(f"   ‚è© Skipping empty chunk {i}")
                    continue
                
                # Enforce Milvus 65KB limit
                if len(content) > 60000:
                    print(f"   ‚ö†Ô∏è  Chunk {i} too large, truncating...")
                    content = content[:60000] + "... [truncated]"
                
                # Prepare metadata (ACME-specific fields)
                metadata = chunk.get('metadata', {})
                document_id = chunk.get('chunk_id', f"{doc_base}-chunk-{i}")
                
                # CRITICAL: document_id must be in BOTH places (Llama Stack requirement)
                metadata["document_id"] = document_id
                
                # Prepare insert payload (exact EU AI Act pattern)
                insert_data = {
                    "vector_db_id": "rag_documents",
                    "chunks": [{
                        "document_id": document_id,
                        "content": content,
                        "metadata": metadata
                    }]
                }
                
                # Insert via Llama Stack
                call_llamastack_api("v1/vector-io/insert", insert_data)
                total_ingested += 1
                
                if i % 10 == 0:
                    print(f"   ‚úÖ Ingested {i}/{len(chunks)}")
            
            except Exception as e:
                if i == 1:
                    print(f"   ‚ö†Ô∏è  Error ingesting chunk {i}: {e}")
        
        print()
        print(f"üìä Summary: Ingested {total_ingested}/{len(chunks)} chunks")
        print("=" * 70)
