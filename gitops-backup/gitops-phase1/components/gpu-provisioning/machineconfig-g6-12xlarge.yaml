---
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  name: 99-worker-gpu-g6-12xlarge
  labels:
    machineconfiguration.openshift.io/role: worker-gpu
    gpu.openshift.io/instance-type: g6-12xlarge
spec:
  config:
    ignition:
      version: 3.2.0
    storage:
      files:
        # Instance-specific GPU configuration for g6.12xlarge
        - path: /etc/gpu-instance-config.yaml
          mode: 0644
          overwrite: true
          contents:
            inline: |
              # g6.12xlarge GPU Configuration
              # Instance: g6.12xlarge
              # GPUs: 4x NVIDIA L4 (24GB each)
              # vCPUs: 48
              # Memory: 192 GB
              # Storage: NVMe SSD
              # Network: Up to 50 Gbps
              # GPU Interconnect: PCIe 4.0
              
              instance_type: g6.12xlarge
              gpu_count: 4
              gpu_model: NVIDIA-L4
              gpu_memory_per_device: 24576  # MB
              total_gpu_memory: 98304  # MB (4 x 24GB)
              
              # Optimal settings for multi-GPU
              nvidia_driver_settings:
                persistence_mode: 1
                compute_mode: 0  # Default (multiple contexts)
                power_limit: 72  # Watts per GPU (L4 TDP)
              
              # Multi-GPU topology
              gpu_topology:
                type: pcie
                numa_aware: true
                p2p_enabled: true
              
              # CRI-O configuration for multi-GPU
              crio_settings:
                max_container_log_size: 10Mi
                pids_limit: 8192  # Higher for multi-GPU workloads
              
              # Resource allocation recommendations
              recommended_limits:
                # For large vLLM models (Mistral 24B full)
                vllm_large:
                  gpu: "4"
                  memory: "80Gi"
                  cpu: "16"
                  gpu_memory_utilization: "0.90"
                  tensor_parallel_size: 4
                  max_model_len: 65536
                
                # For distributed inference
                distributed:
                  enable_tensor_parallelism: true
                  enable_pipeline_parallelism: false
                  nccl_socket_ifname: "eth0"
              
              # NCCL (NVIDIA Collective Communications Library) settings
              nccl:
                ib_disable: 1  # Disable InfiniBand (not available on g6)
                p2p_disable: 0  # Enable P2P
                net_gdr_level: 0
                debug: INFO
                socket_ifname: eth0
              
              # Monitoring and logging
              monitoring:
                dcgm_exporter: true
                nvidia_smi_interval: 30s  # More frequent for multi-GPU
                log_level: info
                enable_nvlink_monitoring: false  # No NVLink on g6
        
        # Tuning parameters for multi-GPU workloads
        - path: /etc/sysctl.d/99-g6-12xlarge-gpu.conf
          mode: 0644
          overwrite: true
          contents:
            inline: |
              # g6.12xlarge Specific Tuning
              # Optimized for multi-GPU tensor parallel workloads
              
              # CPU scheduling for multi-GPU workloads
              kernel.sched_rt_runtime_us = -1
              kernel.sched_latency_ns = 10000000
              kernel.sched_min_granularity_ns = 3000000
              
              # Memory settings for 192GB RAM
              vm.dirty_ratio = 15
              vm.dirty_background_ratio = 5
              vm.min_free_kbytes = 1048576  # 1GB reserved
              
              # Huge pages for large model inference (optional)
              # vm.nr_hugepages = 8192  # 16GB in 2MB pages
              
              # IPC settings for multi-GPU communication
              kernel.shmmax = 68719476736  # 64GB
              kernel.shmall = 16777216
              kernel.shmmni = 4096
              kernel.sem = 250 32000 100 1024
              
              # Network settings for distributed training/inference
              net.core.netdev_max_backlog = 250000
              net.core.optmem_max = 67108864
              net.ipv4.tcp_congestion_control = bbr
              
              # File descriptor limits
              fs.file-max = 1048576
        
        # SystemD drop-in for kubelet with multi-GPU settings
        - path: /etc/systemd/system/kubelet.service.d/20-gpu-g6-12xlarge.conf
          mode: 0644
          overwrite: true
          contents:
            inline: |
              [Service]
              Environment="KUBELET_GPU_ARGS=--feature-gates=DevicePlugins=true,CPUManager=true"
              Environment="NVIDIA_VISIBLE_DEVICES=all"
              Environment="NVIDIA_DRIVER_CAPABILITIES=compute,utility,video"
              # NCCL settings for multi-GPU
              Environment="NCCL_DEBUG=INFO"
              Environment="NCCL_IB_DISABLE=1"
              Environment="NCCL_P2P_DISABLE=0"
              Environment="NCCL_SOCKET_IFNAME=eth0"
        
        # CPU pinning configuration for NUMA-aware GPU workloads
        - path: /etc/kubernetes/cpu-manager-policy.yaml
          mode: 0644
          overwrite: true
          contents:
            inline: |
              # CPU Manager Policy for g6.12xlarge
              # Enables NUMA-aware CPU allocation for GPU pods
              
              apiVersion: v1
              kind: ConfigMap
              metadata:
                name: cpu-manager-policy
                namespace: kube-system
              data:
                policy: static
                reserved-cpus: "0-3"  # Reserve 4 CPUs for system

