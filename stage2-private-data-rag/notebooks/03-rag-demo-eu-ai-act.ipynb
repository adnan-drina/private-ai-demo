{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EU AI Act RAG Demo - Compliance Assistant\n",
    "\n",
    "This notebook demonstrates **Retrieval-Augmented Generation (RAG)** for EU AI Act compliance queries.\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "User Question ‚Üí Llama Stack Agent ‚Üí Milvus Vector Search ‚Üí Retrieved Context ‚Üí Mistral 24B ‚Üí Grounded Answer\n",
    "```\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Baseline vs. RAG Comparison**: See the difference between generic LLM responses and RAG-enhanced answers\n",
    "- **Source Attribution**: Every answer includes precise citations (Article, Page, Document)\n",
    "- **Compliance Use Case**: Demonstrates AI for legal/regulatory scenarios\n",
    "- **Universal Relevance**: EU AI Act applies globally to any AI deployed in EU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q llama-stack-client openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "from openai import OpenAI\n",
    "from llama_stack_client import LlamaStackClient, Agent, AgentEventLogger\n",
    "\n",
    "# Configuration\n",
    "MISTRAL_URL = \"https://mistral-24b-quantized-private-ai-demo.apps.cluster-n8cnx.n8cnx.sandbox2830.opentlc.com/v1\"\n",
    "LLAMASTACK_URL = \"http://llama-stack-service.private-ai-demo.svc:8321\"\n",
    "MODEL_ID = \"mistral-24b-quantized\"\n",
    "\n",
    "# Initialize clients\n",
    "vllm_client = OpenAI(base_url=MISTRAL_URL, api_key=\"dummy\")\n",
    "llama_client = LlamaStackClient(base_url=LLAMASTACK_URL)\n",
    "\n",
    "print(\"‚úÖ Clients initialized\")\n",
    "print(f\"   Direct vLLM: {MISTRAL_URL}\")\n",
    "print(f\"   Llama Stack: {LLAMASTACK_URL}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Questions\n",
    "\n",
    "We'll test 4 types of queries that showcase RAG's value for compliance scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test questions covering different aspects of the EU AI Act\n",
    "QUESTIONS = {\n",
    "    \"prohibited\": \"According to Article 5 of the EU AI Act, what AI practices are explicitly prohibited? List only the distinct categories found in the retrieved context.\",\n",
    "    \"high_risk\": \"Is an AI-powered CV screening tool for hiring considered high-risk under the EU AI Act? Why or why not?\",\n",
    "    \"timeline\": \"When do the main obligations of the EU AI Act come into force? What are the key dates?\",\n",
    "    \"gpai\": \"What are the specific obligations for General Purpose AI (GPAI) models under the EU AI Act?\"\n",
    "}\n",
    "\n",
    "# We'll use the \"high_risk\" question for the main demo\n",
    "DEMO_QUESTION = QUESTIONS[\"high_risk\"]\n",
    "\n",
    "print(\"üìã Test Questions:\")\n",
    "for key, question in QUESTIONS.items():\n",
    "    print(f\"\\n{key.upper()}:\")\n",
    "    print(f\"  {question}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 1: Baseline (No RAG)\n",
    "\n",
    "First, let's ask the model **without RAG**. The model will respond based only on its training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"BASELINE RESPONSE (No RAG - Direct vLLM)\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(f\"Question: {DEMO_QUESTION}\")\n",
    "print()\n",
    "\n",
    "# Call vLLM directly (no RAG)\n",
    "response_baseline = vllm_client.chat.completions.create(\n",
    "    model=MODEL_ID,\n",
    "    messages=[{\"role\": \"user\", \"content\": DEMO_QUESTION}],\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "baseline_answer = response_baseline.choices[0].message.content\n",
    "print(baseline_answer)\n",
    "print()\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è Issues with Baseline Response:\n",
    "\n",
    "- Generic, may be outdated\n",
    "- No specific citations\n",
    "- No reference to actual EU AI Act text\n",
    "- Vague recommendations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 2: RAG (With Llama Stack Agent)\n",
    "\n",
    "Now let's use **RAG with the Llama Stack Agent**. The agent will:\n",
    "1. Search for relevant EU AI Act content in Milvus\n",
    "2. Retrieve precise articles and sections\n",
    "3. Generate an answer grounded in the actual legal text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"RAG RESPONSE (With Vector Retrieval - Llama Stack Agent)\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(f\"Question: {DEMO_QUESTION}\")\n",
    "print()\n",
    "\n",
    "try:\n",
    "    # Create RAG agent using high-level API (Red Hat pattern)\n",
    "    rag_agent = Agent(\n",
    "        llama_client,\n",
    "        model=MODEL_ID,  # Note: \"model\" not \"model_id\" for Agent class!\n",
    "        instructions=(\n",
    "            \"You are an EU AI Act compliance assistant. \"\n",
    "            \"Answer questions using ONLY information from the retrieved EU AI Act documents. \"\n",
    "            \"\\n\"\n",
    "            \"For list-based questions about prohibited practices or similar:\\n\"\n",
    "            \"- State how many items the retrieved context contains\\n\"\n",
    "            \"- List each distinct item ONCE with [OJ p.X, Art.Y] citation\\n\"\n",
    "            \"- If items look similar, they are likely the same - list only once\\n\"\n",
    "            \"- STOP immediately after listing all distinct items\\n\"\n",
    "            \"\\n\"\n",
    "            \"For analytical questions (e.g., 'is X considered high-risk'):\\n\"\n",
    "            \"- Provide a clear, complete explanation\\n\"\n",
    "            \"- Reference specific Articles and Annexes with citations\\n\"\n",
    "            \"- Explain the reasoning and criteria\\n\"\n",
    "            \"\\n\"\n",
    "            \"If information is not in the sources, say 'Not found in sources.'\"\n",
    "        ),\n",
    "        tools=[\n",
    "            {\n",
    "                \"name\": \"builtin::rag/knowledge_search\",\n",
    "                \"args\": {\"vector_db_ids\": [\"rag_documents\"]},\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Agent created\")\n",
    "    print()\n",
    "    \n",
    "    # Create session and query\n",
    "    session_id = rag_agent.create_session(session_name=f\"eu-ai-act-demo-{uuid.uuid4().hex[:8]}\")\n",
    "    print(f\"‚úÖ Session created: {session_id}\")\n",
    "    print()\n",
    "    \n",
    "    # Ask question with RAG (streaming)\n",
    "    response = rag_agent.create_turn(\n",
    "        messages=[{\"role\": \"user\", \"content\": DEMO_QUESTION}],\n",
    "        session_id=session_id,\n",
    "        stream=True,\n",
    "    )\n",
    "    \n",
    "    # Capture and log the response\n",
    "    rag_answer = \"\"\n",
    "    for log in AgentEventLogger().log(response):\n",
    "        log.print()\n",
    "        # Capture all content chunks (streaming tokens)\n",
    "        if hasattr(log, 'content') and log.content:\n",
    "            rag_answer += log.content\n",
    "    \n",
    "    print()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"\\n‚ÑπÔ∏è  Note: RAG requires documents to be ingested into Milvus.\")\n",
    "    print(\"   Run the Tekton pipeline to process documents first.\")\n",
    "    rag_answer = None\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Side-by-Side Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def print_comparison(baseline, rag):\n",
    "    print(\"=\"*140)\n",
    "    print(f\"{'BASELINE (No RAG)':^70} | {'RAG (With Retrieval)':^70}\")\n",
    "    print(\"=\"*140)\n",
    "    \n",
    "    baseline_lines = textwrap.wrap(baseline or \"N/A\", width=68)\n",
    "    rag_lines = textwrap.wrap(rag or \"N/A\", width=68)\n",
    "    \n",
    "    max_lines = max(len(baseline_lines), len(rag_lines))\n",
    "    \n",
    "    for i in range(max_lines):\n",
    "        baseline_line = baseline_lines[i] if i < len(baseline_lines) else \"\"\n",
    "        rag_line = rag_lines[i] if i < len(rag_lines) else \"\"\n",
    "        print(f\"{baseline_line:68} | {rag_line:68}\")\n",
    "    \n",
    "    print(\"=\"*140)\n",
    "\n",
    "if rag_answer:\n",
    "    print_comparison(baseline_answer, rag_answer)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  RAG answer not available - documents may need to be ingested\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test All Questions\n",
    "\n",
    "Let's test all 4 question types to see RAG's performance across different scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_question_with_rag(question, question_type):\n",
    "    \"\"\"Test a question with RAG\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"TEST: {question_type.upper()}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print()\n",
    "    \n",
    "    try:\n",
    "        # Create new session for each question using the Agent class\n",
    "        new_session_id = rag_agent.create_session(session_name=f\"test-{question_type}-{uuid.uuid4().hex[:8]}\")\n",
    "        \n",
    "        # Ask question with streaming\n",
    "        response = rag_agent.create_turn(\n",
    "            messages=[{\"role\": \"user\", \"content\": question}],\n",
    "            session_id=new_session_id,\n",
    "            stream=True,\n",
    "        )\n",
    "        \n",
    "        # Capture and log the response\n",
    "        answer = \"\"\n",
    "        for log in AgentEventLogger().log(response):\n",
    "            log.print()\n",
    "            # Capture all content chunks (streaming tokens)\n",
    "            if hasattr(log, 'content') and log.content:\n",
    "                answer += log.content\n",
    "        \n",
    "        print()\n",
    "        return answer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Test all questions if agent is available\n",
    "if rag_answer and 'rag_agent' in locals():\n",
    "    results = {}\n",
    "    for q_type, question in QUESTIONS.items():\n",
    "        if q_type != \"high_risk\":  # Already tested\n",
    "            results[q_type] = test_question_with_rag(question, q_type)\n",
    "    \n",
    "    print(\"\\n‚úÖ All questions tested!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping additional tests - agent not available\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Value: Compliance Assistant\n",
    "\n",
    "### Why This Matters for Enterprises\n",
    "\n",
    "**1. Compliance Cost Reduction**\n",
    "- ‚ùå Without RAG: Manual legal review, expensive consultants, weeks of research\n",
    "- ‚úÖ With RAG: Instant, accurate compliance guidance with citations\n",
    "\n",
    "**2. Risk Mitigation**\n",
    "- ‚ùå Without RAG: Generic answers, potential non-compliance, fines up to ‚Ç¨35M\n",
    "- ‚úÖ With RAG: Precise, cited answers grounded in actual legal text\n",
    "\n",
    "**3. Speed to Market**\n",
    "- ‚ùå Without RAG: Weeks to understand requirements\n",
    "- ‚úÖ With RAG: Minutes to get actionable guidance\n",
    "\n",
    "**4. Global Relevance**\n",
    "- EU AI Act applies to ANY AI system deployed in the EU\n",
    "- Affects US, Asian, and global companies\n",
    "- Penalties: Up to 7% of global annual turnover\n",
    "\n",
    "### Use Cases Beyond EU AI Act\n",
    "\n",
    "This same architecture works for:\n",
    "- **Legal**: Contract analysis, case law research\n",
    "- **Healthcare**: Clinical guidelines, drug interactions\n",
    "- **Finance**: Regulatory compliance (GDPR, MiFID II, etc.)\n",
    "- **Manufacturing**: Safety standards, quality procedures\n",
    "- **Any domain** with complex documentation requirements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red Hat AI Four Pillars\n",
    "\n",
    "This demo showcases all four pillars:\n",
    "\n",
    "### 1. ‚úÖ Efficient Inferencing\n",
    "- Quantized Mistral 24B (4-bit compression)\n",
    "- 50%+ cost savings vs. full precision\n",
    "- Tool calling for efficient RAG retrieval\n",
    "\n",
    "### 2. ‚úÖ Simplified Data Connection\n",
    "- **Docling**: AI-powered document processing (tables, equations, annexes)\n",
    "- **Tekton Pipeline**: Automated ingestion workflow\n",
    "- **Milvus**: Enterprise vector database\n",
    "- **Llama Stack**: Unified RAG runtime\n",
    "\n",
    "### 3. ‚úÖ Hybrid Cloud Flexibility\n",
    "- All data on-premise (sovereign AI)\n",
    "- Multi-tenant architecture (`ai-infrastructure` shared services)\n",
    "- GitOps-managed deployments\n",
    "- Air-gap ready\n",
    "\n",
    "### 4. ‚úÖ Agentic AI Delivery\n",
    "- Llama Stack Agent with tool calling\n",
    "- Autonomous retrieval and generation\n",
    "- Foundation for Stage 3 (MCP servers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What We Demonstrated:**\n",
    "- ‚úÖ RAG significantly improves answer quality for compliance queries\n",
    "- ‚úÖ Source attribution builds trust and auditability\n",
    "- ‚úÖ Automated document processing (Tekton pipeline)\n",
    "- ‚úÖ Enterprise-grade architecture (Llama Stack, Milvus, vLLM)\n",
    "- ‚úÖ Red Hat AI Four Pillars in action\n",
    "\n",
    "**Key Takeaway:**  \n",
    "RAG transforms LLMs from generic chatbots into **precise, auditable compliance assistants** that can save enterprises millions in legal review costs while mitigating regulatory risk.\n",
    "\n",
    "---\n",
    "\n",
    "**Status**: ‚úÖ Production Ready  \n",
    "**Demo Time**: 10-15 minutes  \n",
    "**Audience**: Legal, Compliance, Risk Management, Enterprise AI\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
