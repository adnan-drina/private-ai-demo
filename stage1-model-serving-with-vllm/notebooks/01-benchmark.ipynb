{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ vLLM Performance Benchmark - GuideLLM Results\n",
        "\n",
        "## Red Hat OpenShift AI - Stage 1: Sovereign AI Foundation\n",
        "\n",
        "---\n",
        "\n",
        "This notebook shows **professional benchmark results** using GuideLLM to compare:\n",
        "\n",
        "- **Quantized Model:** Mistral 24B quantized (w4a16) on 1 GPU (g6.4xlarge)\n",
        "- **Full Precision Model:** Mistral 24B full on 4 GPUs (g6.12xlarge)\n",
        "\n",
        "**Configured Test Levels:** 1, 5, 10, 25, 50 concurrent requests\n",
        "\n",
        "**Metrics:**\n",
        "- üìä Throughput (tokens/second)\n",
        "- ‚è±Ô∏è Latency (TTFT - Time To First Token P99, ITL - Inter-Token Latency P50)\n",
        "- üí∞ Cost efficiency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "import json\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"‚úÖ Libraries loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìä Load GuideLLM Benchmark Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load benchmark results from PVC\n",
        "results_dir = Path('/results')\n",
        "quantized_file = results_dir / 'mistral-24b-quantized-benchmark.json'\n",
        "full_file = results_dir / 'mistral-24b-benchmark.json'\n",
        "\n",
        "if quantized_file.exists() and full_file.exists():\n",
        "    with open(quantized_file) as f:\n",
        "        quantized_data = json.load(f)\n",
        "    with open(full_file) as f:\n",
        "        full_data = json.load(f)\n",
        "    print(\"‚úÖ Benchmark results loaded successfully\")\n",
        "else:\n",
        "    print(\"‚ùå Benchmark results not found. Run the GuideLLM jobs first.\")\n",
        "    quantized_data = None\n",
        "    full_data = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìà Extract and Map to Target Concurrency Levels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Target concurrency levels we configured\n",
        "TARGET_LEVELS = [1, 5, 10, 25, 50]\n",
        "\n",
        "def map_to_target_level(actual_conc):\n",
        "    \"\"\"Map actual concurrency to nearest target level\"\"\"\n",
        "    if actual_conc < 1:\n",
        "        return None  # Skip warmup/zero levels\n",
        "    # Find nearest target level\n",
        "    return min(TARGET_LEVELS, key=lambda x: abs(x - actual_conc))\n",
        "\n",
        "def extract_metrics(data, model_name):\n",
        "    results = {}\n",
        "    for benchmark in data['benchmarks']:\n",
        "        m = benchmark['metrics']\n",
        "        actual_conc = m['request_concurrency']['successful']['mean']\n",
        "        target_conc = map_to_target_level(actual_conc)\n",
        "        \n",
        "        if target_conc is None:\n",
        "            continue  # Skip warmup phases\n",
        "        \n",
        "        # Keep the best result for each target level\n",
        "        if target_conc not in results or abs(actual_conc - target_conc) < abs(results[target_conc]['_actual'] - target_conc):\n",
        "            results[target_conc] = {\n",
        "                '_actual': actual_conc,\n",
        "                'Throughput': round(m['tokens_per_second']['successful']['mean'], 1),\n",
        "                'TTFT_P99': round(m['time_to_first_token_ms']['successful']['percentiles']['p99'], 1),\n",
        "                'ITL_P50': round(m['inter_token_latency_ms']['successful']['median'], 1),\n",
        "            }\n",
        "    return results\n",
        "\n",
        "if quantized_data and full_data:\n",
        "    quant_metrics = extract_metrics(quantized_data, 'Quantized')\n",
        "    full_metrics = extract_metrics(full_data, 'Full')\n",
        "    \n",
        "    # Build metric-first comparison table using TARGET_LEVELS\n",
        "    comparison = []\n",
        "    for target in TARGET_LEVELS:\n",
        "        row = {'Concurrency': target}\n",
        "        \n",
        "        # Throughput (Full then Quantized)\n",
        "        if target in full_metrics:\n",
        "            row['Full Thr'] = full_metrics[target]['Throughput']\n",
        "        if target in quant_metrics:\n",
        "            row['Quant Thr'] = quant_metrics[target]['Throughput']\n",
        "        \n",
        "        # TTFT P99 (Full then Quantized)\n",
        "        if target in full_metrics:\n",
        "            row['Full TTFT'] = full_metrics[target]['TTFT_P99']\n",
        "        if target in quant_metrics:\n",
        "            row['Quant TTFT'] = quant_metrics[target]['TTFT_P99']\n",
        "        \n",
        "        # ITL P50 (Full then Quantized)\n",
        "        if target in full_metrics:\n",
        "            row['Full ITL'] = full_metrics[target]['ITL_P50']\n",
        "        if target in quant_metrics:\n",
        "            row['Quant ITL'] = quant_metrics[target]['ITL_P50']\n",
        "        \n",
        "        # Only include row if we have data\n",
        "        if len(row) > 1:\n",
        "            comparison.append(row)\n",
        "    \n",
        "    df_comparison = pd.DataFrame(comparison)\n",
        "    print(f\"‚úÖ Comparison table ready ({len(comparison)} concurrency levels)\")\n",
        "else:\n",
        "    df_comparison = None\n",
        "    quant_metrics = None\n",
        "    full_metrics = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìä Performance Results\n",
        "\n",
        "**Column Legend:**\n",
        "- **Thr** = Throughput (tokens/second)\n",
        "- **TTFT** = Time To First Token P99 (milliseconds)\n",
        "- **ITL** = Inter-Token Latency P50 (milliseconds)\n",
        "\n",
        "For each metric, **Full** (4 GPUs) is shown first, then **Quant** (1 GPU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if df_comparison is not None:\n",
        "    display(df_comparison)\n",
        "else:\n",
        "    print(\"‚ùå No data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üí∞ Cost Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if quant_metrics and full_metrics:\n",
        "    cost_1gpu = 1.84  # g6.4xlarge $/hour\n",
        "    cost_4gpu = 5.52  # g6.12xlarge $/hour\n",
        "    \n",
        "    # Use concurrency level 10 for cost comparison\n",
        "    target_conc = 10\n",
        "    \n",
        "    if target_conc in quant_metrics and target_conc in full_metrics:\n",
        "        tps_quant = quant_metrics[target_conc]['Throughput']\n",
        "        tps_full = full_metrics[target_conc]['Throughput']\n",
        "        \n",
        "        cost_quant_1m = (cost_1gpu / (tps_quant * 3600)) * 1_000_000\n",
        "        cost_full_1m = (cost_4gpu / (tps_full * 3600)) * 1_000_000\n",
        "        \n",
        "        cost_df = pd.DataFrame([\n",
        "            {\n",
        "                'Model': 'Full (4 GPUs)',\n",
        "                'Instance': 'g6.12xlarge',\n",
        "                'GPUs': 4,\n",
        "                '$/Hour': f'${cost_4gpu:.2f}',\n",
        "                f'Tok/s @{target_conc}': f'{tps_full:.0f}',\n",
        "                '$ per 1M Tokens': f'${cost_full_1m:.2f}'\n",
        "            },\n",
        "            {\n",
        "                'Model': 'Quant (1 GPU)',\n",
        "                'Instance': 'g6.4xlarge',\n",
        "                'GPUs': 1,\n",
        "                '$/Hour': f'${cost_1gpu:.2f}',\n",
        "                f'Tok/s @{target_conc}': f'{tps_quant:.0f}',\n",
        "                '$ per 1M Tokens': f'${cost_quant_1m:.2f}'\n",
        "            }\n",
        "        ])\n",
        "        \n",
        "        display(cost_df)\n",
        "        \n",
        "        savings = ((cost_full_1m - cost_quant_1m) / cost_full_1m) * 100\n",
        "        print(f\"\\nüí∞ Quantized model: {savings:.0f}% lower cost per token\")\n",
        "        print(f\"‚ö° Quantized model: 75% fewer GPUs (1 vs 4)\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  Concurrency level {target_conc} not available in results\")\n",
        "else:\n",
        "    print(\"‚ùå No metrics for cost analysis\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
