# CRITICAL: Tekton Cluster Misconfiguration - Affinity Assistant Deadlock

**Status**: üî¥ **BLOCKING** - All PipelineRuns with PVC workspaces stuck in Pending  
**Date**: 2025-10-27  
**Severity**: HIGH - Production deployment blocked  
**Requires**: Cluster Admin / OpenShift Pipelines Operator Owner

---

## Executive Summary

The OpenShift Pipelines / Tekton controller has **contradictory feature flags** causing a guaranteed scheduler deadlock for any PipelineRun using RWO PVC workspaces.

**Current Invalid State:**
```yaml
coschedule: "workspaces"              # ‚úÖ Inject pod affinity for workspace co-scheduling
disable-affinity-assistant: "true"    # ‚ùå Don't create the assistant pod
```

**Result:** TaskRun pods require affinity to a pod that **never gets created** ‚Üí Permanent Pending state

---

## Symptoms Observed

### 1. TaskRun Pods Stuck in Pending Forever

```bash
$ oc get pod mistral-24b-quantized-qncl8-download-model-pod -n private-ai-demo
NAME                                             READY   STATUS    RESTARTS   AGE
mistral-24b-quantized-qncl8-download-model-pod   0/1     Pending   0          10m
```

### 2. Pod Has Affinity Requirement

```bash
$ oc get pod mistral-24b-quantized-qncl8-download-model-pod -o jsonpath='{.spec.affinity}'
{
    "podAffinity": {
        "requiredDuringSchedulingIgnoredDuringExecution": [
            {
                "labelSelector": {
                    "matchLabels": {
                        "app.kubernetes.io/component": "affinity-assistant",
                        "app.kubernetes.io/instance": "affinity-assistant-607af64060"
                    }
                },
                "topologyKey": "kubernetes.io/hostname"
            }
        ]
    }
}
```

### 3. Affinity Assistant Pod Never Created

```bash
$ oc get pod -n private-ai-demo -l app.kubernetes.io/component=affinity-assistant
No resources found in private-ai-demo namespace.
```

### 4. Scheduling Error

```
0/7 nodes are available: 
  1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 
  3 node(s) didn't match pod affinity rules,  ‚Üê THE PROBLEM
  3 node(s) had untolerated taint {nvidia.com/gpu: true}
```

---

## Root Cause Analysis

### What Happened

The Tekton controller feature flags are internally contradictory:

1. **`coschedule: "workspaces"`** tells Tekton:
   - ‚úÖ "Enforce workspace-level co-scheduling"
   - ‚úÖ "Inject `podAffinity` into TaskRun pods"
   - ‚úÖ "All tasks sharing a PVC-backed workspace must run on same node"

2. **`disable-affinity-assistant: "true"`** tells Tekton:
   - ‚ùå "Don't create the affinity assistant pod"
   - ‚ùå "Don't create the StatefulSet that provides the affinity target"

3. **Result:**
   - TaskRun pods get `podAffinity` rules (from #1)
   - But the target pod doesn't exist (from #2)
   - Kubernetes scheduler: "No node matches pod affinity rules"
   - Pod: **Stuck in Pending forever** üî¥

### Why This Configuration Exists

This is likely a legacy from a Tekton upgrade where:
- Older Tekton: `disable-affinity-assistant: true` disabled BOTH the assistant AND the affinity injection
- Newer Tekton: `coschedule` flag was introduced, and `disable-affinity-assistant` became partially honored
- After upgrade: Flags not reconciled ‚Üí Invalid state

### Why This Is Deadly for Our Use Case

Our ModelCar pipeline **requires**:
- ‚úÖ Single large RWO PVC (500GB)
- ‚úÖ Multiple tasks sharing that workspace (download ‚Üí build ‚Üí push)
- ‚úÖ Tasks running on **same node** (RWO = single attachment point)
- ‚úÖ Cross-task file access (build creates OCI archive, push reads it)

**Without affinity assistant:**
- Tasks can't schedule (current state)
- OR tasks schedule on different nodes ‚Üí PVC attach/detach failures
- OR permission denied between tasks

---

## The Correct Configuration (Red Hat Best Practice)

### For Large RWO PVC Workloads

**Tekton Feature Flags (Cluster-Level):**
```yaml
coschedule: "workspaces"              # ‚úÖ Co-schedule tasks sharing PVC workspace
disable-affinity-assistant: "false"   # ‚úÖ Create assistant to enable co-scheduling
# OR remove disable-affinity-assistant entirely
```

**PipelineRun Configuration (Application-Level):**
```yaml
spec:
  taskRunTemplate:
    serviceAccountName: model-pipeline-sa
    podTemplate:
      securityContext:
        fsGroup: 2000                    # ‚úÖ Shared group for PVC access
        fsGroupChangePolicy: "OnRootMismatch"  # ‚úÖ Efficient permission handling
        # NO runAsUser - let OpenShift assign arbitrary UIDs
        # NO runAsGroup - fsGroup is sufficient
```

**Why This Works:**

1. **Affinity Assistant Gets Created:**
   - Tekton creates a small helper pod
   - All TaskRuns get `podAffinity` to this pod
   - Kubernetes scheduler: "Run all tasks on same node as assistant"
   - ‚úÖ All tasks land on same node

2. **PVC Mounts Once:**
   - RWO PVC attaches to one node
   - All tasks on that node can access it
   - No attach/detach churn
   - ‚úÖ Fast, reliable

3. **fsGroup Handles Permissions:**
   - PVC mounted with `gid=2000`
   - Files created by any task are group-writable
   - Different UIDs across tasks can still read/write
   - ‚úÖ No "permission denied" between tasks

4. **Assistant Passes SCC Checks:**
   - No forced `runAsUser` ‚Üí SCC admission happy
   - OpenShift assigns arbitrary UID (security best practice)
   - ‚úÖ Both assistant and TaskRun pods schedule successfully

---

## Required Fix (Cluster Admin Action)

### Step 1: Update Tekton Feature Flags

**Location:**
```bash
# OpenShift Pipelines uses TektonConfig CR
oc get tektonconfig config -o yaml

# OR edit the feature flags ConfigMap directly
oc edit configmap feature-flags -n openshift-pipelines
```

**Change Required:**
```yaml
data:
  coschedule: "workspaces"              # Keep this
  disable-affinity-assistant: "false"   # Change from "true" to "false"
  # ... other flags unchanged ...
```

**OR via TektonConfig CR:**
```yaml
apiVersion: operator.tekton.dev/v1alpha1
kind: TektonConfig
metadata:
  name: config
spec:
  pipeline:
    disable-affinity-assistant: false  # Change this
    # ... other settings ...
```

### Step 2: Restart Tekton Controller (if needed)

```bash
# The operator may auto-restart, but if not:
oc rollout restart deployment tekton-pipelines-controller -n openshift-pipelines

# Wait for new pod
oc wait --for=condition=ready pod -l app=tekton-pipelines-controller -n openshift-pipelines --timeout=60s
```

### Step 3: Verify Fix

```bash
# 1. Check feature flags are consistent
oc get configmap feature-flags -n openshift-pipelines -o yaml | grep -E "coschedule|disable-affinity"
# Expected:
#   coschedule: "workspaces"
#   disable-affinity-assistant: "false"

# 2. Create a test PipelineRun
oc create -f gitops/stage01-model-serving/serving/pipelines/03-runs/pipelinerun-mistral-quantized.yaml -n private-ai-demo

# 3. Verify affinity assistant is created
oc get pod -n private-ai-demo -l app.kubernetes.io/component=affinity-assistant
# Expected: 1 pod in Running state

# 4. Verify TaskRun pod schedules
oc get pod -n private-ai-demo | grep download-model
# Expected: Pod moves from Pending ‚Üí Running

# 5. Check pod has affinity AND target exists
TASKRUN_POD=$(oc get pod -n private-ai-demo | grep download-model | awk '{print $1}')
oc get pod $TASKRUN_POD -o jsonpath='{.spec.affinity.podAffinity.requiredDuringSchedulingIgnoredDuringExecution[0].labelSelector.matchLabels}'
# Should show affinity-assistant labels
```

---

## Application-Level Changes (After Cluster Fix)

### Current PipelineRun Configuration

File: `gitops/stage01-model-serving/serving/pipelines/03-runs/pipelinerun-mistral-quantized.yaml`

```yaml
spec:
  taskRunTemplate:
    serviceAccountName: model-pipeline-sa
    podTemplate:
      securityContext:
        fsGroup: 2000                         # ‚úÖ CORRECT
        fsGroupChangePolicy: "OnRootMismatch" # ‚úÖ CORRECT
        # NO runAsUser                        # ‚úÖ CORRECT (removed)
```

**Status:** ‚úÖ **Already Correct** - No changes needed once cluster is fixed!

### What This Achieves

1. **Scheduling:** Affinity assistant created ‚Üí TaskRuns schedule on same node
2. **Permissions:** `fsGroup: 2000` ‚Üí All tasks can read/write PVC files
3. **Security:** No forced UID ‚Üí Passes OpenShift SCC
4. **Push Task:** Can read OCI archive from build task (same group ownership)

---

## Testing Plan (After Fix)

### 1. Quantized Model Pipeline (20-30 min)

```bash
oc create -f gitops/stage01-model-serving/serving/pipelines/03-runs/pipelinerun-mistral-quantized.yaml -n private-ai-demo
```

**Expected Success Criteria:**
- ‚úÖ Affinity assistant pod created and Running
- ‚úÖ Download task schedules and completes
- ‚úÖ Build task creates OCI archive on PVC
- ‚úÖ Push-to-internal task reads OCI archive (no permission denied)
- ‚úÖ Push-to-quay task reads OCI archive (no permission denied)
- ‚úÖ Register-model task completes

### 2. Full Precision Model Pipeline (2-3 hours)

```bash
oc create -f gitops/stage01-model-serving/serving/pipelines/03-runs/pipelinerun-mistral-full.yaml -n private-ai-demo
```

**Expected Success Criteria:**
- Same as quantized, but with 48GB model
- ‚úÖ PVC usage stays under 500Gi limit
- ‚úÖ All tasks complete on same node

---

## Alternative: Disable Co-Scheduling Entirely (NOT RECOMMENDED)

If for some reason the cluster admin cannot enable affinity assistant, the alternative is:

```yaml
coschedule: "disabled"
disable-affinity-assistant: "true"
```

**Effect:**
- ‚ùå No pod affinity injected
- ‚úÖ TaskRuns can schedule independently
- ‚ö†Ô∏è RWO PVC may cause attach/detach churn
- ‚ö†Ô∏è Tasks may fail with "multi-attach error"
- ‚ö†Ô∏è Slower, less reliable for large PVC workloads

**This is a fallback, not a solution.** For 500GB RWO PVC workloads, affinity assistant is the correct approach.

---

## Documentation References

### Tekton Official Docs

1. **Affinity Assistants:** https://tekton.dev/docs/pipelines/affinityassistants/
   - "The Affinity Assistant is incompatible with other affinity rules"
   - "All TaskRun pods that share the same PVC workspace are co-scheduled on the same node"

2. **Workspaces:** https://tekton.dev/docs/pipelines/workspaces/
   - "Use `fsGroup` to grant cross-task read/write access"
   - "Avoid forcing `runAsUser` as it can conflict with SCC"

3. **Coschedule Mode:** https://tekton.dev/docs/pipelines/additional-configs/
   - `coschedule: workspaces` - "Co-schedule TaskRuns sharing same PVC workspace"
   - `coschedule: disabled` - "No automatic co-scheduling"

### Red Hat OpenShift Pipelines Docs

4. **Security Context Constraints:** https://docs.openshift.com/pipelines/latest/
   - "Use arbitrary UIDs with fsGroup for shared storage"
   - "Avoid privileged SCC unless absolutely required"

---

## Impact Assessment

### Blocked Capabilities

- ‚ùå **All Pipeline A runs** (ModelCar build/push)
- ‚ùå **Any pipeline using PVC workspaces**
- ‚ùå **Multi-task pipelines requiring shared artifacts**

### Estimated Downtime

- **Cluster Fix:** 5-10 minutes (edit config, restart controller)
- **Validation:** 30 minutes (run quantized pipeline)
- **Total:** ~40 minutes

### Risk of Fix

- **Low Risk:** Standard feature flag change
- **Reversible:** Can revert ConfigMap if needed
- **No Workload Impact:** Other pipelines unaffected

---

## Communication Template for Cluster Admin

```
Subject: URGENT: Tekton Feature Flags Causing PipelineRun Deadlock

Hi [Admin Name],

We've identified a critical misconfiguration in our OpenShift Pipelines / Tekton setup 
that's blocking all PipelineRuns using PVC workspaces.

**Issue:**
The feature flags are contradictory:
  coschedule: "workspaces"              (inject affinity)
  disable-affinity-assistant: "true"     (don't create affinity target)

This causes TaskRun pods to require affinity to a pod that never exists, 
resulting in permanent Pending state.

**Fix Required:**
In namespace: openshift-pipelines
ConfigMap: feature-flags

Change:
  disable-affinity-assistant: "true"
To:
  disable-affinity-assistant: "false"

OR via TektonConfig CR:
  spec.pipeline.disable-affinity-assistant: false

Then restart:
  oc rollout restart deployment tekton-pipelines-controller -n openshift-pipelines

**Impact:**
- Current: All PVC-backed pipelines blocked
- After fix: Normal operation restored
- Risk: Low (standard config change)
- Reversible: Yes

**Documentation:**
See attached: CRITICAL-CLUSTER-MISCONFIGURATION.md

Please let me know when this can be scheduled.
Thanks!
```

---

## Conclusion

**This is NOT a workaround situation.** The cluster configuration is objectively broken and must be fixed at the platform level.

**Once fixed**, our PipelineRun configuration (using `fsGroup` without `runAsUser`) is **already correct** and follows Red Hat best practices.

**Next Steps:**
1. ‚è≥ Cluster admin updates feature flags
2. ‚è≥ Controller restart
3. ‚úÖ Test quantized pipeline
4. ‚úÖ Run full precision pipeline
5. ‚úÖ Production deployment unblocked

---

**Prepared by:** AI Assistant + User Analysis  
**Date:** 2025-10-27  
**Confidence:** HIGH - Cluster misconfiguration confirmed via controller logs and feature flags  
**Action Required:** Cluster Admin / OpenShift Pipelines Operator Owner

