apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  name: chunk-acme-documents
  namespace: private-ai-demo
  labels:
    app: docling-pipeline
    scenario: acme
spec:
  description: Chunk ACME documents with manufacturing-specific metadata
  params:
    - name: doc-name
      type: string
      description: Document filename
  workspaces:
    - name: documents
      description: PVC containing documents
  steps:
    - name: chunk-document
      image: python:3.11-slim
      env:
        - name: PYTHONUNBUFFERED
          value: "1"
      script: |
        #!/usr/bin/env python3
        import json
        import re
        from pathlib import Path
        from typing import List, Dict

        DOC_NAME = "$(params.doc-name)"
        DOCS_DIR = "/workspace/documents"
        
        # Chunking parameters (same as EU AI Act)
        MAX_CHUNK_TOKENS = 500
        OVERLAP_TOKENS = 50
        
        print(f"ğŸ“„ Chunking ACME document: {DOC_NAME}")
        print("=" * 70)
        
        # Load metadata
        doc_base = DOC_NAME.replace('.pdf', '')
        metadata_path = Path(DOCS_DIR) / 'acme' / 'parsed' / f"{doc_base}_metadata.json"
        md_path = Path(DOCS_DIR) / 'acme' / 'parsed' / f"{doc_base}_docling.md"
        
        if not metadata_path.exists() or not md_path.exists():
            print(f"âš ï¸  Required files not found")
            print(f"   Metadata: {metadata_path.exists()}")
            print(f"   Markdown: {md_path.exists()}")
            exit(1)
        
        with open(metadata_path, 'r') as f:
            base_metadata = json.load(f)
        
        with open(md_path, 'r') as f:
            content = f.read()
        
        print(f"ğŸ“‹ Document: {base_metadata['doc_name']} ({base_metadata['doc_type']})")
        print(f"ğŸ“ Content length: {len(content)} characters")
        
        # Simple token estimation (words * 1.3)
        def estimate_tokens(text: str) -> int:
            return int(len(text.split()) * 1.3)
        
        # Heading-aware chunking
        def chunk_by_headings(text: str, sections: List[Dict]) -> List[Dict]:
            chunks = []
            
            # Split by major headings first
            lines = text.split('\n')
            current_chunk = []
            current_section = None
            current_page = 1
            chunk_tokens = 0
            
            for line in lines:
                line_stripped = line.strip()
                
                # Detect page markers
                page_match = re.search(r'\[Page (\d+)\]', line)
                if page_match:
                    current_page = int(page_match.group(1))
                    continue
                
                # Detect section headings
                is_heading = line_stripped.startswith('#')
                if is_heading:
                    # Save previous chunk if exists
                    if current_chunk:
                        chunk_text = '\n'.join(current_chunk)
                        if chunk_text.strip():
                            chunks.append({
                                'text': chunk_text,
                                'section': current_section,
                                'page_start': current_page,
                                'tokens': estimate_tokens(chunk_text)
                            })
                        current_chunk = []
                        chunk_tokens = 0
                    
                    # Update current section
                    current_section = line_stripped.lstrip('#').strip()
                
                # Add line to current chunk
                current_chunk.append(line)
                chunk_tokens = estimate_tokens('\n'.join(current_chunk))
                
                # Split if chunk too large
                if chunk_tokens > MAX_CHUNK_TOKENS and not is_heading:
                    chunk_text = '\n'.join(current_chunk)
                    chunks.append({
                        'text': chunk_text,
                        'section': current_section,
                        'page_start': current_page,
                        'tokens': chunk_tokens
                    })
                    # Keep overlap
                    overlap_lines = current_chunk[-5:] if len(current_chunk) > 5 else current_chunk
                    current_chunk = overlap_lines
                    chunk_tokens = estimate_tokens('\n'.join(current_chunk))
            
            # Save final chunk
            if current_chunk:
                chunk_text = '\n'.join(current_chunk)
                if chunk_text.strip():
                    chunks.append({
                        'text': chunk_text,
                        'section': current_section,
                        'page_start': current_page,
                        'tokens': estimate_tokens(chunk_text)
                    })
            
            return chunks
        
        # Generate chunks
        chunks = chunk_by_headings(content, base_metadata.get('sections', []))
        
        print(f"âœ‚ï¸  Generated {len(chunks)} chunks")
        print(f"ğŸ“Š Avg tokens per chunk: {sum(c['tokens'] for c in chunks) / len(chunks):.0f}")
        
        # Enrich chunks with ACME metadata
        enriched_chunks = []
        section_prefix = base_metadata.get('section_prefix', 'Â§DOC')
        
        for i, chunk in enumerate(chunks, 1):
            # Generate section ID
            section_text = chunk['section'] or 'Unknown'
            section_num = i
            section_id = f"{section_prefix}-{section_num}"
            
            # Extract layer info from chunk text if present
            layer_match = re.search(r'\b(M\d+|V\d+)\b', chunk['text'])
            layer = layer_match.group(1) if layer_match else base_metadata.get('layer')
            
            # Build enriched chunk
            enriched = {
                'chunk_id': f"{base_metadata['doc_name']}_chunk_{i}",
                'text': chunk['text'],
                'metadata': {
                    'doc_name': base_metadata['doc_name'],
                    'doc_type': base_metadata['doc_type'],
                    'version': base_metadata['version'],
                    'section_id': section_id,
                    'section_title': section_text[:100],  # Truncate long titles
                    'page_start': chunk['page_start'],
                    'tool_model': base_metadata.get('tool_model'),
                    'product': base_metadata.get('product'),
                    'layer': layer,
                    'anchor': f"pdf://{base_metadata['doc_name']}#page={chunk['page_start']}",
                    'tokens': chunk['tokens']
                }
            }
            
            # Add limit values if available (for SPC chunks)
            if base_metadata['doc_type'] == 'SPC' and base_metadata.get('limits'):
                enriched['metadata']['limits'] = base_metadata['limits']
            
            enriched_chunks.append(enriched)
        
        # Save chunks
        chunks_path = Path(DOCS_DIR) / 'acme' / 'parsed' / f"{doc_base}_chunks.json"
        with open(chunks_path, 'w') as f:
            json.dump(enriched_chunks, f, indent=2)
        
        print()
        print("=" * 70)
        print(f"âœ… Saved {len(enriched_chunks)} enriched chunks")
        print(f"ğŸ“ Output: {chunks_path}")
        
        # Show sample chunk
        if enriched_chunks:
            sample = enriched_chunks[0]
            print()
            print("ğŸ“‹ Sample chunk metadata:")
            print(f"   ID: {sample['chunk_id']}")
            print(f"   Section: {sample['metadata']['section_id']} - {sample['metadata']['section_title']}")
            print(f"   Tool: {sample['metadata']['tool_model']}, Product: {sample['metadata']['product']}")
            print(f"   Text preview: {sample['text'][:150]}...")


