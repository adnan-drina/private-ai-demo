---
apiVersion: trustyai.opendatahub.io/v1alpha1
kind: LMEvalJob
metadata:
  name: mistral-24b-full-eval
  namespace: private-ai-demo
  labels:
    app.kubernetes.io/name: trustyai-eval
    app.kubernetes.io/component: lm-eval
    app.kubernetes.io/part-of: trustyai-evaluation
    model: mistral-24b
    eval-profile: "tuning-step-1"
  annotations:
    description: "Step 1: Raise concurrency (2→4) for full precision model"
spec:
  # Step 1: Raise Concurrency ONLY
  # All other parameters unchanged from Step 0
  model: local-completions
  modelArgs:
    - name: model
      value: "mistral-24b"
    - name: base_url
      value: "https://mistral-24b-private-ai-demo.apps.cluster-qtvt5.qtvt5.sandbox2082.opentlc.com/v1/completions"
    - name: tokenizer
      value: "mistralai/Mistral-Small-24B-Instruct-2501"
    # CHANGED: Raise concurrency from 2 to 4
    - name: num_concurrent
      value: "4"              # ← CHANGED from 2
    # UNCHANGED: All other params from Step 0
    - name: max_retries
      value: "5"              # unchanged
    - name: timeout
      value: "300"            # unchanged (monitor first)
    - name: tokenized_requests
      value: "false"
    # Deterministic sampling (locked for fairness)
    - name: temperature
      value: "0.0"
    - name: top_p
      value: "1.0"
    - name: max_tokens
      value: "256"
    - name: seed
      value: "1234"
  
  # Tasks to evaluate (locked for fairness)
  # Fast-track mode: Using 2 tasks for speed (10x faster)
  # Uncomment gsm8k and truthfulqa_mc2 below for full evaluation
  taskList:
    taskNames:
      - arc_easy         # General knowledge
      - hellaswag        # Common sense reasoning
      # - gsm8k          # Math reasoning (slow, ~25% of eval time)
      # - truthfulqa_mc2 # Truthfulness (slow, ~25% of eval time)
  
  # Few-shot configuration (locked)
  numFewShot: 0
  
  # Sampling limits (locked for fairness)
  # Fast-track: 100 samples (5x speedup), change to 500 for production eval
  limit: "100"
  
  # UNCHANGED: Batch size from Step 0
  batchSize: "1"              # unchanged
  
  # Enable online dataset access
  allowOnline: true
  allowCodeExecution: true
  
  # Log samples for debugging
  logSamples: false
  
  # Pod configuration
  pod:
    container:
      env:
        - name: HF_HUB_OFFLINE
          value: "0"
        - name: TRANSFORMERS_OFFLINE
          value: "0"
  
  # Output configuration
  outputs:
    pvcManaged:
      size: "1Gi"
