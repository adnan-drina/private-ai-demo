{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG Demo: Red Hat OpenShift AI Documentation\n",
        "\n",
        "## Scenario 1: Enhancing LLM Knowledge with Official Documentation\n",
        "\n",
        "**Business Context**: A Cloud Architect needs specific technical details about deploying RAG workloads in Red Hat OpenShift AI.\n",
        "\n",
        "**Objective**: Demonstrate how RAG transforms generic responses into accurate, source-cited answers.\n",
        "\n",
        "**Architecture**: This demo follows Red Hat's official RAG pattern:\n",
        "- **Baseline**: Direct vLLM API call (OpenAI-compatible)\n",
        "- **RAG**: Llama Stack Agent with vector retrieval\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install llama-stack-client openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup and imports\n",
        "from llama_stack_client import LlamaStackClient, Agent, AgentEventLogger\n",
        "from openai import OpenAI\n",
        "import uuid\n",
        "import json\n",
        "\n",
        "# Configuration\n",
        "VLLM_URL = \"https://mistral-24b-quantized-predictor-private-ai-demo.apps.cluster-qtvt5.qtvt5.sandbox2082.opentlc.com/v1\"\n",
        "LLAMASTACK_URL = \"http://llamastack.private-ai-demo.svc.cluster.local:8000\"\n",
        "MODEL_ID = \"mistral-24b-quantized\"\n",
        "VECTOR_DB_ID = \"rag_documents\"  # Shared vector database for all scenarios\n",
        "\n",
        "# Initialize clients\n",
        "vllm_client = OpenAI(base_url=VLLM_URL, api_key=\"dummy\")\n",
        "stack_client = LlamaStackClient(base_url=LLAMASTACK_URL)\n",
        "\n",
        "print(f\"✅ vLLM client configured: {VLLM_URL}\")\n",
        "print(f\"✅ Llama Stack client configured: {LLAMASTACK_URL}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Question\n",
        "\n",
        "We'll ask the same question twice:\n",
        "1. **Without RAG**: Generic response based on model's training\n",
        "2. **With RAG**: Specific response based on retrieved Red Hat documentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The test question\n",
        "QUESTION = (\n",
        "    \"What are the exact hardware and software prerequisites for deploying \"\n",
        "    \"a LlamaStack distribution in Red Hat OpenShift AI? Include GPU requirements, \"\n",
        "    \"operator dependencies, and the exact configuration steps.\"\n",
        ")\n",
        "\n",
        "print(\"Test Question:\")\n",
        "print(QUESTION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Baseline Query (Without RAG)\n",
        "\n",
        "First, let's see what the model knows without access to the documentation.\n",
        "We call vLLM directly using the OpenAI-compatible API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Query without RAG - Direct vLLM call\n",
        "print(\"=\" * 70)\n",
        "print(\"BASELINE RESPONSE (No RAG - Direct vLLM):\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "response_baseline = vllm_client.chat.completions.create(\n",
        "    model=MODEL_ID,\n",
        "    messages=[{\"role\": \"user\", \"content\": QUESTION}],\n",
        "    max_tokens=300\n",
        ")\n",
        "\n",
        "print(response_baseline.choices[0].message.content)\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. RAG Query (With Vector Retrieval)\n",
        "\n",
        "Now let's use Llama Stack's Agent API to retrieve relevant documentation and enhance the response.\n",
        "This follows Red Hat's official RAG pattern."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Query with RAG - Using Llama Stack Agent\n",
        "print(\"=\" * 70)\n",
        "print(\"RAG RESPONSE (With Vector Retrieval - Llama Stack Agent):\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Create RAG agent following Red Hat's pattern\n",
        "rag_agent = Agent(\n",
        "    stack_client,\n",
        "    model=MODEL_ID,\n",
        "    instructions=\"You are a helpful assistant with access to Red Hat OpenShift AI documentation.\",\n",
        "    tools=[\n",
        "        {\n",
        "            \"name\": \"builtin::rag/knowledge_search\",\n",
        "            \"args\": {\"vector_db_ids\": [VECTOR_DB_ID]},\n",
        "        }\n",
        "    ],\n",
        ")\n",
        "\n",
        "# Create session and query\n",
        "session_id = rag_agent.create_session(session_name=f\"rag-demo-{uuid.uuid4().hex[:8]}\")\n",
        "\n",
        "response_rag = rag_agent.create_turn(\n",
        "    messages=[{\"role\": \"user\", \"content\": QUESTION}],\n",
        "    session_id=session_id,\n",
        "    stream=True,\n",
        ")\n",
        "\n",
        "# Log the response\n",
        "for log in AgentEventLogger().log(response_rag):\n",
        "    log.print()\n",
        "\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Comparison\n",
        "\n",
        "**Baseline (No RAG)**: Generic answer based on the model's training data.\n",
        "\n",
        "**RAG (With Retrieval)**: Specific, accurate answer citing Red Hat documentation.\n",
        "\n",
        "---\n",
        "\n",
        "## Red Hat AI Four Pillars Demonstrated\n",
        "\n",
        "1. **Efficient Inferencing**: Quantized Mistral 24B on vLLM\n",
        "2. **Simplified Data Connection**: Milvus vector store via Llama Stack\n",
        "3. **Hybrid Cloud Flexibility**: OpenShift AI on any infrastructure\n",
        "4. **Agentic AI Delivery**: Llama Stack Agent with RAG tools"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
